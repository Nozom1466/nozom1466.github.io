[{"content":"Homepage\nFor BART, T5, mT5 and AlexaTM 20B\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series.\nFigure 1. LLMs evolutionary tree.\nContributions Combination of bidirectional encider and autoregressive decoder: BERT(bi-directional) + GPT(uni-direct auto-regressive) Pretraining with better noising approaches: shuffling + in-filling scheme Approach Architecture: BART use seq2seq Transformer architecture. Mofidications: ReLUs are modified as GeLUs, with initialization from @\\mathcal{N}(0, 0.02)@. 6 layers in encoder and 12 layers in decoder (following BERT) Remove Feed-Forward network before word prediction Pretraining: Trained by corrupting documents and then optimizing a reconstriction loss. Corruptions are introduced in Figure 2. Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed.\nFinetuning: Representations produced by BART are used in Seq/token classification, seq generationa and MT tasks. In MT, the author replace BART\u0026rsquo;s encoder embedding layer with a new randomly initialized encoder, as illustarted in Figure 3. Figure 3. Fine tuning BART for classification and translation.\nPretraining Objective Comparison Models: LM(GPT), Permuted LM(XLNet), Masked LM(BERT), Multitask Masked LM(UniLM), Masked seq2seq(MASS) Tasks: SQuAD(context + question -\u0026gt; context span) MNLI(con + q -\u0026gt; relation) ELI5, XSum, CNN/DM(con + q -\u0026gt; abstraction) ConvAI2(dialogue gen) Insights: Performance of pre-training methods varies significantly across tasks Token masking is crucial Left-to-right pre-training improves generation Bidirectional encoders are crucial for SQuAD The pre-training objective is not the only important factor Pure language models perform best on ELI5 BART achieves the most consistently strong performance. Large-scale Pre-training Experiments Pretraining with large batchsize (8000 a batch. 500k steps following RoBERTa):\nDiscriminative Tasks: BART’s improvements on generation tasks do not come at the expense of classification performance. Generation Tasks: Summarization, Dialogue and Abstrctive QA. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer This work explores the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. And the model proposed in this paper, known as T5, is trained on unified text2text framework, where all of outputs are regarded as texts. (\u0026ldquo;3.8\u0026rdquo;, \u0026ldquo;5\u0026rdquo;)\nMotivation Previously the training in ML is amenable to downstream learning tasks and the knowledge required for the learning is learned as part of auxiliary task.(word vectors). Later the scheme shifted to pretraining on data-rich tasks using unlabeled data (like Common Crawl project). The work focus on the understanding of burgeoning transfer learning techniques.\nBasic idea: treat every text processing problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output.\nArchitecture of T5 Model To summarize, T5 is roughly equivalent to the original Transformer with the exception of\nremoving the Layer Norm bias：activations are only rescaled and no additive bias is applied. placing the layer normalization outside the residual path using a different position embedding scheme.: We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model. Check out Transformers without Normalization from meta\u0026hellip;. Layer Norm is replaced by @tanh(\\cdot)@，\nDataset - The Colossal Clean Crawled Corpus Adopted from Common Crawl dataset, using cleaning up techniques:\nend in . page \u0026gt; 3 sentences; lines \u0026gt;= 5 words remove bad words lorem ipsum placeholder removed curly bracket removed citation markers removed policy \u0026amp; cookies removed leave only one in 3-span sentence sliding window pages not written in English Downstream Tasks Indeed an in-depth tech report \u0026hellip;\n750 GB dataset\nmachine translation: WMT question answering: SQuAD abstractive summarization: CNN/Daily Mail text classification: GLUE, SuperGLUE Baselines Goal: Pre-train a standard Transformer using a simple denoising objective and then separately fine-tune on each of our downstream tasks. Model: encoder and decoder are each similar in size and configuration to a BERT_BASE. Each has 12 blocks, FF layer @d_{ff}=3072@, @d_{kv} = 64@, multi-attention with 12 heads. Dropout prob 0.1. Training: As all tasks are regarded as t2t, loss f: teacher forcing and cross-entropy loss. AdaFactor as optimizor. Learning rate schedule: inverse square root (@1 / \\sqrt{\\max{(n, k)}}@), with n as iteration index and k as number of warm-up steps. Vocabulary: SentencePiece to encode text as WordPiece tokens. Unsupervised Objective: (Denoising) An objective that randomly samples and then drops out 15% of tokens in the input sequence, as shown in Figure 4. Figure 4.Schematic of the objective we use in our baseline model. In this example, we process the sentence “Thank you for inviting me to your party last week.” The words “for”, “inviting” and “last” (marked with an ×) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as and ) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel . The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token .\nThe following sections are discussing model performance from Architectures, Unsupervised Objectives, Pre-training Datasets, Training strategy and Scaling.\nArchitectures Another classification other than encoder/decoder: look into attention mask adopted by the model. There are 3 types of mask patterns: Fulli-visible, Casual and Casual with prefix, as shown in Figure 5.\nFigure 5. Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from “the future”. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\nStandard Encoder-Decoder architecture: All-visible mask(Encoder) + Casual mask(Decoder) Language Model: Casual mask, Language models are typically used for compression or sequence generation Prefix LM: Casual with Prefix mask, could be considered as encoder+decoder combined. Architectures as shown in Figure 6.\nPrefix LMs resembles BERT for classification tasks when you feeding the model with tasks like: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target: (Of course! Didn\u0026rsquo;t see why the author are adding this paragraph \u0026hellip;). Attention masking seems to be an interesting topic, which involves many other memory-efficient or computing-efficient methods (paged attention for vllm?). Figure 6. Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use “.” to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fullyvisible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input.\nHere the authors mentioned criterions in selecting models, condidering these models are in different architectures and parameters. We suppose two models are equivalent if they have the same parameter @P@ or the same computational cost @C@. Consider an encoder-decoder model with @L + L@ layers, @P + P@ parameters and a language model(decoder) with @2L@ layers and @2P@ parameters. The parameters are approximately the same for these models but the computation cost of language model is approx. twice of that in encoder-decoder model. Because the latter has to deal with both input squence and output sequence but the former deal with inputs and outputs separately. (has lots to do with sequence length \u0026hellip;). Theresfore, they select:\ne-d, L + L -\u0026gt; 2P, M flops\ne-d, shared params -\u0026gt; P, M flops\ne-d, L/2 + L/2 -\u0026gt; P, M/2 flops\nd, L -\u0026gt; P, M flops\nd, prefix -\u0026gt; P, M flops\nwhere e-d for encoder and decoder and L for layers, P for parameters, M for computational cost.\nResults (for different architecture): Denoising task (metioned in previous section) and language modeling task (predicting the whole sentence for language model and predicting the second half of the sentence given the first half). Sharing the params across e-d performed very well and halfing params hurts the performace.\nUnsupervised Objectives Examples of common unsupervised objectives (Figure 7). Models are fisrt pretrained based on these unsupervised objectives then evaluated on downstream tasks (GLUE, CNNDM, SQuAD, SGLUE, EnDe, EnFr and EnRo). This section extends in: 3 common unsupervised objectives -\u0026gt; variants of BERT objective (MLM) -\u0026gt; exploration of corruption rate -\u0026gt; exploration of corrupting spans.\nFigure 7. Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text “Thank you for inviting me to your party last week .” Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. denotes a shared mask token and , , and denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple.\nHigh Level Approaches: Tha author evaluated 3 types of objectives: Prefix language modeling, BERT-syle(MLM) and deshuffuling (as illustarted in Figure 7). BERT objective stands out (signifigantly over Deshuffling).\nVariants of BERT Objective: Purpose: better performance and better efficiency.\nVARIANT 1: MASS-style, reconstruct the original uncorrupted sequence(4th in Figure 7); VARIANT 2: Unique mask token, predict token prefixed by special token(5th in Figure 7).; VARIANT 3: Drop Corrupted Tokens, concatenate predicted tokens(6th in Figure 7). All these variants performs similarly. Notice that performace of dropping corrupted tokens fluctuates on several metrics. Dropping is still attractive because it reduces the input length thus making training faster. Corruption Rate: Limited effect on performance. Larger corruption rate -\u0026gt; more inference time.\nCorrupting Spans: BERT mask follows i.i.d., masking tokens independently. While in some cases we need consecutive corruption. Number of corruption span and span lengths are determined by parameters. Again, this trick has limited effect on downstream task performance. While span corruption slightly speeds up training because it produces shorter sequence on average.\nConclusions:\nDenoising objectives(BERT MLM) outperforms language modeling and deshuffling. Choosing among the denoising objectives we considered here should mainly be done according to their computational cost(since similar approaches yields slight improvements). It may be fortuitous to explore entirely different ways of leveraging unlabeled data. For conclusion 2, the paper seems only to explore BERT variants. What about the other two?\nPretraining Data set The effects of pretraininig dataset. There are these fun facts:\nC4 dataset proposed by this paper invloves a heuristic filtering strategy, which proved to be helpful in pretraining. Pretraining on in-domain unlabeled data can improve performance on downstream tasks. (not superising? like SFT?) But it\u0026rsquo;s not good if we want our model to adapt to language tasks from arbitraray domains. BUT the dataset gathered for specific domains are much smaller. Pretraining dataset size is also a key factor. The performance of pretrained model degrades as the size of dataset getting smaller. (model trys to memorize the dataset rather than learning.) Use large dataset as possible. Repeated dataset will degrade the performance while its ok if the repeated time are smaller than 64 (som metrics are even better) ","permalink":"http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/","summary":"\u003cp\u003e\u003ca href=\"https://www.cs.princeton.edu/courses/archive/fall22/cos597G/\"\u003eHomepage\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor BART, T5, mT5 and AlexaTM 20B\u003c/p\u003e\n\u003ch2 id=\"bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension\"\u003eBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\u003c/h2\u003e\n\u003cp\u003eEncoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series.\u003c/p\u003e","title":"COS597G 22 Encoder Decoder Models"},{"content":"\r最喜欢的蓝调时刻\n在宇治的日落时分\n京阪宇治线的下午\n温柔的冬日阳光\nHaruka 到站！\n","permalink":"http://localhost:1313/travel/serendipity/","summary":"一些旅行碎片","title":"Serendipity"},{"content":"Homepage\n(ELMo) Deep contextualized word representations Before Reading Authors are from AI2 and UW. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.\nMotivation ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation.\nELMo: Embedidngs from LM ELMo is built on biLM respresentations. BiLM gives prediction of token @t_k@ by combining forward and backward LM. Log likelihood of token @t_k@ is given by:\n$$\r\\begin{aligned} \u0026 \\sum_{k=1}^N\\left(\\log p\\left(t_k \\mid t_1, \\ldots, t_{k-1} ; \\Theta_x, \\vec{\\Theta}_{L S T M}, \\Theta_s\\right)\\right. \\\\ \u0026 \\left.\\quad+\\log p\\left(t_k \\mid t_{k+1}, \\ldots, t_N ; \\Theta_x, \\overleftarrow{\\Theta}_{L S T M}, \\Theta_s\\right)\\right)\\end{aligned}\r$$where @N@ is the number of tokens, @\\Theta@ denotes parameters, with subscript @x@ as token representations and @s@ as softmax layer. Note that parameters of forward and backward LM are separately maintained.\nELMo is the combination of intermediate respresentation in biLSTMs where representation set with @L@-layer biLM is given by:\n$$\r\\begin{aligned} R_k \u0026 =\\left\\{\\mathbf{x}_k^{L M}, \\overrightarrow{\\mathbf{h}}_{k, j}^{L M}, \\overleftarrow{\\mathbf{h}}_{k, j}^{L M} \\mid j=1, \\ldots, L\\right\\} \\\\ \u0026 =\\left\\{\\mathbf{h}_{k, j}^{L M} \\mid j=0, \\ldots, L\\right\\}\\end{aligned}\r$$where @\\mathbf{h}_{k, 0}^{L M}@ is the token layer and @\\mathbf{h}_{k, j}^{L M} = [\\overrightarrow{\\mathbf{h}}_{k, j}^{L M}; \\overleftarrow{\\mathbf{h}}_{k, j}^{L M}]@. Basically, EMLo concatenates hidden representation of forward and backward LSTM model by layer. Architecture shown in Figure 1.\nFig. 1 ELMo architecture (illustration form BERT)\nCollapsed ELMo representations are used in downstream NLP tasks. The author adds scale parameters @\\gamma^{\\text{task}}@ and softmax-normalized weights @s^{\\text{task}}@ for different layers:\n$$\r\\mathbf{E L M o}_k^{\\text {task }}=E\\left(R_k ; \\Theta^{\\text {task }}\\right)=\\gamma^{\\text {task }} \\sum_{j=0}^L s_j^{\\text {task }} \\mathbf{h}_{k, j}^{L M}.\r$$ELMo vector could either be added in inputs for enhanced representation @[x_k;\\mathbf{ELMo}_k]@ or be concatenated with output @[h_k;\\mathbf{ELMo}_k]@.\nFor computational requirements, the author cuts hidden dimensions to half (to 512) and incorporate residual connections from the first to second layer. CNN-BIG-LSTM trained for 10 epochs yield 39.7 on average forward and backward perplexity, with 9.7 increase compared with forward CMM-BIG-LSTM\nExperiments Tasks and datasets \u0026amp; mectrics:\nQA: SQuAD, F1 Textual entailment: SNLI, accuracy Semantic role labeling: SRL, F1 (OntoNotes) Conference resolution: OntoNotes coreference annotation, avg. F1 NER: Reuters RCV1 corpus, accuracy SST-5: , F1 Adding ELMo representations yields SOTA results, as illustrated in Figure 2.\nFig. 2 Results by adding ELMo across 6 tasks.\nWhere to add ELMo?: The author add the representation in the lowest layer in this paper yet claims that some tasks may prefer adding representation in the output of the layer.\nDifferences between layers: for tasks like Word Sense Disambiguation, last layer is better than the first layer probably because of semantic meanings in final layer. However, for tasks like POS Tagging, as structural information is needed, the first layer outperforms the last layer.\nEfficiency in sampling: In the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set. Faster convergence by adding \u0026ldquo;offsets\u0026rdquo; to vectors in high dimension space, which helps model be optimized towards optimal points efficiently?\nImproving Language Understanding by Generative Pre-Training Before reading Authors are from OpenAI (Ilya Sutskever!). Cited by 11755 (30/11/2024).\nHow time flies \u0026hellip; 6 years passed and OpenAI has grown into a renowned tech company with $3.4 billion annual revenue. GPT chat bot is well known by people around the globe and everyone can enjoy part of the bonus that AI continues to bring to our society. But Ilya and many other founders left OpenAI with a growing concern about LLM safety; AGI is coming yet it seems like an illusion given the poor performance of current LLM bots. Well, just embrace the changes and move forward and grow up with AI.\nThe paper introduced a semi-supervised approach by combining pre-training and fine-tuning. Authors also introduced a task-specific input adaption startegy for fine-tuning.\nMotivation Training on labeled data has received successful results on NLP tasks, while using unlabeled data is challenging. The optimization objective is unclear and there is no consensus on effective way of transfer learning with learnt representations.\nFramework There are two stages of training procedure: unsupervised pretraining and supervised fine-tuning. For fine-tuning, the paper introduces a task-agnostic approach to better adapt learnt representations to spcific tasks.\nUnsupervised pre-training\nClassic Transformer Decoder next word prediction with multi-head attention, FFN \u0026hellip; Next word prediction objective is given by\n$$\rL_1(\\mathcal{U})=\\sum_i \\log P\\left(u_i \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right)\r$$$$\r\\begin{aligned}\rh_0 \u0026= UW_e + W_p \\\\\rh_l \u0026= \\text{transformer_block}(h_{l - 1}), \\forall \\in [1, n] \\\\\rP(u) \u0026= \\text{softmax}(h_nW_e^{T})\r\\end{aligned}\r$$\nwhere @\\mathcal{U = \\{u_1, \\dots, u_{i - 1}\\}}@ are unsupervised tokens, parameters @\\Theta@, @W_e@ token embedding matrix, @W_p@ position embedding matrix.\nSupervised fine-tuning\nWe get labeled dataset @\\mathcal{C}@ in supervised fine-tuning, in which input tokens @x^{i}, i \\in [1, m]@ are labeled with @y@. @y@ prediction is formulated as\n$$\rP\\left(y \\mid x^1, \\ldots, x^m\\right)=\\operatorname{softmax}\\left(h_l^m W_y\\right).\r$$ Objective is given by\n$$\rL_2(\\mathcal{C})=\\sum_{(x, y)} \\log P\\left(y \\mid x^1, \\ldots, x^m\\right).\r$$In order to improve generalization and to speed up convergence, the objective is given by\n$$\rL_3(\\mathcal{C}) = L_2(\\mathcal{C}) + \\lambda \\cdot L_1(\\mathcal{C}).\r$$ Task-specific input transformations\nThe paper also introduced a task-specific strategy in fine-tuning so as to aviod making extensive changes to the model architecture across tasks. Startegy is illustrated in Figure 3. Fig. 3: (left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks. We convert all structured inputs into tokensequences to be processed by our pre-trained model, followed by a linear+softmax layer.\nSounds like structured prompt input.\nExperiments Setups: For pre-training, the paper use BooksCorpus dataset \u0026amp; 1B Word Benchmark (used by ELMo), because both datasets contains long and contigious contexts. For fine-tuning, parameters are learning rate 6.25e-5, batchsize 32, linear learning rate decay with 0.2% training warm up.\nResults: 4 downstream NLP tasks in fine-tuning: Natural Language Inference (recognizing textual entailment), Question answering and commonsense reasoning, Semantic Similarity, Classification. The approach achieved SOTA in 9 out of 12 datasets and works well on both small and large datasets\nAnalysis\nImpact of the number of transferred layers on overall performance: all layers are useful and each layer adds approx. 9% of performance increase on datasets RACE and Mutlti NLI. Zero-shot performance of pretraining models on NLP tasks: performance steadily increases as over pretraining, which suggests that generative pretraining supports the learning of a wide variety of task relevant functionality. Ablation studies: Performance without auxuliary LM objective @L_1(\\mathcal{C})@:larger dataset benefit from @L_1(\\mathcal{C})@ while smaller dataset not. Importance of using Transformer: the author compares Transformer with LSTM using the same framework. Transformer outperforms LLMs on most tasks. Importance of pretraining: performance drops when the model is trained directly on tasks without pertraining BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Before Reading Authors are from Google AI Language. Citation 120133 (30/11/2024). Paper accepted by NAACL 2019, awarded with Best Long Paper.\nThe paper introduces BERT, a bidirectional pretraining method using Transformer. The representations are learnt from left to right and right to left, which provides better representation.\nMotivation There are two strategies in applying pre-trained language representations: feature-based methods (ELMo) and fine-tuning (OpenAI GPT). However, the current approached limited power of representation bacause of the nature of learning from left to right. BERT uses bidirectional training and applied two novel pretraining objectives, namely MLM and NSP, to get pretraininig representations of both token-level and sentence-level. BERT also reduce the need of task-specific archituctures.\nBERT Training: There are two steps of BERT: pre-training and fine-tuning. During pre-training, BERT utilize two objectives to get pre-training representations. Fine-tuning is firstly initialized with pre-trained parameters and all of the parameters are fine-tuned.\nArchitecture: BERT is basically a multi-layer bidirectional Transformer encoder, which is different from GPT constrained by left-to-right nature. (In my opinion, bidirectional is mostly illustrated by attention mechanism in encoder)\nInput/Output Representations: In BERT, the input sequence might be a single sentence or a pack of two sentences. The first token is always [CLS] and seperation token between two sentences is [SEP]. For two sentences, the author add learned segment embedding @E_A, E_B@ to mark tokens in two sentences @A@ and @B@. The final input is the summation of token, segment embedding and position embedding, as illustrated in Figure 4.\nFig. 4 BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\nPre-training:\nMasked LM (MLM): Because of bi-directional feature of BERT, each word would see itself, therefore we need a new pre-training objective. Inspired by Cloze task, the author decided to randomly mask out 15% tokens in each sequence by replacing it with token [MASK]. However, since token [MASK] will not appear in fine-tuning stage after pre-training, there is a mismatch between pre-training and fine-tuning tokens. The authoer then elaborates on the detailed approach of setting [MASK]: within 15% tokens, 80% tokens are replaced by [MASK], 10% tokens are replaced by a random token and the rest stay unchanged. Next Sentence Prediction (NSP): A simple binarized task of deciding whether the sentence @B@ is the next sentence of sentence @A@ in sentence pair @[A, B]@. BERT thus learned sentence-level information. Pre-training data: document-level literature with long contexts, such as BooksCorpus and English Wikipedia in order ot extract contiguous sequences.\nFine-tuning: Plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. Sentence @[A, B]@ could be interpreted as different meanings like QA, hypothesis-premise pairs etc..\nExperiments Tested on four different tasks:\nGLUE: last hidden state + classification weights + softmax SQuAD v1.1: QA pairs, predict on the answer span index @[i, j]@ SQuAD v2.0: The answer probably does not exists in contexts. No answer -\u0026gt; span from [CLS] to [CLS]. The rule of SQuAD also applies. SWAG: Given a sentence, the task is to choose the most plausible continuation among four choices. Ablations Effect of Pre-training tasks: experiments on No NSP, LTR(left2right)\u0026amp;No NSP removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1 The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. Effect of Model Size: scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. We hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small. Ablation w/o fine-tuning Pre-computed representations lower down the costs BERT is effective for both finetuning and feature-based approaches (only pre-training). RoBERTa: A Robustly Optimized BERT Pretraining Approach Before Reading 17176 citation up to 01/09/2025. It\u0026rsquo;s a follow-up work of BERT, in which the author introduced better settings for BERT model training.\nMotivation The RoBERTa proposed an improved receipe for training BERT models. Main settings are training time, size of batches, elimination of NSP training, longer sequence and dynamic masking patterns. The results showed improved performance on metrics in BERT.\nTraining Settings of BERT BERT is optimized with Adam with @\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon=1e-6@ and @L_2@ weight decay of @0.01@ (warm up 10000 steps to 1e-4 and linearly decayed). Dropout rate 0.1 on all layers. GELU activation. Models trained for 1000000 updates with 256 as batchsize and max-length 512. Models are trained with mixed precision floating point, 8xV100.\nTraining data includes BOOKCORPUS, CC-NEWS, OPENWEBTEXT and STORIES.\nTraining Analysis Dynamic masking and static masking: To avoid using the same mask for each epoch, the training data were duplicated 10 times and were masked with different ways for each epoch. This was introduced in BERT and called static masking. While for dynamic masking, masking patterns are generated every time we feed a sequence to the model. And \u0026hellip; as the results presented, we indeed see the increase though being marginal.\nNext Sentence Prediction: NSP loss was questioned by replication experiments. The authors found:\nUsing individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies. Removing the NSP loss matches or slightly improves downstream task performance. Restricting sequences to come from a single document performs slightly better than packing sequences from multiple documents Training with large batches: Training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training.\nText Encoding: Train BERT model using Byte-Pair Encoding. RoBERTa Training settings Three points: Dynamic masking, trained with FULL-SENTENCES dataset without NSP loss, large mini-batches and byte-level BPE. More settings revolves around data used for pretraining and number of passes through the data.\nThe author further conbimed three datasets for training (160GB) and trained the model from 100K to 500K steps.\nEvaluations Models are evaluated on GLUE, SQuAD and RACE.\nGLUE\nThere are 2 types of tasks: single-task and emsembled task in GLUE. RoBERTa was finetuned for single task on each training dataset based on pretrained model. And for ensembled task, RoBERTa did not depend on multi-task finetuning. Instead, for RTE, STS and MRPC, the model was fine-tuned on MNLI single-task model.\nSQuAD\nRoBERTa finetuned only on SQuAD training data without data augmentation like previous works. The single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation.\nRACE\nEach candidate answer was concatenated with the corresponding question and passage. The total length is at most 512 tokens.\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators Before Reading Accepted by ICLR 2020. Authors are from Stanford and Google Brain (Manning!). Citation 4424 up to 01/23/2025.\nMajor improvements on pretraining with MASK (Masked language modeling task, MLM) in BERT. Instead of training with fixed [MASK] token, ELECTRA predicts whether the word is replaced by generator or not, which means all tokens in the input will be considered as prediction objectives.\nMotivation In MLM task, only 15% of the tokens are learnt by the model as the number of masked tokens are limited. In this work, the author proposed a pretraining task replaced token detection, in which the model learns to distinguish real input tokens from generated replacements. The side-product of this setting is that it solves mismatch between training and testing in BERT. (Remember 15%-80%-10%-10%?) Note that the model is seen as a generator that predicts the original identity of corrupted tokens. Moreover, the ELECTRA also features with compute-efficiency and parameter-efficiency in pretraining stage.\nMethod There are two NNs in this work, namely Generator and Discriminator. The generator is in charge of putting mask on input sentence and generating corrupted sentence by replacing masks with other words. The discriminator then tries to distinguish which word in the corrupted sentence is replaced by Generator. In Generator mask words @m_i@, which follows uniform distribution:\n$$\rm_i \\sim \\operatorname{unif}\\{1, n\\} \\text{ for } i=1 \\text{ to } k \\quad \\mathbf{x}^{\\text {masked }}=\\operatorname{REPLACE}(\\mathbf{x}, \\mathbf{m},[ \\text{MASK} ])\r$$\nwhere @\\text{REPLACE(\\mathbf{x}, \\mathbf{m}, p)}@ means replace masked elements in @\\mathbf{x}@ with p using @\\mathbf{m}@ as mask. @h@ are hidden representations and @e@ are embeddings of generator encoder. Then the masked elements @m_i@ are replaced with new words @\\hat x_i@, which follows the distribution given by softmax normalization:\n$$\r\\begin{aligned}\r\\hat{x}_i \u0026\\sim p_G\\left(x_i \\mid \\mathbf{x}^{\\text { masked }}\\right)\\text{ for } i \\in \\mathbf{m} \\\\\rp_G\\left(x_t \\mid \\mathbf{x}\\right)\u0026=\\exp \\left(e\\left(x_t\\right)^T h_G(\\mathbf{x})_t\\right) / \\sum_{x^{\\prime}} \\exp \\left(e\\left(x^{\\prime}\\right)^T h_G(\\mathbf{x})_t\\right) \\\\\r\\mathbf{x}^{\\text {corrupt }}\u0026=\\operatorname{REPLACE}(\\mathbf{x}, \\mathbf{m}, \\hat{\\mathbf{x}}) \\end{aligned}\r$$The corrupted input @\\mathbf{x}^{\\text{corrupt}}@ is the input of Discriminator, which tries to distinguish the word replaced by Generator. The possibility for each word is given by:\n$$\rD(\\mathbf{x}^{\\text {corrupt}}, t) = \\text{sigmoid}(w^{T}h_{D}(\\mathbf{x}^{\\text {corrupt }})_t)\r$$\nThe loss function is the combination of MLM task and discrimination task. Figure 4 illustrates ELECTRA using an example.\nFig 4. An overview of replaced token detection. The generator can be any model that producesan output distribution over tokens, but we usually use a small masked language model that is trainedjointly with the discriminator. Although the models are structured like in a GAN, we train thegenerator with maximum likelihood rather than adversarially due to the difficulty of applying GANsto text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks.\n$$\r\\begin{aligned}\r\u0026\\min _{\\theta_G, \\theta_D} \\sum_{\\mathbf{x} \\in \\mathcal{X}} \\mathcal{L}_{\\mathrm{MLM}}\\left(\\mathbf{x}, \\theta_G\\right)+\\lambda \\mathcal{L}_{\\text {Disc }}\\left(\\mathbf{x}, \\theta_D\\right) \\\\\r\u0026 \\mathcal{L}_{\\mathrm{MLM}}\\left(\\mathbf{x}, \\theta_G\\right)=\\mathbb{E}\\left(\\sum_{i \\in \\mathbf{m}}-\\log p_G\\left(x_i \\mid \\mathbf{x}^{\\text {masked }}\\right)\\right) \\\\ \u0026 \\mathcal{L}_{\\mathrm{Disc}}\\left(\\mathbf{x}, \\theta_D\\right)=\\mathbb{E}\\left(\\sum_{t=1}^n-\\mathbb{1}\\left(x_t^{\\mathrm{corrupt}}=x_t\\right) \\log D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)-\\mathbb{1}\\left(x_t^{\\text {corrupt }}f \\neq x_t\\right) \\log \\left(1-D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)\\right)\\right)\r\\end{aligned}\r$$ Disc loss: cross-entropy loss for discrimination.\nDifference with GAN If the generated token happens to ben correct, the token will be considered \u0026ldquo;real\u0026rdquo; instead of \u0026ldquo;fake\u0026rdquo;. The generator is trained with maximum likelihood rather than being trained adversarially to fool the\ndiscriminator. (Ad training is challenging because of it is impossible to backpropergate through sampling from generator.) Experiments Datasets are GLUE, SQuAD. EM and F1 scores.\nModel extension: Some techiques used in initialization and training. Weight sharing: share all/parts of parameters between generator and discriminator Smaller generators: large models generates challenging tasks for discriminator, and sometimes being too hard to answer by discriminator. Smaller generator works effectively. (small model here: keep some of params in generator constant without updating in BP) Training algorithms: Two stage procedure: training MLM task for n steps; initialize params in discriminator using params in trained generator. Train the discriminator with generator frozen. The author also discusses about the small and large ELECTRA models using weaker training hyperparameters. Further discussions are about the efficiency of ELECTRA. Thr author designed three variations to test token learning in ELECTRA. Results suggest that a large amount of ELECTRA’s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. (btw proves 10% random replacement in BERT is insufficient to solve the issue).\nReferences [1] Tsang, S. (2022, January 8). Review — ELMO: Deep Contextualized Word Representations. Medium. https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c\n[2] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., \u0026amp; Zettlemoyer, L. (2018, February 15). Deep contextualized word representations. arXiv.org. https://arxiv.org/abs/1802.05365\n[3] Radford, A. (2018). Improving language understanding by generative pre-training.\n[4] Devlin, J., Chang, M., Lee, K., \u0026amp; Toutanova, K. (2018, October 11). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv.org. https://arxiv.org/abs/1810.04805\n[5] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., \u0026amp; Stoyanov, V. (2019, July 26). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv.org. https://arxiv.org/abs/1907.11692\n[6] Clark, K., Luong, M., Le, Q., V., \u0026amp; Manning, C. D. (2020, March 23). ELECTRA: Pre-training text encoders as discriminators rather than generators. arXiv.org. https:// arxiv.org/abs/2003.10555\n","permalink":"http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/","summary":"\u003cp\u003e\u003ca href=\"https://www.cs.princeton.edu/courses/archive/fall22/cos597G/\"\u003eHomepage\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"elmo-deep-contextualized-word-representations\"\u003e(ELMo) Deep contextualized word representations\u003c/h2\u003e\n\u003ch3 id=\"before-reading\"\u003eBefore Reading\u003c/h3\u003e\n\u003cp\u003eAuthors are from \u003ca href=\"https://allenai.org/\"\u003eAI2\u003c/a\u003e and \u003ca href=\"https://www.cs.washington.edu/\"\u003eUW\u003c/a\u003e. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation.\u003c/p\u003e","title":"COS597G 22 Encoder Only Models"},{"content":" Homepage\nHuman Language Understanding \u0026amp; Reasoning Introductory reading authored by Christopher D. Manning.\nBrief introduction of NLP history The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms.\nThe second era was from 1970 to 1992 and systems were able to deal with syntax and reference in human language. The new generation of hand-built systems had a clear separation between declarative linguistic knowledge and its procedural processing and which benefited from the development of a range of more modern linguistic theories.\nNLP dramatically changed in the third era, 1993 - 2012 because of the emergence of digital text. At the beginning, researchers tend to extract certain model from a large corpus of data by counting certain facts. Early attempts to learn language structure from text collections were fairly unsuccessful, which led most of the field to concentrate on constructing annotated linguistic resources. Supervised machine learning dominates NLP techniques.\nThe last era features with deep learning and growing artificial intelligence methods. Word \u0026amp; sentence embedding went viral. From 2013 to 2018, deep learning promotes the advantages of embedding thus leading NLP techiques to vector spaces. In 2018, very large scale self-supervised neural network succeeded in learning an enormous amount of knowledge by simly exposed to large contexts. Representative tasks are net word prediction and filling masked words or phrases.\nNow-dominant neural network Since 2018, the dominant neural network model for NLP applications has been the transformer neural network. The dominant idea is one of attention, by which a representation at a position is computed as a weighted combination of representations from other positions. Masked word prediction turns out to be very powerful because it is universal: every form of linguistic and world knowledge, from sentence structure, word connotations, and facts about the world, help one to do this task better. As a result, these models assemble a broad general knowledge of the language and world to which they are exposed.\nWhat can we do with LPLMs? Multilingual machine translation trained on all languages simutaneously; for other tasks like QA, sentiment classification, NER and fluent text generation, LPLMs turns out to be the best solution.\nProspects What\u0026rsquo;s the meaning in contexts? The dominant approach to describing meaning is a denotational semantics approach or a theory of reference: the meaning of a word, phrase, or sentence is the set of objects or situations in the world that it describes. This contrasts with the simple distributional semantics (or use theory of meaning) of modern empirical work in NLP, whereby the meaning of a word is simply a description of the contexts in which it appears. Manning claims that meaning arises from understanding the network of connections between a linguistic form and other things, whether they be objects in the world or other linguistic forms. Using this definition whereby understanding meaning consists of understanding networks of connections of linguistic forms, there can be no doubt that pretrained language models learn meanings. As well as word meanings, they learn much about the world.\nOne of the exciting prospects is learning from multi modal data, such as vision, robotics, knowledge graphs, bioinformatics, and multimodal data. Manning also mentions external database as the source of model while he still addresses the importance of multi-modal learning.\nWe will witness the comming of foundation models, with its specializations handling most information processing and analysis tasks. There might be concerns of risks that foundation models are controlled by several powerful and influencial groups and somehow it will\nbe difficult to tell if models are safe to use in particular contexts because the models and their training data are so large. Manning believes in the limitation of models while also gives postive comments on their utility and foresees the future that models are widly deployed.\nAttention is All You Need Transformer architecture is firstly introduced in this work.\nBefore Reading Attention is All You Need is well known for its contribution of Transformer architecture and therefore probably be seen as the inception of LLM era. 14k citations well demonstrate its significance. Authors are from Google Brain team or UofT. All of the authors shared the same contribution. Paper was accepted by NIPS 2017.\nIntroduction RNNs established SOTA approaches in sequence modeling while fell short of efficiency because of its non-parallizable computation. Previous attention mechanisms attempted to solving the problem yet only in few cases or by combining RNNs. The author proposed Transformer architecture to deal with parallelization by relying entirely on an attention mechanism.\nBackground Previous attempts on reducing sequential computation is to use convolutional neural networks. Though Convs proved efficient, the operations to relate signals from different positions grows either linearly or logarithmically. Transformer coinstrains the number of operations to constant and counteract resolution cost by applying Multi-head Attention. Self-attention performs well on a wide range of tasks and Transformer relies on self-attention without combining with RNNs.\nModel Architecture Key concepts: encoder-decoder structure, stacked self-attention, fully connected layers\nThe Transformer - model architecture.\nEncoder\nEncoder is composed of 6 identical stacked layers which contains 2 sub-layers. Residual connection are employed around each of the two sub-layers, followed by layer normalization. Sub-layers are multi-head self-attention layer and a simple, position-wise fully connected feed-forward network. Output dimension @d_{\\text{model}} = 512@\nDecoder\nDecoder is composed of 6 identical stacked layers. Self-attention sub-layer is modified by applying mask, which prevent the model from attending to subsquent positions.\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nScaled Dot-Product Attention\nTwo common attention functions: additive attention and dot-product attention. The difference is compatibility function. Additive attention use a feed-forward network with a single hidden layer while dot-product attention use Query and Key, which resembles to attention mechanism introduced in the paper. The two are similar in complexity but the latter could be optimized by matrix multiplicatoin, thus being more space-efficient in practice.\nScaled Dot-Product Attention:\n$$\r\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V\r$$For larger values of @d_k@, result of @QK^T@ is going to be large, which falls into @\\text{softmax}@ regions where its has extremely small gradients. Therefore, scaling is added. Notice that the scaling factor is the dimension of Key: @1 / \\sqrt{d_k}@\nMulti-Head Attention Instead of performing a single attention function with @d_{\\text{model}}@-dimensional keys, values and queries, it is better to linearly project the queries, keys and values @h@ times with different, learned linear projections to @d_k@, @d_k@ and @d_v@ dimensions, respectively. Computation is carried out in parallel and output values are concatenated and once again projected.\n(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\nPerform linear output transformation after concatenation.\n$$\r\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_i)W^O\r$$where @\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^{K}, VW_i^{V})@. Dimensions (Sequence length @n@): @K \\in \\mathbb{R}^{n \\times d_{\\text{model}}}, W_{i}^K \\in \\mathbb{R}^{d_{\\text{model}}\\times d_k},@ @ W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}@. In the paper @h = 8, d_k = d_v = d_{\\text{model}} / h = 64@. @d_{\\text{model}}@ could be seen as dimension of embedding.\nApplications of Attention In deocoder, query comes from last layer while key and values comes from encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Mask is implemented inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\nPosition-wise FFN FFN consists of two linear transformation with ReLU activation n between.\n$$\r\\text{FFN} = \\text{ReLU}(xW_1 +b_1)W_2 + b_2\r$$Dimensions: inner dimension 2048, output dimension 512.\nEmbeddings and Softmax The implemetation share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by @\\sqrt{d_{\\text{model}}}@.\nPositional Embedding Sine and cosine functions:\n$$\r\\begin{aligned}\rPE_{(pos, 2i)} \u0026= \\sin(pos / 10000^{2i / d_{\\text{model}}}) \\\\\rPE_{(pos, 2i + 1)} \u0026= \\cos(pos / 10000^{2i / d_{\\text{model}}})\r\\end{aligned}\r$$\nwhere @pos@ is the position and @i@ is the dimension. The author also tries positional embeddings, which yields similar results. But sin/cos PE could extrapolate to sequence longer inputs.\nWhy Self-Attention Total computational complexity per layer Type Complexity per Layer Self-Attention @O(n^2 \\cdot d)@ Self-Attention(restricted) @O(r \\cdot n \\cdot d)@ Recurrent @O(n \\cdot d^2)@ Conv @O(k \\cdot n \\cdot d^2)@ where @d@ is the representation dimension and @n@ is the length of sequence. Self-attention does not perform well when @n \u003e d@ compared with RNNs. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size @r@ in the input sequence centered around the respective output position.\nThe amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The path length between long-range dependencies in the network. More interpretable models by investigating attention values. Training Experiments on machine translation: WMT 2014 English-German/French. 8xP100 GPU, Adam optimizaer, regularization: residual dropout \u0026amp; label smoothing. Metrics: BLEU, Trainig cost(FLOPs)\nResults BLEU: EN-DE 28.4 EN-FR 41.8\nEnglish constituency parsing for checking ability of task generalization.\nAblation Number of attention heads Attention key size @d_k@ Model size Other positional embeddings Blog Post: The Illustrated Transformer Visualizing QK In encoder, @QK@ matrix mutiplication could be seen as quries multiplying keys, then get summation.\nVisualization of Encoder self-attention.\nEncoder-Decoder Attention Detailed version of encoder-decoder architecture:\nVisualization of encoder-decoder architecture\nWhat happened between encoder and decoder: encoder-decoder architecture gif:\nEncoder-decoder attention\nPosition Embedding Visualization Position Embedding Visualization\nx-axis: Embedding dimension; y-axis: Token position.\nAbout model outputs Because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Parameter temperature can affect output as well by adding a scaling factor: @\\text{softmax(x / T)}@.\nReferences [1] C. D. Manning, \u0026ldquo;Human Language Understanding \u0026amp; Reasoning,\u0026rdquo; journal-article, 2022. [Online]. Available: https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf\n[2] A. Vaswani et al., \u0026ldquo;Attention Is All You Need,\u0026rdquo; arXiv.org, Jun. 12, 2017. https://arxiv.org/abs/1706.03762\n[3] J. Alammar, \u0026ldquo;The Illustrated Transformer.\u0026rdquo; https://jalammar.github.io/illustrated-transformer/\n","permalink":"http://localhost:1313/learning/llm/cos597g-22-introduction/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://www.cs.princeton.edu/courses/archive/fall22/cos597G/\"\u003eHomepage\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"human-language-understanding--reasoning\"\u003eHuman Language Understanding \u0026amp; Reasoning\u003c/h2\u003e\n\u003cp\u003eIntroductory reading authored by \u003ca href=\"https://nlp.stanford.edu/~manning/\"\u003eChristopher D. Manning\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"brief-introduction-of-nlp-history\"\u003eBrief introduction of NLP history\u003c/h3\u003e\n\u003cp\u003eThe NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms.\u003c/p\u003e","title":"COS597G 22 Introduction"},{"content":"Overview An inspiring book in changing the personal finance view of the readers. The wealth never comes easy and especially tougher in the era of Rat Race where individuals exploit themselves to get the salary as the rewards of their \u0026ldquo;diligence\u0026rdquo;. The deeply rooted view held by majorities and me, before I encountered with the book, is subverted by Robert T.Kiyosaki in Rich Dad Poor Dad. Working in Rat Race provides us with stable or growing income and, at the same time, turns the master of money \u0026ndash; us \u0026ndash; into slaves who is fettered by conventional mindset of personal finance. The book enlightened readers with the way out of Rat Race and secrets of building personal wealth.\nCurrently I am at my transition from an college student detached from society to an adult who confronts with real world problems. And the primary concern of my first step out of college comfort zone is the topic of money. Unfortunately, I did not ever have any common sense with personal finace and it was actually kind of overwhelming when I received salary from my first internship. Therefore, out of concerns and curiosity, I searched for books and videos for finance management online and that is when I came across with Rich Dad Poor Dad. And I was glad the book comes with a plain yet powerful style in writting, which is accessible even to students in high school, nevertheless presents us with insights into personal wealth.\nThe book starts with education from \u0026ldquo;Rich\u0026rdquo; dad and \u0026ldquo;Poor\u0026rdquo; dad, which represents two types of views on money and wealth. The poor and middle class stick to salary and liabilities and reluctant to take the risk of investments on assets while the rich generate money with better arranged cash flow between income and assets, which is described as \u0026ldquo;Each dollar is employeed by you and work for you 24 hour 7 days a week\u0026rdquo;. The stocks, bonds, real states and the business run by your agents work for the owner without their presence onsite. Though simple as it may sounds, coming up with the way of making the money work for you is never easy and that is exactly where finacial intelligence works. Finacial Literacy and Finacial Intelligence are both the prerequisites for obtaining wealth or being finacially independent. The author suggests accounting, investing, understanding the market and learning the law are four technical pillars of financial intelligence and their synergy will be of great help in the pursuit of wealth. Apart from technical skills, psychology of investing is another important factor in personal finance action. Until we have overcome fear, cynicism, laziness, arrogance and get bad habits kicked off, we could develop our assets columns which will bring us with large cash flow.\nLots of bloggers and youtubers recommand the book as the best introductory reading material of personal finance and my reading experience attest to their applause as an accessible, powerful and thought-provoking book designed for financial novices.\nReview by Chapter:\nLesson 1: The Rich Don\u0026rsquo;t Work For Money The poor and the middle class work for money. The rich have money work for them. People\u0026rsquo;s lives are forever controlled by two emotions: fear and greed. So many people say, \u0026ldquo;Oh, I\u0026rsquo;m not interested in money.\u0026rdquo; Yet they\u0026rsquo;ll work at a job for eight hours a day. Lesson 2: Why Teach Financial Literacy It\u0026rsquo;s not how much the money you make. It\u0026rsquo;s how much money you keep. Rich people acquire assets. The poor and middle class acquire liabilities that they think are assets. You must know the difference between an asset and a liability, and buy assets. Cash flow tells the story of how a person handles money. Observations: The rich buy assets. The poor only have expenses. The middle class buy liabilities they think are assets. Rule #1: You must know the difference between an asset and a liability, and buy assets.\ncash-flow pattern of an asset:\nCash-flow pattern of an asset\nCash-flow pattern of liability\nCash-flow pattern of the poor\nCash-flow pattern of the middle class\nFinancial statements\nLesson 3: Mind Your Own Bussiness The rich focus on their asset columns while everyone else focuses on their income statements. Financial struggle is often the result of people working all their lives for someone else. Keep your daytime job, but start buying real assets, not liabilities or personal effects that have no real value once you get them home. Real assets fall into the following categories: Businesses that do not require my presence Stocks Bonds Income-generating real state Notes(IOUs) Businesses that do not require my presence Royalties from intellectual property such as music, scripts, and patents Anything else that has value,produces income or appreciates, and has a ready market Lesson 4: The History of Taxes and the Power of Corperations My rich dad just played the game smart, and he did it through corporations—the biggest secret of the rich. My rich dad did not see Robin Hood as a hero.He called Robin Hood a crook. If you work for money, you give the power to you employer. If money works for you, you keep the power and control it. If you work for money, you give the power to you employer. If money works for you, you keep the power and control it. Financial IQ is made up of knowledge from four broad areas of expertise: Accounting Investing Understanding markets The law Tax advantages Protection from lawsuits Corporate structure\nLesson 5: The Rich Invent Money Games reflect behavior. They are instant feedback systems. The single most powerful asset we all have is our mind. If it is trained well, it can create enormous wealth. The single most powerful asset we all have is our mind. If it is trained well, it can create enormous wealth. It is not gambling if you know what you’re doing. It is gambling if you’re just throwing money into a deal and praying. Great opportunities are not seen with your eyes. They are seen with your mind. If you want to be the investor that creates invenstments, you need to develop three main skills: If you want to be the second type of investor,you need to develop three main skills. Raise money. Organize smart people. The single most powerful asset we all have is our mind. If it is trained well, it can create enormous wealth.\nLesson 6: Work to Learn - Don\u0026rsquo;t Work for Money Job security meant everything to my educated dad. Learning meant everything to my rich dad. \u0026ldquo;You want to know a little about a lot\u0026rdquo; was rich dad’s suggestion. Job is an acronym for \u0026ldquo;Just Over Broke.\u0026rdquo; The main management skills needed for success are: Management of cash flow Management of systems Management of people Lesson 7 Overcoming Obstacles Fear: The primary difference between a rich person and a poor person is how they manage fear. For most people, the reason they don\u0026rsquo;t win financially is because the pain of losing money is far greater than the joy of being rich. Failure inspires winners. Failure defeats losers. FOCUS: Follow One Course Until Successful. Cynicism: Doubt is expensive. Laziness So what is the cure for laziness? The answer is—a little greed. Rich dad believed that the words \u0026lsquo;I can’t afford it\u0026rsquo; shut down your brain. \u0026lsquo;How can I afford it?\u0026rsquo; opens up possibilities, excitement, and dreams. Bad Habits Arrogance Lesson 8 Getitng Started There is gold everywhere. Most people are not trained to see it. 10 steps as a process to develop your God-given powers, powers over which only you have control. Find a reason greater than reality: the power of spirit Make daily choices: the power of choice Choose friends carefully: the power of association Master a formula and then learn a new one: the power of learning quickly Pay yourself first: the power of self-discipline To successfully pay yourself first, keep the following in mind: Don’t get into large debt positions that you have to pay for. When you come up short, let the pressure build and don’t dip into your savings or investments. Pay your brokers well: the power of good advice Be an Indian giver: the power of getting something for nothing The sophisticated investor’s first question is:“How fast do I get my money back?” They also want to know what they get for free, also called a “piece of the action.” That is why the ROI, or return on investment, is so important. I move a sizable amount of money into the stock of a company that he feels is just about to make a move that will add value to the stock, like announcing a new product. I will move my money in for a week to a month while the stock moves up.Then I pull my initial dollar amount out, and stop worrying about the fluctuations of the market, because my initial money is back and ready to work on another asset. So my money goes in, and then it comes out, and I own an asset that was technically free. Use assets to buy luxuries: the power of focus Choose heroes: the power of myth But heroes do more than simply inspire us.Heroes make things look easy. Making it look easy convinces us to want to be just like them. If they can do it, so can I. Teach and you shall receive: the power of giving Still Want More? Here Are Some To Do\u0026rsquo;s Stop doing what you’re doing. Stop doing what is not working, and look for something new. Look for new ideas. Find someone who has done what you want to do. Take them to lunch and ask them for tips and tricks of the trade. Take classes, read, and attend seminars. Make lots of offers. More suggestions:\nFinding a good deal, the right business, the right people, the right investors, or whatever is just like dating. Jog, walk, or drive a certain area once a month for 10 minutes. Shop for bargains in all markets. Look in the right places. Look for people who want to buy first.Then look for someone who wants to sell. Think big. Learn from history. Action always beats inaction. Three forms of income:\nOrdinary earned Portfolio Passive All of you were given two great gifts: your mind and your time. It is up to you to do what you please with both. I wish you great wealth and much happiness with this fabulous gift called life. – Robert Kiyosaki\nMore Reading More books recommanded (credit to Youtuber 艾财说imoneytalk) CashFlow game ","permalink":"http://localhost:1313/ideas/rich-dad-poor-dad-review/","summary":"\u003ch3 id=\"overview\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eAn inspiring book in changing the personal finance view of the readers. The wealth never comes easy and especially tougher in the era of Rat Race where individuals exploit themselves to get the salary as the rewards of their \u0026ldquo;diligence\u0026rdquo;. The deeply rooted view held by majorities and me, before I encountered with the book, is subverted by Robert T.Kiyosaki in \u003cem\u003eRich Dad Poor Dad\u003c/em\u003e. Working in Rat Race provides us with stable or growing income and, at the same time, turns the master of money \u0026ndash; us \u0026ndash; into slaves who is fettered by conventional mindset of personal finance. The book enlightened readers with the way out of Rat Race and secrets of building personal wealth.\u003c/p\u003e","title":"Rich Dad Poor Dad Review"},{"content":"我和 7 这个数字很有缘分。\n初高中时每次考试分发座位表，我都会第一时间去查看自己的班级号和座位号。在我有限的印象里，我的号码总是与 7 这个数字有着各种联系：7？17？14？21？考试时看到这个并不和谐的质数总是会点亮一下我的心情，这个在数字文化里被冷落的符号对我来说有一种天然的亲近感。\n我不太记得哪个网站说过，七年是全身细胞更新的周期，在生理上，我相对于七年之前的自己是一个全新的存在。我知道每一个器官组织的更新速率不同，但是总要选一个平均值的上限来代表一下这个过程，一个带着点宿命味道的数字是再好不过了。我倒是很喜欢这个不长不短的时间，还带着点儿初高中回忆的味道。今年 21 岁，正好经过了三次更新迭代，兢兢业业工作的器官组织们都至少走过了三个轮回。在十多个轮回中，三个七年仅仅是三分之一的时光，在这个节点上去回顾这短短的二十一年未免有些做作，但是简单回头去看看过去的自己倒是一件蛮有趣的事情。\n在第一个七年的节点我在上二三年级，刚刚从一楼带着铁栅栏的教室升入二楼的小教室里。四班在走廊的堵头，挨着小水池倒还有一点乐趣。夏天的时候班上总有几个不怕挨打的小孩拿着灌好的水气球扔来扔去，我是不太去玩这种水气球炸弹，但是也不妨碍我在旁边观摩几员猛将的拼杀，有时候也会被误伤就是了。被告知不好的事情坚决不碰是我小小的脑袋瓜里的唯一哲学，我好像也是家长和老师面前的好孩子。我记得最骄傲的事情是被选上写字课代表，每周抱着全班的练字本在座位上用红笔批改。写字本沉甸甸的，放在桌膛里边整整齐齐，也是一个小小的成就。好孩子的标准是不说脏话不吃辣条不到处乱跑，但是从根本上说这些有点坏的习惯对我来说从来没有存在的必要。每天的生活对我这个地球玩家新手来说仍然很明亮，这个小镇好大，我怎么也探索不完每一个有趣的角落。我记得端午节和家人去山上采带着露珠的艾草，下完雨去找松树墩旁边的小蘑菇，秋天躺在金黄的落叶床垫上闻一闻泥土的腥味，每次回来那种筋疲力竭躺在床上的感觉总是能让我很满足。放学路上我看到路尽头金红色的小山丘披着西斜的阳光，我总是感觉到一股暖流腾起。从林子里回来，我应该也裁了一片小小的阳光留在我心里了吧。学校微机室的大肚子电脑、夏天三十九度的教室里凉凉的红色小水桶、长满齐腰高杂草的神秘小水沟、查理九世的青铜棺和游戏王的三幻神好像都在回忆的黑柔滤镜下变得有点不太真实。\n在第二个七年的节点我已经是初三的学生了。在转学之后的教育体系中，小学只有五年而初中有四年的时间。三年时间对于一个十多岁的毛头小子来说很漫长，我只记得分数从及格线爬到了班级前几，我从一个小透明也似乎变成了有点存在感的小角色了。关于初三一半的记忆是灰色纸的试卷，堆成山的作业试题和每周每一天考试的心惊胆战。我们学校的学习强度还是比较有名的，每一周对于数学英语物理化学必定会有标准考试，不定时语文也会占用时间来考试，我就在这每一周的循环中经历心跳和肾上腺素的波动起伏。我记得初三的我总是有那种莫名的恐惧，生怕被别人超过分数或者名次，于是在作业练习里边疯狂训练，每一周的考试都当作证明自己的机会一样极为重视。结果当然不会辜负这样“自残”一样的努力，我的名字也能和那些被大家誉为“大神”的角色写在一起了，老师和同学们似乎也比以前温和了许多，我好像获得了阶段性的成功。是吗？我好像更加敏感了，那些成绩和排名兜兜转转都离不开周围的评价。至于对我自己我可能自始至终都没有认知，我只知道在半夜一点刷完这本习题册能让我在下次考试当中多一点胜算，分数高一点这样我也可以高兴地度过这周余下的几天，然后再迎接下一周的血压飙升。在成绩起伏大时候我会自己报复自己，不需要老师督促，毕竟我是自诩的“好孩子”嘛。怕落在别人后边是我唯一的想法。关于初三的另一半呢是一位女生。我记得有一段时间是坐在我的斜后方，她平时比较内向，我们的交集也并不是很多。初三学生的喜欢总是停留在对只言片语和微小的动作理解上，有些幼稚的行为现在想想也是颇有趣的。有时候多交谈几句能够开心好几天；悄悄地把她的QQ设为特别关心，在节日打了很多字删掉送上最普通的祝福；给她讲题时略带兴奋但也小心翼翼克制着不想让对方发现\u0026hellip;\u0026hellip;我记得有一次同学打赌，比写练习册的页数，页数少的人给某某表白，我表白的对象正好是她。我怎么都是不好意思的，于是在当天晚上写了八十多页练习册，是对方的八倍多\u0026hellip;\u0026hellip;哈哈哈这些事情真的是幼稚到极点，比安昙小太郎和水野茜的初期发展还要缓慢，但是相信我那时能够每次上学看到她就已经很开心了吧。我不会形容初三学生双向暗恋的感觉，我只知道当时的世界对于七年前的我来说还是太过复杂，这两种记忆交织着描绘了七年前的我的生活，混着铅灰色和玫瑰色的色彩。七年前的我还好吗？\n第三个七年的结尾是在刚上大四的秋天，我带着七年的回忆来到北京开始了很不顺利的实习生活。高中的三年有太多太多情感，我永远不会忘记三年中的很多瞬间，那是第三节人生中最最珍贵的三年。紧接着是高考砸锅、转专业成功、专业学习长征、科研阻力和长时间的焦虑与迷茫。奇怪的是，这七年我感觉被按下了加速键，我找不到在时间轴上被偷走的那几年。那几段时间就那么堂而皇之地溜走了。有留下什么吗？我倒是觉得大学三年所有情绪的重量都留下了，有一小部分开心的，有大部分焦虑的，也有一部分是无所谓的。大三课程结束我终于意识到大学生活这场漫长的跋涉竟然快走到了尽头，那个日夜埋头赶路的自己应该会很欣慰吧。七年旅途的末尾有上低音号的尾音，有实验没有起色的数字，有utown的落日和晚霞。我倒是又捡起了相机去拍拍星轨，拿起标日初级看看泡温泉的小李，看完了之前总是想查但是总忘记的导演的电影，给自己留出近乎奢侈的时间来思考一些天马行空的问题。我也会躺在床上刷上几个小时的 bilibili，在屏幕前对着新老 cp 傻笑，闷头一觉睡到中午。但是也许现在这个处在七年节点的我也可以说，在这第三节里我的生活过的实实在在。\n我昨天看了小坂流加老师作品改编的 餘命10年 电影，如果单从剧情的角度这部电影没有什么新奇，甚至可以说某些情节有一些老套和不合理。但是每次看到茉莉挣扎着在最后的日子里生活时我却总能被感触到，那种在存活百分制概率下生活的希望和动力让我有些触动。小坂流加老师没能看到作品上映，但是我能够想到在病床前写下这些文字的情景，那种强烈的想要延展生活的渴望。在过去两年里，我有些时候觉得我患上了虚无感的绝症，在各种压力的催化下这种症状更加明显，恶性循环之中导致了无语和迷茫。我弄不明白这生活热情到底是什么，跟着环境亦步亦趋也许是折衷之策吧，毕竟大学三年我都是这样。所以看到茉莉我才意识到，生活本身就是一首最朴实且丰富的诗歌，尽全力去想下一刻想要做的事，尽全力去做下一刻想要做的事已经赋予了生活本身最好的意义，哪怕是已经在绝症当中。\n我记得高中的时候老师留了一篇议论文周记，是有关生活与死亡的讨论，我洋洋洒洒地写了很长一篇有理有据的论述死亡意义的文章，但是关于生活的部分只是起到陪衬死亡这个“明显更加重大的话题”的作用，我还清晰地记得老师用了一句论语的句子做批注：\n未知生，焉知死？ ——《论语 · 先进篇》\n我当时是不以为然，毕竟高中生想法的倔劲谁都有过感受，现在回想起来不禁有些感动。海德格尔在《存在与时间》中有过向死而生的表述，在茉莉的生活中，这份因死而生的心情表达的淋漓尽致。有时候我会胡乱瞎想，如果我真的得了什么绝症，那最后存活的那几年一定是我未来人生中最精彩的几年吧，那种生活的色彩一定会超过过去单色生活的总和。我们意识不到过于遥远的终点，在当下似乎漫漫的旅途当中做些无关紧要的小事浪费时间也不会影响下一段旅程。\n所以七年的时间节点突然跳了出来。\n在七年之后我可以说“我”一定会死亡，那么在这之前我想有怎样的生活呢？站在第四个七年的起点处，我希望在七年之后的我可以带着七年充满生命力的生活回忆满足地走入生理周期的坟墓，然后由下一个我开启另一段向死而生的旅程。\n我想稍稍修改一下小坂流加老师的作品名称：\n餘命 7 年。\n","permalink":"http://localhost:1313/ideas/%E9%A4%98%E5%91%BD-7-%E5%B9%B4/","summary":"\u003cp\u003e我和 7 这个数字很有缘分。\u003c/p\u003e\n\u003cp\u003e初高中时每次考试分发座位表，我都会第一时间去查看自己的班级号和座位号。在我有限的印象里，我的号码总是与 7 这个数字有着各种联系：7？17？14？21？考试时看到这个并不和谐的质数总是会点亮一下我的心情，这个在数字文化里被冷落的符号对我来说有一种天然的亲近感。\u003c/p\u003e\n\u003cp\u003e我不太记得哪个网站说过，七年是全身细胞更新的周期，在生理上，我相对于七年之前的自己是一个全新的存在。我知道每一个器官组织的更新速率不同，但是总要选一个平均值的上限来代表一下这个过程，一个带着点宿命味道的数字是再好不过了。我倒是很喜欢这个不长不短的时间，还带着点儿初高中回忆的味道。今年 21 岁，正好经过了三次更新迭代，兢兢业业工作的器官组织们都至少走过了三个轮回。在十多个轮回中，三个七年仅仅是三分之一的时光，在这个节点上去回顾这短短的二十一年未免有些做作，但是简单回头去看看过去的自己倒是一件蛮有趣的事情。\u003c/p\u003e\n\u003cp\u003e在第一个七年的节点我在上二三年级，刚刚从一楼带着铁栅栏的教室升入二楼的小教室里。四班在走廊的堵头，挨着小水池倒还有一点乐趣。夏天的时候班上总有几个不怕挨打的小孩拿着灌好的水气球扔来扔去，我是不太去玩这种水气球炸弹，但是也不妨碍我在旁边观摩几员猛将的拼杀，有时候也会被误伤就是了。被告知不好的事情坚决不碰是我小小的脑袋瓜里的唯一哲学，我好像也是家长和老师面前的好孩子。我记得最骄傲的事情是被选上写字课代表，每周抱着全班的练字本在座位上用红笔批改。写字本沉甸甸的，放在桌膛里边整整齐齐，也是一个小小的成就。好孩子的标准是不说脏话不吃辣条不到处乱跑，但是从根本上说这些有点坏的习惯对我来说从来没有存在的必要。每天的生活对我这个地球玩家新手来说仍然很明亮，这个小镇好大，我怎么也探索不完每一个有趣的角落。我记得端午节和家人去山上采带着露珠的艾草，下完雨去找松树墩旁边的小蘑菇，秋天躺在金黄的落叶床垫上闻一闻泥土的腥味，每次回来那种筋疲力竭躺在床上的感觉总是能让我很满足。放学路上我看到路尽头金红色的小山丘披着西斜的阳光，我总是感觉到一股暖流腾起。从林子里回来，我应该也裁了一片小小的阳光留在我心里了吧。学校微机室的大肚子电脑、夏天三十九度的教室里凉凉的红色小水桶、长满齐腰高杂草的神秘小水沟、查理九世的青铜棺和游戏王的三幻神好像都在回忆的黑柔滤镜下变得有点不太真实。\u003c/p\u003e\n\u003cp\u003e在第二个七年的节点我已经是初三的学生了。在转学之后的教育体系中，小学只有五年而初中有四年的时间。三年时间对于一个十多岁的毛头小子来说很漫长，我只记得分数从及格线爬到了班级前几，我从一个小透明也似乎变成了有点存在感的小角色了。关于初三一半的记忆是灰色纸的试卷，堆成山的作业试题和每周每一天考试的心惊胆战。我们学校的学习强度还是比较有名的，每一周对于数学英语物理化学必定会有标准考试，不定时语文也会占用时间来考试，我就在这每一周的循环中经历心跳和肾上腺素的波动起伏。我记得初三的我总是有那种莫名的恐惧，生怕被别人超过分数或者名次，于是在作业练习里边疯狂训练，每一周的考试都当作证明自己的机会一样极为重视。结果当然不会辜负这样“自残”一样的努力，我的名字也能和那些被大家誉为“大神”的角色写在一起了，老师和同学们似乎也比以前温和了许多，我好像获得了阶段性的成功。是吗？我好像更加敏感了，那些成绩和排名兜兜转转都离不开周围的评价。至于对我自己我可能自始至终都没有认知，我只知道在半夜一点刷完这本习题册能让我在下次考试当中多一点胜算，分数高一点这样我也可以高兴地度过这周余下的几天，然后再迎接下一周的血压飙升。在成绩起伏大时候我会自己报复自己，不需要老师督促，毕竟我是自诩的“好孩子”嘛。怕落在别人后边是我唯一的想法。关于初三的另一半呢是一位女生。我记得有一段时间是坐在我的斜后方，她平时比较内向，我们的交集也并不是很多。初三学生的喜欢总是停留在对只言片语和微小的动作理解上，有些幼稚的行为现在想想也是颇有趣的。有时候多交谈几句能够开心好几天；悄悄地把她的QQ设为特别关心，在节日打了很多字删掉送上最普通的祝福；给她讲题时略带兴奋但也小心翼翼克制着不想让对方发现\u0026hellip;\u0026hellip;我记得有一次同学打赌，比写练习册的页数，页数少的人给某某表白，我表白的对象正好是她。我怎么都是不好意思的，于是在当天晚上写了八十多页练习册，是对方的八倍多\u0026hellip;\u0026hellip;哈哈哈这些事情真的是幼稚到极点，比安昙小太郎和水野茜的初期发展还要缓慢，但是相信我那时能够每次上学看到她就已经很开心了吧。我不会形容初三学生双向暗恋的感觉，我只知道当时的世界对于七年前的我来说还是太过复杂，这两种记忆交织着描绘了七年前的我的生活，混着铅灰色和玫瑰色的色彩。七年前的我还好吗？\u003c/p\u003e\n\u003cp\u003e第三个七年的结尾是在刚上大四的秋天，我带着七年的回忆来到北京开始了很不顺利的实习生活。高中的三年有太多太多情感，我永远不会忘记三年中的很多瞬间，那是第三节人生中最最珍贵的三年。紧接着是高考砸锅、转专业成功、专业学习长征、科研阻力和长时间的焦虑与迷茫。奇怪的是，这七年我感觉被按下了加速键，我找不到在时间轴上被偷走的那几年。那几段时间就那么堂而皇之地溜走了。有留下什么吗？我倒是觉得大学三年所有情绪的重量都留下了，有一小部分开心的，有大部分焦虑的，也有一部分是无所谓的。大三课程结束我终于意识到大学生活这场漫长的跋涉竟然快走到了尽头，那个日夜埋头赶路的自己应该会很欣慰吧。七年旅途的末尾有上低音号的尾音，有实验没有起色的数字，有utown的落日和晚霞。我倒是又捡起了相机去拍拍星轨，拿起标日初级看看泡温泉的小李，看完了之前总是想查但是总忘记的导演的电影，给自己留出近乎奢侈的时间来思考一些天马行空的问题。我也会躺在床上刷上几个小时的 bilibili，在屏幕前对着新老 cp 傻笑，闷头一觉睡到中午。但是也许现在这个处在七年节点的我也可以说，在这第三节里我的生活过的实实在在。\u003c/p\u003e\n\u003cp\u003e我昨天看了小坂流加老师作品改编的 \u003cem\u003e餘命10年\u003c/em\u003e 电影，如果单从剧情的角度这部电影没有什么新奇，甚至可以说某些情节有一些老套和不合理。但是每次看到茉莉挣扎着在最后的日子里生活时我却总能被感触到，那种在存活百分制概率下生活的希望和动力让我有些触动。小坂流加老师没能看到作品上映，但是我能够想到在病床前写下这些文字的情景，那种强烈的想要延展生活的渴望。在过去两年里，我有些时候觉得我患上了虚无感的绝症，在各种压力的催化下这种症状更加明显，恶性循环之中导致了无语和迷茫。我弄不明白这生活热情到底是什么，跟着环境亦步亦趋也许是折衷之策吧，毕竟大学三年我都是这样。所以看到茉莉我才意识到，生活本身就是一首最朴实且丰富的诗歌，尽全力去想下一刻想要做的事，尽全力去做下一刻想要做的事已经赋予了生活本身最好的意义，哪怕是已经在绝症当中。\u003c/p\u003e\n\u003cp\u003e我记得高中的时候老师留了一篇议论文周记，是有关生活与死亡的讨论，我洋洋洒洒地写了很长一篇有理有据的论述死亡意义的文章，但是关于生活的部分只是起到陪衬死亡这个“明显更加重大的话题”的作用，我还清晰地记得老师用了一句论语的句子做批注：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e未知生，焉知死？ ——《论语 · 先进篇》\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e我当时是不以为然，毕竟高中生想法的倔劲谁都有过感受，现在回想起来不禁有些感动。海德格尔在《存在与时间》中有过向死而生的表述，在茉莉的生活中，这份因死而生的心情表达的淋漓尽致。有时候我会胡乱瞎想，如果我真的得了什么绝症，那最后存活的那几年一定是我未来人生中最精彩的几年吧，那种生活的色彩一定会超过过去单色生活的总和。我们意识不到过于遥远的终点，在当下似乎漫漫的旅途当中做些无关紧要的小事浪费时间也不会影响下一段旅程。\u003c/p\u003e\n\u003cp\u003e所以七年的时间节点突然跳了出来。\u003c/p\u003e\n\u003cp\u003e在七年之后我可以说“我”一定会死亡，那么在这之前我想有怎样的生活呢？站在第四个七年的起点处，我希望在七年之后的我可以带着七年充满生命力的生活回忆满足地走入生理周期的坟墓，然后由下一个我开启另一段向死而生的旅程。\u003c/p\u003e\n\u003cp\u003e我想稍稍修改一下小坂流加老师的作品名称：\u003c/p\u003e\n\u003cp\u003e餘命 7 年。\u003c/p\u003e","title":"餘命 7 年"},{"content":"安装与切换语言 安装：（Windows10）设置 -\u0026gt; 时间和语言 -\u0026gt; 语言 -\u0026gt; 首选语言 -\u0026gt; 添加语言 -\u0026gt; 日语 切换语言： Alt + Shift 或者 Windows + Space 切换相关 Ctrl + caps lock： 片假名输入切换为平假名输入 Alt + caps lock：平假名输入切换为片假名输入 F6: 片假名输入过程切换为平假名 F7: 平假名输入过程切换为片假名 F8: 变窄 (ｹｰｷ) 使用 Alt + ` 切换日文与英文 输入 长音 按照字面音打字，片假名长音打减号\n高校（こうこう）koukou コーヒー　ko-hi- 拨音 两个 n\n任務（にんむ）ninnmu 促音 促音后的音的辅音重复\n実際（じっさい）jissai 雑誌（ざっし）zasshi 拗音与外来语特殊拗音\n正常拗音正常输入，小写字符前加 x 或者 l, 记得写片假名切换 Alt + caps lock\n地球（ちきゅう）chikyuu 写真（しゃしん）shashinn/syashinn 女性（じょせい）jyosei/josei ティ texi フェ fuxe ヴ vu 特殊助词 按照五十音\n私は中国人です　ha 家へ行きました he ビールを飲みます wo Reference [1] “win10 日文输入法的安装与使用_win10日文输入法-CSDN博客.” https://blog.csdn.net/qq_40309341/article/details/103823853\n[2] 日语朋友YC老师, “安卓手机/WIN10电脑日语输入法添加安装使用一条龙教学！无法安装时有啥替代输入法？附特殊音节输入方法！,” YouTube. Jan. 19, 2023. [Online]. Available: https://www.youtube.com/watch?v=I0kXEmB4daA\n","permalink":"http://localhost:1313/learning/japanese/%E6%97%A5%E8%AF%AD%E8%BE%93%E5%85%A5%E6%B3%95/","summary":"\u003ch3 id=\"安装与切换语言\"\u003e安装与切换语言\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e安装：（Windows10）设置 -\u0026gt; 时间和语言 -\u0026gt; 语言 -\u0026gt; 首选语言 -\u0026gt; 添加语言 -\u0026gt; 日语\u003c/li\u003e\n\u003cli\u003e切换语言： \u003ccode\u003eAlt + Shift\u003c/code\u003e 或者 \u003ccode\u003eWindows + Space\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"切换相关\"\u003e切换相关\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eCtrl + caps lock\u003c/code\u003e： 片假名输入切换为平假名输入\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAlt + caps lock\u003c/code\u003e：平假名输入切换为片假名输入\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eF6\u003c/code\u003e: 片假名输入过程切换为平假名\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eF7\u003c/code\u003e: 平假名输入过程切换为片假名\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eF8\u003c/code\u003e: 变窄 (ｹｰｷ)\u003c/li\u003e\n\u003cli\u003e使用 Alt + ` 切换日文与英文\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"输入\"\u003e输入\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e长音\u003c/strong\u003e 按照字面音打字，片假名长音打减号\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e高校（こうこう）koukou\u003c/li\u003e\n\u003cli\u003eコーヒー　ko-hi-\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e拨音\u003c/strong\u003e 两个 n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e任務（にんむ）ninnmu\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e促音\u003c/strong\u003e 促音后的音的辅音重复\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e実際（じっさい）jissai\u003c/li\u003e\n\u003cli\u003e雑誌（ざっし）zasshi\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e拗音与外来语特殊拗音\u003c/strong\u003e\u003cbr\u003e\n正常拗音正常输入，小写字符前加 \u003ccode\u003ex\u003c/code\u003e 或者 \u003ccode\u003el\u003c/code\u003e, 记得写片假名切换 \u003ccode\u003eAlt + caps lock\u003c/code\u003e\u003c/p\u003e","title":"日语输入法"},{"content":"Text This is a plain text. Text with Stress, Italic, Del line. Text with inline item.\nGIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nKatex Inline MathJax: @\\sum_{i}a_i@\nBlock MathJax:\n$$\r\\int_{a}^{b}f(t)\\text{d}t\r$$\\mathbb, \\mathcal test\n$$\r\\mathbb{ABCDEFG}\\mathcal{ABCDEFG}\r$$Environment test\n$$\r\\begin{aligned}\rA \u0026= aaa \\\\\r\u0026= bbb \\rightarrow\r\\end{aligned}\r$$Coding Test for Python\ndef test(param): \u0026#34;\u0026#34;\u0026#34;Test for hugo website Args: param: parameter of function Return: type of param \u0026#34;\u0026#34;\u0026#34; return type(param) Test for C++\n#include \u0026lt;iostream\u0026gt; using namespace std; int test(int param){ // Test comment return true; } line number:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; highlights\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Image Tables A B C D a b c d ","permalink":"http://localhost:1313/learning/hugo/hugo-feature-test/","summary":"\u003ch3 id=\"text\"\u003eText\u003c/h3\u003e\n\u003cp\u003eThis is a plain text. Text with \u003cstrong\u003eStress\u003c/strong\u003e, \u003cem\u003eItalic\u003c/em\u003e, \u003cdel\u003eDel line\u003c/del\u003e. Text with \u003ccode\u003einline item\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003eGIF\u003c!-- raw HTML omitted --\u003e is a bitmap image format.\u003c/p\u003e\n\u003cp\u003eH\u003c!-- raw HTML omitted --\u003e2\u003c!-- raw HTML omitted --\u003eO\u003c/p\u003e\n\u003cp\u003eX\u003c!-- raw HTML omitted --\u003en\u003c!-- raw HTML omitted --\u003e + Y\u003c!-- raw HTML omitted --\u003en\u003c!-- raw HTML omitted --\u003e = Z\u003c!-- raw HTML omitted --\u003en\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e","title":"Hugo Feature Test"}]