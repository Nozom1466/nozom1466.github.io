[{"content":"[未完待续] 一 北楼的教室总是展示着同济的历史感，褪色的投影荧幕，吱呀作响的木门和折叠椅凳很容易给我一种回到十年前二十年前的感觉。这种“复古”的感觉常常会拖慢时间的体感，仿佛挂在墙上的电子钟总能给坐在北楼教室里的学生更加慷慨的时间长度，至少四年前的我是这么认为的。在凝固的时间里，我总是爱想四年以后的情景。在我的想象中，我再次坐在北楼的教室里，必定是某个带着暖色的下午，我翻着大一的照片回想起来某些在四平校园某个角落的瞬间。\n北楼教室\n二 四平校园不大，与嘉定比起来就更显得袖珍了。不过不论是从生活的哪个方面，四平都要超出嘉定一大截。对大一的我来说所谓的生活环境并不重要，可能当时我也没有心情去好好品味一下这个校园的文化氛围。寝室教室图书馆和食堂就是每天的打卡地，不过某些时候我实在受不了学苑食堂的时候我会跑到北苑或者西苑改善一下伙食，食堂之间的距离倒也不远。学苑是我第一次感受所谓“吃在同济”的地方，在二楼左侧中间窗口点了一小罐砂锅（或者是蛋炒饭？），拎着包手足无措地挤过密不透风的人群找地方坐下。我学着人群找到收拾碗筷的地方，像高年级学生一样把餐具推到回收点，心想总算是熟悉了大学吃饭的流程。不过那就是关于这座食堂的最好回忆了。余下大一的学习日子里一楼的速食面和拌饭伴我度过了无数苦闷的时光，\n学苑食堂，二楼的自选餐厅还是不错的\n如果图书馆旁边的学苑全都是黑白单色的记忆，那西苑倒是有一些滤镜在的。西苑食堂同样说不上多好吃，一楼的份饭和地下一层的炒饭也是万年不变的只够充饥的食物。但是每每想到西苑这两个字，我总能回想起大一转专业之前去南校区上课之前吃的一琬琬热气腾腾的馄饨。升腾的热气掺杂着清晨的阳光，一碗馄饨下肚是一种别样的满足感。大一早八去南校区上机械制图是我少有的吃早饭的原因。特有海派风味的老教授在黑板上给我们画圆锥的切面图，讲讲当时东方明珠塔塔尖天线悬吊趣闻。除了让人心情沉重的 GPA， 好像一周里吃完早饭踩着铃声去教室看老师画图也是一件饶有趣味的事。\n西苑食堂的下沉广场\n同济大排档我倒是只吃过一次，好像价格并不是我随时想吃就能享受一番的，于是抱着尝试心态试过之后就作罢了. 我关于西苑另一段比较清晰的记忆是我们在寝室隔离的一段时间. 大一下学期的那几个月,我和 1010 的室友在西南一背后的小路上走了三个月, 带着口罩,抱着四个人热气腾腾的盒饭穿梭于寝室和西苑之间. 我对这条小路有一种别样的感激之情. 在一楼背光处的我们并没有多少机会感受到阳光, 只有在取饭的这段小小的任务中我们能短暂感受到阳光亲吻肌肤的触感. 自由的感觉全都沉淀在往返食堂的这段路程中, 走在路上我好像还能感受到那时候稍带幼稚的快乐.\n西南一背后的森林小路\n不像校园的其他地方翻修多次,这里好像没有变, 从这头走到那头依旧是那么长. 像当时一样, 我无比期待着这条小路没有尽头. 我想尽量走地慢一点, 再慢一点.\n三 从赤峰路入口到西南一寝室, 中间总是要经过这里的学子超市. 我特别喜欢这里的小路曲折一下的感觉, 是一种类似回家前的门廊的感觉, 熟悉又安心. 小小的学子超市承担了大部分西南一学生的购物需求, 尤其是在囤货的时候. 记得我和室友每个人买了四五桶水, 气喘吁吁地一趟又一趟往返超市和寝室, 成功地为 1010 攒了至少十三桶农夫山泉，那必然是有小小的骄傲的。\n学子超市一角\n西南一草坪\n我记得在 2022 封闭的日子刚刚结束的那几天里，每天傍晚我都会坐在草坪周围的某个地方发呆，或者想着转专业的繁琐程序，或者就静静地沉醉在傍晚蓝调时刻。月牙就在那高高的枝桠上，嵌在醉人的深蓝色中。\n蓝调的月牙-iPhone12Pro\n西南一旁秋天的颜色-iPhone12Pro\n西南一路标\n再向前走走就是西南一宿舍楼了，这栋三层楼的建筑呈 E 字形，两个缺口处是两块巨大的草坪。其中种有一棵三层楼高大树的一侧是整栋楼的入口. 各种社团经常在这里开展活动。 成员们三五成群地围在野餐垫旁边听学长讲音乐史，或者吉他社抱着几把吉他在树下弹民谣。一般是类似于周杰伦《稻香》之类的曲子。若是晴天我可能就坐在不远处不会打扰到他们的距离静静听一会再回寝室。\n西南一入口前的大草坪，经常有社团在这里活动，音乐社团经常在这里弹吉他。\n西南一的占地比其他宿舍要大，每一间宿舍的内部空间也相对更大一些。但是在 21 级刚入学的时候，分摊到每个人的租金却只有一学期 800 元。在洋浦四平路的这个地段有这个价位我已经心满意足了。从外观上来看，西南一还是十分古朴典雅的，瓦片门梁颇有一番风味。在 21-22 年的时候，我们的窗子还是木制的，在冬天的时候时常关不严。冷气就从窗户缝偷偷溜进来，时常让我感叹其他宿舍楼的健全设施。每每路过超市旁的宿舍隔着走廊窗子看到明亮的走廊以及刷卡门禁，总让我觉得这每学期省下来的 500 元住宿费好像不那么值（我记得其他宿舍好像是 1300 元住宿费每学期）。\n好像国豪书院在西南一\n西南一入口\n我记得大一的冬天，我学着以往东北冬天的方法，买来一块大塑料膜照在窗户上，四个角用胶带封好，再拉上窗帘抵御冷气的侵袭。但是好景不长，西南一的墙皮老化十分严重，没过几天胶带就集体脱落了，塑料膜保卫计划就此告罄。后来我们发现木头窗子靠着铁质窗闩根本封不严，于是我拿着绑行李的绳带绑着窗户，时不时往里拉一拉，然后用纸团堵住凹陷的窗户框，这才勉勉强强度过了在南方的第一个冬天。\n西南一的木头窗子\n22 年夏天的西南一，1010寝室一侧\n毕业时的西南一，1010寝室一侧\n不过西南一在我们离开四平之后翻修了很久，现在已经是四平数一数二的宿舍了。现在各种设施配套齐全，所有宿舍开门方式也从钥匙更换成了学生卡刷卡。走廊里整洁明亮，像是正经学生宿舍的样子。曾经的 1010 宿舍现在经过重新编号已经是 1007 宿舍了，不知道这里学习的是什么专业呢？不远处的尽头是洗漱间和卫生间，我记得当时的卫生间的装潢就像高级宾馆一样，与我们的宿舍和洗漱间形成了强烈反差。我当时还说过整个西南一就厕所像正经宿舍一样，令人感慨。果然毕业时回去最感受不到区别的就是卫生间了，哭笑不得。不过要说为什么关于卫生间有这么深的记忆，还是因为在疫情的时候封在寝室，几步就到的卫生间就是我们最远能去到的地方。我记得我常常在走廊尽头多逗留一会儿，两三只大橘猫经常窝在窗沿底下睡觉撒娇，小小的灌木丛里有一些掉落的衣服和小物件，这里倒确实像一个小猫的乐园。\n22年的洗漱间-iPhone12Pro\n走廊尽头窗户外的灌木丛-iPhone12Pro\n卫生间的里侧有一个窗户，门口就是一颗大大的梅花/樱花树，开春的时候全身都装点上粉色，阳光照下来看的人心里暖暖的。卫生间窗外也是橘猫的领地，多的时候甚至有十多只橘猫围着这里玩耍。记得刚入学的时候我还经常在旁边看猫盟的同学们拿着猫粮和猫条在这边喂食。疫情转到宿舍之后我就只能趁着上卫生间的时间看看小猫玩耍了。小猫们在水泥台上时常躺着晒太阳，可能没人玩耍他们也很寂寞叭。当时每次洗漱的时候其实最期待的事情是小猫顺着窗户爬进来溜达一圈，事实上会有这样的期待也是因为有那么一次一只花斑猫晚上十一点多悄悄爬上窗户和我对视了好一会，小猫在那里做了个猫氏伸展之后踮着脚跑掉了。那一天剩下的时间都被这只小猫点亮了，毫无拘束的小猫带着自由的晚风是那时候我的精神图腾。\n22年卫生间外的小花（梅花/樱花？）-iPhone12Pro\n22年卫生间外的橘猫-iPhone12Pro\n22年卫生间外的橘猫-iPhone12Pro\n22年卫生间外的橘猫-iPhone12Pro\n22年卫生间外的橘猫-iPhone12Pro\n隐隐约约地记得，在疫情结束的那段时间里好像有领养这些小猫的通知，自那以后好像西南一周围的猫就消失了。无论我哪个季节什么时间从嘉定回四平，路过西南一的时候我总会习惯性的寻找他们的踪迹。结果是什么也找不到，大橙军团驻守西南一的故事也随着疫情和宿舍改造被一并带走了。现在你们在哪里呢？\n寝室旁边的台阶\n寝室旁边的记忆是最多的，每个角落好像都藏着点过去的影子。数不清有多少次我在宿舍旁边的楼梯旁准备 pre 和德语口语，我总是拿着写好的稿子一遍又一遍地的在这个“小洼地”翻来覆去地背诵。有时候我翻到手机里的录音听到那时候录制的还算流畅的德语对话，想一想现在只记得 \u0026ldquo;Ich liebe dich\u0026rdquo; 和学各种格助词的痛苦，真的是过了好久哇。记得学术英语写作那门课需要做一个关于科学家贡献的演讲，我找来霍金的生平介绍，在 word 文档里写好中文，用 DeepL 翻译为英文，逐字逐句修改词句。也是在这个楼梯口花了两三个小时背诵并不长的讲稿，背到口干舌燥就回寝室喝几口水出来继续背，到最后背出来就像唱戏一样。想想当时为了转专业真的是比较拼命了。不过这样的程度也让我在大二之后有些疲倦了，计科每一位同学都是这种程度。\n搬离西南一的那一天-门口\n总有离开这里的一天——离开西南一前的那几天我总这么想，毕竟期待着后边三年更精彩的大学生活，并没有什么心思怀旧。我比较遗憾的是我们寝室最后最后没有留下一张四个人的合照，毕竟在这个小小宿舍里生活了一年并且还度过了一个疫情封控，也算是亲密战友了。走廊的行李堆在通道两旁，我站在楼梯口时不时要给同学让路。我不知道会在嘉定有什么样新的回忆，门口的运输车轰隆隆响个不停，一车又一车把西南一的行李带走，西南一的宿舍像一年前一样又空了出来。夏天的暖风吹得让人烦躁。外边的阳光靠着光亮的地砖涌进来十分刺眼。我尽量把视线移开门外曝光过高的世界，把注意力集中在收拾我那堆杂乱的东西上。我对西南一的记忆就停止在那个有着热烈阳光的下午。\n曾经 1010 寝室夜晚-iPhone12Pro\n从寝室门口看去-iPhone12Pro\n老西南一的记忆随着 21 级同学的离开被带到了嘉定，四平也彻底告别陈旧的宿舍了，前进的四平让人觉得欣喜，同济也终于把建筑的功力用在了校园里。就像高中我是最后一批在地下室的高一学生一样，这里有我带着时间坐标的记忆, 我喜欢陈旧的西南一，以及曾经在这里居住的同学们和带着温度的回忆。\n四 大一的时候我还在中德实验班，需要学英语和德语以及通识课程和机械类的专业课程。比较重要的是 8 学分的德语课，在我第一个学期的学分中直接占比四分之一左右。由于是朝着转专业方向在努力，这门课的成绩直接决定了我后边三年的学习内容。这样图书馆旁的学苑的拌面和拌饭陪我度过了一个学期，口语录音一个又一个，单词一遍又一遍。这没有什么可以表述的，关于较劲儿努力的例子比比皆是，有成功的有失败的，我还挺幸运的最后拿到了一个满意的分数。我那时候沉迷吹响吧上低音号（谁能想到四年之后还是很喜欢），里边有个角色是伞木希美，各种方式努力了最后的结果也不尽人意（从读者来说），从各种方面来说都是一个悲剧色彩满满的角色。我觉得其中最触动的一点就是在她高二努力回归的那一刻，那种坚持追逐的劲儿给了我挺大触动。虽然说终究是一部艺术作品，但是说实话这种劲儿是给了我在大一熬夜焦虑的时候很多支持的。\n我总是对生活的某几个瞬间记得特别清楚，就像按下快门一样定格在那个时间点上，成为后续我回忆经历的坐标。我特别清楚地记得我在情人坡后身的台阶上背德语的情景。在图书馆学累了我就拿着德语书在门口的小河旁边背德语对话和单词。那时候我突然背到 “himmel(天堂)” 和 \u0026ldquo;sterne(星星)\u0026quot;，然后我抬头看了看傍晚的天空。不知道发明这个词的时候他们有没有停下来看一看窗外的天空呢，他们会疑惑那里有天堂吗。\n情人坡后身\n到今天我还能清楚地记得德语期末考试那一段经历，那就是一种第二次高考的感觉。七点半我从床上以仪式般的姿态起身，按照前天晚上计划好的方式检查了一遍复习资料，并且郑重地把初中英语老师赠送的小和田玉塞进包里——我每逢重大考试都会带着这块玉，陪伴我走完了中考高考——没有遗漏就叫上口语搭子出发了。四平还没有睡醒，早上的雾气带着昨天小雨的味道拍打在我脸上，我看到西南一门口草地上的露珠闪闪发光。好天气呢——我又迅速把注意力转回到那几句用德语点餐的对话上。我不太喜欢跟别人要祝福，那样好像祝福也会随着这种设计性而丧失了原有的意味。但是考试那天我特别想有谁能给我发一个加油之类的小祝福，但是又怕我会因为得到了别人的期待而变得负担加重起来（大一的时候确实是这么纠结的，现在回想起来有点奇怪哈哈）。手足无措的时候我给高中同桌发了微信，然后就羞愧地直接调成免打扰扔进包里。\n聊天记录\n门口等了两个小时，抽中的口语话题正好是我们熟练练习的餐馆话题，我松了口气逐字逐句地背出来德国幼稚园餐厅中才会发生的理想对话。接下来是相对比较轻松的笔试，正反两面的卷子没多长时间就答完了，最后检查到我几乎能背出来的程度交了卷。就这样吧，毕竟也努力过了。\n宿舍旁的小石阶\n我会说我在德语这一科上其实是很幸运的，遇到了一位好老师，抽到了比较适合的口语题目，最后的卷子还蛮简单的。出分那一天晚上我看到成绩真从椅子上跳了起来。抓起衣服冲进 21 和 22 年交际的冬天里。那时确实很高兴，确实能送一口气。在宿舍旁边的小石阶上我拨通了爸妈的视频通话，剩下的几门考试已经不用担心什么了。学德语的过程十分痛苦，这不仅来自于学的困难，更来自于为什么要学的疑惑。最后如愿的成绩并不能减轻一点背单词练口语的痛苦，这份感受就是那么真真切切地在那段日子里留存着，是我大一上学期每一天的日常。\n","permalink":"http://localhost:1313/travel/%E5%9B%9B%E5%B9%B3%E8%B7%AF1239%E5%8F%B7/","summary":"关于四平路1239号的一些回忆","title":"四平路1239号"},{"content":"Homepage\nFor BART, T5, mT5 and AlexaTM 20B\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series.\nFigure 1. LLMs evolutionary tree.\nContributions Combination of bidirectional encider and autoregressive decoder: BERT(bi-directional) + GPT(uni-direct auto-regressive) Pretraining with better noising approaches: shuffling + in-filling scheme Approach Architecture: BART use seq2seq Transformer architecture. Mofidications: ReLUs are modified as GeLUs, with initialization from @\\mathcal{N}(0, 0.02)@. 6 layers in encoder and 12 layers in decoder (following BERT) Remove Feed-Forward network before word prediction Pretraining: Trained by corrupting documents and then optimizing a reconstriction loss. Corruptions are introduced in Figure 2. Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed.\nFinetuning: Representations produced by BART are used in Seq/token classification, seq generationa and MT tasks. In MT, the author replace BART\u0026rsquo;s encoder embedding layer with a new randomly initialized encoder, as illustarted in Figure 3. Figure 3. Fine tuning BART for classification and translation.\nPretraining Objective Comparison Models: LM(GPT), Permuted LM(XLNet), Masked LM(BERT), Multitask Masked LM(UniLM), Masked seq2seq(MASS) Tasks: SQuAD(context + question -\u0026gt; context span) MNLI(con + q -\u0026gt; relation) ELI5, XSum, CNN/DM(con + q -\u0026gt; abstraction) ConvAI2(dialogue gen) Insights: Performance of pre-training methods varies significantly across tasks Token masking is crucial Left-to-right pre-training improves generation Bidirectional encoders are crucial for SQuAD The pre-training objective is not the only important factor Pure language models perform best on ELI5 BART achieves the most consistently strong performance. Large-scale Pre-training Experiments Pretraining with large batchsize (8000 a batch. 500k steps following RoBERTa):\nDiscriminative Tasks: BART’s improvements on generation tasks do not come at the expense of classification performance. Generation Tasks: Summarization, Dialogue and Abstrctive QA. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer This work explores the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. And the model proposed in this paper, known as T5, is trained on unified text2text framework, where all of outputs are regarded as texts. (\u0026ldquo;3.8\u0026rdquo;, \u0026ldquo;5\u0026rdquo;)\nMotivation Previously the training in ML is amenable to downstream learning tasks and the knowledge required for the learning is learned as part of auxiliary task.(word vectors). Later the scheme shifted to pretraining on data-rich tasks using unlabeled data (like Common Crawl project). The work focus on the understanding of burgeoning transfer learning techniques.\nBasic idea: treat every text processing problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output.\nArchitecture of T5 Model To summarize, T5 is roughly equivalent to the original Transformer with the exception of\nremoving the Layer Norm bias：activations are only rescaled and no additive bias is applied. placing the layer normalization outside the residual path using a different position embedding scheme.: We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model. Check out Transformers without Normalization from meta\u0026hellip;. Layer Norm is replaced by @tanh(\\cdot)@，\nDataset - The Colossal Clean Crawled Corpus Adopted from Common Crawl dataset, using cleaning up techniques:\nend in . page \u0026gt; 3 sentences; lines \u0026gt;= 5 words remove bad words lorem ipsum placeholder removed curly bracket removed citation markers removed policy \u0026amp; cookies removed leave only one in 3-span sentence sliding window pages not written in English Downstream Tasks Indeed an in-depth tech report \u0026hellip;\n750 GB dataset\nmachine translation: WMT question answering: SQuAD abstractive summarization: CNN/Daily Mail text classification: GLUE, SuperGLUE Baselines Goal: Pre-train a standard Transformer using a simple denoising objective and then separately fine-tune on each of our downstream tasks. Model: encoder and decoder are each similar in size and configuration to a BERT_BASE. Each has 12 blocks, FF layer @d_{ff}=3072@, @d_{kv} = 64@, multi-attention with 12 heads. Dropout prob 0.1. Training: As all tasks are regarded as t2t, loss f: teacher forcing and cross-entropy loss. AdaFactor as optimizor. Learning rate schedule: inverse square root (@1 / \\sqrt{\\max{(n, k)}}@), with n as iteration index and k as number of warm-up steps. Vocabulary: SentencePiece to encode text as WordPiece tokens. Unsupervised Objective: (Denoising) An objective that randomly samples and then drops out 15% of tokens in the input sequence, as shown in Figure 4. Figure 4.Schematic of the objective we use in our baseline model. In this example, we process the sentence “Thank you for inviting me to your party last week.” The words “for”, “inviting” and “last” (marked with an ×) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as and ) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel . The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token .\nThe following sections are discussing model performance from Architectures, Unsupervised Objectives, Pre-training Datasets, Training strategy and Scaling.\nArchitectures Another classification other than encoder/decoder: look into attention mask adopted by the model. There are 3 types of mask patterns: Fulli-visible, Casual and Casual with prefix, as shown in Figure 5.\nFigure 5. Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from “the future”. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\nStandard Encoder-Decoder architecture: All-visible mask(Encoder) + Casual mask(Decoder) Language Model: Casual mask, Language models are typically used for compression or sequence generation Prefix LM: Casual with Prefix mask, could be considered as encoder+decoder combined. Architectures as shown in Figure 6.\nPrefix LMs resembles BERT for classification tasks when you feeding the model with tasks like: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target: (Of course! Didn\u0026rsquo;t see why the author are adding this paragraph \u0026hellip;). Attention masking seems to be an interesting topic, which involves many other memory-efficient or computing-efficient methods (paged attention for vllm?). Figure 6. Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use “.” to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fullyvisible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input.\nHere the authors mentioned criterions in selecting models, condidering these models are in different architectures and parameters. We suppose two models are equivalent if they have the same parameter @P@ or the same computational cost @C@. Consider an encoder-decoder model with @L + L@ layers, @P + P@ parameters and a language model(decoder) with @2L@ layers and @2P@ parameters. The parameters are approximately the same for these models but the computation cost of language model is approx. twice of that in encoder-decoder model. Because the latter has to deal with both input squence and output sequence but the former deal with inputs and outputs separately. (has lots to do with sequence length \u0026hellip;). Theresfore, they select:\ne-d, L + L -\u0026gt; 2P, M flops\ne-d, shared params -\u0026gt; P, M flops\ne-d, L/2 + L/2 -\u0026gt; P, M/2 flops\nd, L -\u0026gt; P, M flops\nd, prefix -\u0026gt; P, M flops\nwhere e-d for encoder and decoder and L for layers, P for parameters, M for computational cost.\nResults (for different architecture): Denoising task (metioned in previous section) and language modeling task (predicting the whole sentence for language model and predicting the second half of the sentence given the first half). Sharing the params across e-d performed very well and halfing params hurts the performace.\nUnsupervised Objectives Examples of common unsupervised objectives (Figure 7). Models are fisrt pretrained based on these unsupervised objectives then evaluated on downstream tasks (GLUE, CNNDM, SQuAD, SGLUE, EnDe, EnFr and EnRo). This section extends in: 3 common unsupervised objectives -\u0026gt; variants of BERT objective (MLM) -\u0026gt; exploration of corruption rate -\u0026gt; exploration of corrupting spans.\nFigure 7. Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text “Thank you for inviting me to your party last week .” Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. denotes a shared mask token and , , and denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple.\nHigh Level Approaches: Tha author evaluated 3 types of objectives: Prefix language modeling, BERT-syle(MLM) and deshuffuling (as illustarted in Figure 7). BERT objective stands out (signifigantly over Deshuffling).\nVariants of BERT Objective: Purpose: better performance and better efficiency.\nVARIANT 1: MASS-style, reconstruct the original uncorrupted sequence(4th in Figure 7); VARIANT 2: Unique mask token, predict token prefixed by special token(5th in Figure 7).; VARIANT 3: Drop Corrupted Tokens, concatenate predicted tokens(6th in Figure 7). All these variants performs similarly. Notice that performace of dropping corrupted tokens fluctuates on several metrics. Dropping is still attractive because it reduces the input length thus making training faster. Corruption Rate: Limited effect on performance. Larger corruption rate -\u0026gt; more inference time.\nCorrupting Spans: BERT mask follows i.i.d., masking tokens independently. While in some cases we need consecutive corruption. Number of corruption span and span lengths are determined by parameters. Again, this trick has limited effect on downstream task performance. While span corruption slightly speeds up training because it produces shorter sequence on average.\nConclusions:\nDenoising objectives(BERT MLM) outperforms language modeling and deshuffling. Choosing among the denoising objectives we considered here should mainly be done according to their computational cost(since similar approaches yields slight improvements). It may be fortuitous to explore entirely different ways of leveraging unlabeled data. For conclusion 2, the paper seems only to explore BERT variants. What about the other two?\nPretraining Data set The effects of pretraininig dataset. There are these fun facts:\nC4 dataset proposed by this paper invloves a heuristic filtering strategy, which proved to be helpful in pretraining. Pretraining on in-domain unlabeled data can improve performance on downstream tasks. (not superising? like SFT?) But it\u0026rsquo;s not good if we want our model to adapt to language tasks from arbitraray domains. BUT the dataset gathered for specific domains are much smaller. Pretraining dataset size is also a key factor. The performance of pretrained model degrades as the size of dataset getting smaller. (model trys to memorize the dataset rather than learning.) Use large dataset as possible. Repeated dataset will degrade the performance while its ok if the repeated time are smaller than 64 (som metrics are even better) ","permalink":"http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/","summary":"\u003cp\u003e\u003ca href=\"https://www.cs.princeton.edu/courses/archive/fall22/cos597G/\"\u003eHomepage\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor BART, T5, mT5 and AlexaTM 20B\u003c/p\u003e\n\u003ch2 id=\"bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension\"\u003eBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\u003c/h2\u003e\n\u003cp\u003eEncoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series.\u003c/p\u003e","title":"COS597G 22 Encoder Decoder Models"},{"content":"\r最喜欢的蓝调时刻\n在宇治的日落时分\n京阪宇治线的下午\n温柔的冬日阳光\nHaruka 到站！\n","permalink":"http://localhost:1313/travel/serendipity/","summary":"一些旅行碎片","title":"Serendipity"},{"content":"[未完待续] Updates:\n12/13/2024: 订1.25出发机票、1.28返程机票(北京大兴机场\u0026amp;关西国际机场)，订送签旅行社\n12/19/2024: 签证材料邮寄、电子材料发送\n12/20/2024: 租用运动相机(1.18-2.4)、配件、SD卡\n12/23/2024: 旅行社处理材料，送签\n12/28/2024: 预定酒店(1.25-1.28)\n12/30/2024: 出签证结果\n12/31/2024: 拿到护照原件\n01/17/2025: Action5Pro 到货\n01/20/2025: Nikon Z6II + Nikon24-200 到货\n序言\n日本自由行的白日梦从大二一直留到了现在，每次跟别人说起来都让自己笑话，怎么感觉都有点叶公好龙的味道。这种本来一个周末就能实现的小计划在各种“宏伟的”人生计划中显得格格不入，于是在各种借口的催化下去日本玩逐渐变成了一个废案。最近有朋友要去日本，我才意识到那个遥远的白日梦离我好像也不远，经常挂在嘴边的宇治好像也只有一张机票的距离。我才发现好像不需要选择一个自我感动式的纪念日，不需要揣着万事俱备、没有烦心事的完美主义，也能够去“白日梦”的世界里去看一看，这好像不是什么难事。\n所以我想写一写关于日本自由行的准备工作，比如护照和签证、日程安排、物资准备、日本出行、交流工具、注意事项之类。除了想抵抗一下申请季和实习的沉闷心情，也想为未来半年内的行程做一点小小的规划。大二的时候我看到这份小资料会把它命名为什么呢？\u0026hellip;我想应该叫做白日梦手册 =)。\n物品清单 摄影摄像 Nikon Z62 + 24-200 电池 * 2 SD卡 Dji Action5Pro 储存卡 1.5m延申杆 章鱼支架 转换插头 * 2 充电宝 - rosmos 60000mAh 数据线 lightning Type-C 笔记本电脑 转换插头 凭证材料 护照原件 电子签证：纸质 入境申报单：截图 酒店确认单：纸质 保险 巡礼材料 巡礼照片添加到 Google Map Nozomi coat \u0026amp; scarf \u0026amp; backpack \u0026amp; pants 应急准备 手电筒 出行用品 流量卡 纸巾+湿巾 垃圾袋 雨披+雨鞋 眼罩 多功能收纳包 额外装商品的包 货币相关 Visa信用卡 银行卡 零钱卡槽 Todo List 巡礼打卡 - 买周边 - 骏河屋 寺庙抽签 - 御守 护照和签证 护照\n护照很容易申请，走正常手续时间也很短，一般一周左右可以拿到实体的护照本。首先需要携带身份证、拍照衣物 (为了上镜一点) 、120元人民币工本费、10元邮寄费去当地的出入境管理局。在上海办理地点可以参考地点列表。在办理地点领取表单填写、在照相室拍照、等待窗口办理、填写地点办理邮寄即可。等待一周过后（实际上只等待了五个工作日）领取快递即可。个人评价中国护照超级好看，拿在手里很有质感。\n护照办理后 10 年内有效，记得带一件好看的衣服，拍照的时候慈眉善目一点，问一下拍照老师能不能稍微修一修(TAT)\n签证材料准备：根据日本驻华大使馆官网中所列举的签证材料，针对个人单次旅游，在校大学生需要提供：护照、签证申请表（粘贴本人照片）、个人信息处理同意书、能确认本人居住地的资料、大学出具的在学证明书或者毕业证书以及其他要求追加材料。可参照官网pdf核对，对应内容具体展示在图 1 中。\n护照：护照原件，有效期 6 个月以上，2 页以上空白页。 照片：2 张35mm @\\times@ 45mm 尺寸白底彩色照片，3月以内照片。不可戴眼镜，免冠证件照。仅需电子版即可。 身份证正反面复印件 户籍复印件：学校集体户口需要电子证明，封面+个人页。上海集体户口登录随申办申请集体户口证明。 在读证明原件 学信网学籍证明 图 1. 个人单次旅游签证所需材料\n注意这里是通用材料 \u0026amp; 官方介绍材料，我提交的材料将在 办理手续 中说明。\n签证办理手续：\n首先，日本签证办理是划分领区的，去往哪个领区由常住地决定。如果在外地上大学且户口和登记地不同，则直接在大学所在领区办理即可，不需要去户籍所在地 (工作也是同理，在工作地所在领区办理，具体请参考旅行社提供的信息)。\n中国前往日本必须要通过旅行社申请签证，旅行社在官网中有推荐. 在淘宝上对应搜索即可。我这里选择的是 上海中国青年旅行社 (上青旅)。对于不同旅行社，提交的材料可能有所不同。我实际提交的材料有两部分，分别是电子版(发送到旅行社指定邮箱)和纸质版(需要邮寄到旅行社)，详细清单如下：\n电子版本：\n其他信息.txt： 订单号、淘宝交易号、本人地址、旅行社地址 护照 Bio 页扫描件.pdf 签证照片电子版备份.jpg：在照相馆可以索要电子版 日本签证表.pdf：最重要的表单，中介会发给申请人，注意这个 .pdf 是一个可交互的、使用下拉列表的 .pdf。只需要点击下拉列表选择，完成后保存到文件即可。 集体户口户籍证明件.pdf：这里本来应该为 集体户口封面页+集体户口个人页或者 户口本复印件(所有页)，但是由于我的学校户籍科不提供(没有)集体户口个人页，所以只能提交集体户口户籍证明件。注意这里我询问中青旅时被告知，我的这种情况直接使用户口本也是可以的，而且不需要更换领区。但是由于我的户口本上被标明已迁出原注册地，所以不能使用。 个人信息处理同意书.doc：本文件是旅行社发的，主要内容就是同意旅行社处理个人信息的协议，直接签字即可。 身份证复印件.pdf 教育部学籍在线验证报告.pdf：请前往学信档案中上方 在线验证报告中的学籍报告(若为在校生)申请、下载。 学生证扫描件.pdf：本项为旅行社要求，估计是因为没有户口证明件，需要使用学生证辅助证明一下。 校园卡扫描件.pdf：顺便将校园卡扫描 在读证明.pdf：我认为学生证扫描件证明程度不够，所以上传在读证明，上边清晰标明了项目的起止时间。 纸质版：\n电子版的所有材料彩色打印：我为了保险起见全部打印。其实仅需要日本签证表 和 个人信息处理同意书打印即可。 35mm@\\times@45mm的 2 寸照片 6 张(旅行社要求两张) 护照原件：护照原件需要邮寄到旅行社地点 旅行社说明，在送签后约 8 个工作日出签；官网说明，在送签后约 12 个工作日出签。考虑到邮寄时间以及旅行社处理时间，尽量提前 20 天开始准备签证材料。 我一共沟通了两家旅行社，第一家为中国青年旅行社，第二家为上海青年旅行社。选择第二家的原因是上青旅接受只有集体户口户籍证明件送签，而第一家不接受，各个旅行社的文件要求不同。 出行计划 出行地点\n出行地点参考各位 Up 主提供 Google Map\n这里是我的巡礼地图：\n日本交通出行 机场到市区\n我的落地机场是关西国际机场，可以从关西国际机场T1航站楼乘坐はるか(Haruka) 特快，耗时 80min 左右到达京都站；在京都站乘坐奈良线 20min 到达宇治站。Haruka 使用携程预订往返票，可以剩下落地之后的买票时间。\n地铁卡\n京都地区市内使用西瓜卡 (Suica) 就可以。我按照小红书说明在 Apple的Wallets中办理了虚拟西瓜卡 ，我参考的是这篇帖子. 刷卡进站时打开苹果 NFC直接刷就可以。[添加一张NFC刷卡图片]\n出租车\n已经被各种攻略警告不要坐日本出租车了，听说极为昂贵。不过日本出租车外观还是很好看的。[添加出租车外观图片]\n酒店\n由于本次旅行完全为了巡礼准备，我希望居住地和宇治川距离不要太远；同时由于京都市中心也有几处巡礼点，住址还需要在地铁附近。我在携程预定了宇治站(奈良线)附近的民宿 宇治之愿 茶愿寿邸。步行 8 分钟即可到达 久美子ベンチ 的宇治川附近，同时距离奈良线宇治站只隔了两三条街的距离，十分方便。如果不是重度巡礼选手建议还是住在京都市区，交通很方便。抽出一天或者半天来到宇治就可以逛完。\n设备租赁 基本配置是运动相机 + 相机，运动相机用于记录走路片段+第三人称视角，相机用于照片+视频拍摄。我只有一台古董 Nikon D90+腾龙18-200，相片稍微放大就已经糊掉了，同时视频最高支持720p，SD内存卡最多32G，对焦一两秒钟，估计以后拍复古回忆录能用上哈哈。宇治这次旅行真的想配置更好一点设备，只能去租。\n我认为有几个限制条件会对旅拍拍摄过程产生影响：设备性质、配件易用性、电池电量、视频照片存储。其中在我实地拍摄过程中 top 1 需要注意的是设备电量，其次是存储空间。而设备性质和配件易用性可以在拍摄之前搞定。\n运动相机\n由于本身想买一台运动相机，比较纠结 Dji Action5Pro, Dji Pocket3, Insta AcePro2 和 Insta360 拇指相机。经过一两个晚上的纠结之后决定直接去看影视飓风2024设备推荐，直接决定用超长续航的 Dji Action5 pro，毕竟大部分时间是作为“行人记录仪”。我在咸鱼上租了18天的 Action5Pro 畅拍套装。\n在畅拍套装中有三块电池，事实证明对于一天走个 2w 步、12h的巡礼规划，这个电池续航能够胜任。当然这里不是说十二个小时连着拍摄，还是需要稍微克制一下记录冲动的。\n运动相机配件\n我最后选择的运动相机配件是伸缩杆(带小型三脚架)、磁吸胸带以及一些其他商家赠品(未携带)。\n伸缩杆：优篮子的伸缩杆带有1.5m延长和一个小型桌面三脚架。这两个功能在巡礼中我都有用到，延长可以拍高视角和悬空视角；三脚架可以拍第三人称视角，还是十分实用的。[添加各种功能的展示] 磁吸胸带：我额外购买了磁吸胸带，主要用于记录第一人称视角。我选择的这款可以通过磁吸功能快速拆卸，对于快速更换视角或者更换电池很有帮助。 其他：在租运动相机的时候商家赠送了背包夹和挂脖，这两个设备并没有用上，主要是担心取放背包过程中影响拍摄、挂脖稳定性不好（并且不是很隐蔽+美观）。 摄影摄像相机\n我在[内啥租赁]上租了Nikon Z6II，以及一颗 24-200 的镜头。不租Z72， Z8 甚至 Z9是因为真的太贵。顺便想体验一下尼康中高端相机使用的感觉~。\n存储方案\n视频照片存储是非常重要的一环，我的存储方案分为以下几个部分\n相机存储 商家赠送 128GB 存储卡 由于我没有带三脚架和稳定器，相机拍摄视频手抖，所以主要拍摄照片。全部使用RAW 格式拍摄，插卡显示 2.4 k照片，完整一天12小时行程拍摄完成还剩下2.2 k。我认为我拍的还算比较多的哈哈。 运动相机存储 相机内置 40G 内存 + 商家赠送 128G 存储卡 + 自己购买 256G 存储卡 运动相机存储选择对我来说是重头戏，在拍摄过程中我会时刻注意拍摄存储还剩下几小时，属于有些时长焦虑。商家赠送的 128G 在Action5Pro满电情况下可以使用三个多小时，我自己购买的 256 G 可以拍摄六个多小时，虽然没有官网所说的四个小时但是这个时长也OK。在拍摄过程中没有使用过 40 G 内置内存，一般情况下都是电量比卡空间先行一步耗尽。 固态存储 1 TB 移动固态硬盘 + 128 GB U盘 额外的固态硬盘其实也非常重要。每天晚上回到民宿后，第一件事情就是把相机和运动相机的所有数据copy到移动硬盘当中。未第二天的拍摄流出存储空间。传输时长最多半小时到一小时。 充电相关\n为什么把充电单独提出来说就是因为这其实也特别重要。充电需要注意的地方有两点：转换插头\u0026amp;充电器、充电安排。\n转换插头的数量是这次提前安排的一大忽略点，我只买了一个转换插头，导致充电安排智能一个接一个线性排序，特别影响行程安排。建议最好能够携带设备数量 + 1 个转换插头（最最理想的情况），或者可以在日本购买（这个没有试过但是应该有很多地方售卖）。这样保证所有设备可以同时充电。对于常用的设备最好备用多个电池以及多个充电器。这次行程中，相机电池我带了两块，但是充电设备只有一个、转换插头也只有一个。这导致了我如果没有在晚上把两块电池充满，第二天将面临严重的相机电量焦虑。\n关于充电安排，如果仅仅有一个转换插头的话需要安排好充电时间。我的优先级是充电宝 \u0026amp; 运动相机 \u0026gt; 相机。首先充电宝保证手机有电是最重要的，我携带的一个充电宝\n背包选择\n消费 京阿尼周边\n饮食\n参考资料 [1] https://www.cn.emb-japan.go.jp/itpr_zh/visa_kanko.html\n[2]\n","permalink":"http://localhost:1313/travel/%E6%97%A5%E6%9C%AC%E8%87%AA%E7%94%B1%E8%A1%8C%E7%99%BD%E6%97%A5%E6%A2%A6%E6%89%8B%E5%86%8C/","summary":"\u003ch2 id=\"未完待续\"\u003e[未完待续]\u003c/h2\u003e\n\u003cp\u003eUpdates:\u003cbr\u003e\n12/13/2024: 订1.25出发机票、1.28返程机票(北京大兴机场\u0026amp;关西国际机场)，订送签旅行社\u003cbr\u003e\n12/19/2024: 签证材料邮寄、电子材料发送\u003cbr\u003e\n12/20/2024: 租用运动相机(1.18-2.4)、配件、SD卡\u003cbr\u003e\n12/23/2024: 旅行社处理材料，送签\u003cbr\u003e\n12/28/2024: 预定酒店(1.25-1.28)\u003cbr\u003e\n12/30/2024: 出签证结果\u003cbr\u003e\n12/31/2024: 拿到护照原件\u003cbr\u003e\n01/17/2025: Action5Pro 到货\u003cbr\u003e\n01/20/2025: Nikon Z6II + Nikon24-200 到货\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e序言\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e日本自由行的白日梦从大二一直留到了现在，每次跟别人说起来都让自己笑话，怎么感觉都有点叶公好龙的味道。这种本来一个周末就能实现的小计划在各种“宏伟的”人生计划中显得格格不入，于是在各种借口的催化下去日本玩逐渐变成了一个废案。最近有朋友要去日本，我才意识到那个遥远的白日梦离我好像也不远，经常挂在嘴边的宇治好像也只有一张机票的距离。我才发现好像不需要选择一个自我感动式的纪念日，不需要揣着万事俱备、没有烦心事的完美主义，也能够去“白日梦”的世界里去看一看，这好像不是什么难事。\u003c/p\u003e\n\u003cp\u003e所以我想写一写关于日本自由行的准备工作，比如护照和签证、日程安排、物资准备、日本出行、交流工具、注意事项之类。除了想抵抗一下申请季和实习的沉闷心情，也想为未来半年内的行程做一点小小的规划。大二的时候我看到这份小资料会把它命名为什么呢？\u0026hellip;我想应该叫做白日梦手册 =)。\u003c/p\u003e\n\u003ch3 id=\"物品清单\"\u003e物品清单\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e摄影摄像\n\u003col\u003e\n\u003cli\u003eNikon Z62 + 24-200\n\u003col\u003e\n\u003cli\u003e电池 * 2\u003c/li\u003e\n\u003cli\u003eSD卡\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eDji Action5Pro\n\u003col\u003e\n\u003cli\u003e储存卡\u003c/li\u003e\n\u003cli\u003e1.5m延申杆\u003c/li\u003e\n\u003cli\u003e章鱼支架\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e转换插头 * 2\u003c/li\u003e\n\u003cli\u003e充电宝 - rosmos 60000mAh\u003c/li\u003e\n\u003cli\u003e数据线\n\u003col\u003e\n\u003cli\u003elightning\u003c/li\u003e\n\u003cli\u003eType-C\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e笔记本电脑\u003c/li\u003e\n\u003cli\u003e转换插头\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e凭证材料\n\u003col\u003e\n\u003cli\u003e护照原件\u003c/li\u003e\n\u003cli\u003e电子签证：纸质\u003c/li\u003e\n\u003cli\u003e入境申报单：截图\u003c/li\u003e\n\u003cli\u003e酒店确认单：纸质\u003c/li\u003e\n\u003cli\u003e保险\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e巡礼材料\n\u003col\u003e\n\u003cli\u003e巡礼照片添加到 Google Map\u003c/li\u003e\n\u003cli\u003eNozomi coat \u0026amp; scarf \u0026amp; backpack \u0026amp; pants\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e应急准备\n\u003col\u003e\n\u003cli\u003e手电筒\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e出行用品\n\u003col\u003e\n\u003cli\u003e流量卡\u003c/li\u003e\n\u003cli\u003e纸巾+湿巾\u003c/li\u003e\n\u003cli\u003e垃圾袋\u003c/li\u003e\n\u003cli\u003e雨披+雨鞋\u003c/li\u003e\n\u003cli\u003e眼罩\u003c/li\u003e\n\u003cli\u003e多功能收纳包\u003c/li\u003e\n\u003cli\u003e额外装商品的包\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e货币相关\n\u003col\u003e\n\u003cli\u003eVisa信用卡\u003c/li\u003e\n\u003cli\u003e银行卡\u003c/li\u003e\n\u003cli\u003e零钱卡槽\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"todo-list\"\u003eTodo List\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e巡礼打卡 - 买周边 - 骏河屋\u003c/li\u003e\n\u003cli\u003e寺庙抽签 - 御守\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"护照和签证\"\u003e护照和签证\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e护照\u003c/strong\u003e\u003cbr\u003e\n护照很容易申请，走正常手续时间也很短，一般一周左右可以拿到实体的护照本。首先需要携带身份证、拍照衣物 (为了上镜一点) 、120元人民币工本费、10元邮寄费去当地的出入境管理局。在上海办理地点可以参考\u003ca href=\"https://gaj.sh.gov.cn/crj/jgjs/dzsjylb\"\u003e地点列表\u003c/a\u003e。在办理地点领取表单填写、在照相室拍照、等待窗口办理、填写地点办理邮寄即可。等待一周过后（实际上只等待了五个工作日）领取快递即可。个人评价中国护照超级好看，拿在手里很有质感。\u003c/p\u003e","title":"日本自由行：白日梦手册"},{"content":"Homepage\n(ELMo) Deep contextualized word representations Before Reading Authors are from AI2 and UW. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.\nMotivation ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation.\nELMo: Embedidngs from LM ELMo is built on biLM respresentations. BiLM gives prediction of token @t_k@ by combining forward and backward LM. Log likelihood of token @t_k@ is given by:\n$$\r\\begin{aligned} \u0026 \\sum_{k=1}^N\\left(\\log p\\left(t_k \\mid t_1, \\ldots, t_{k-1} ; \\Theta_x, \\vec{\\Theta}_{L S T M}, \\Theta_s\\right)\\right. \\\\ \u0026 \\left.\\quad+\\log p\\left(t_k \\mid t_{k+1}, \\ldots, t_N ; \\Theta_x, \\overleftarrow{\\Theta}_{L S T M}, \\Theta_s\\right)\\right)\\end{aligned}\r$$where @N@ is the number of tokens, @\\Theta@ denotes parameters, with subscript @x@ as token representations and @s@ as softmax layer. Note that parameters of forward and backward LM are separately maintained.\nELMo is the combination of intermediate respresentation in biLSTMs where representation set with @L@-layer biLM is given by:\n$$\r\\begin{aligned} R_k \u0026 =\\left\\{\\mathbf{x}_k^{L M}, \\overrightarrow{\\mathbf{h}}_{k, j}^{L M}, \\overleftarrow{\\mathbf{h}}_{k, j}^{L M} \\mid j=1, \\ldots, L\\right\\} \\\\ \u0026 =\\left\\{\\mathbf{h}_{k, j}^{L M} \\mid j=0, \\ldots, L\\right\\}\\end{aligned}\r$$where @\\mathbf{h}_{k, 0}^{L M}@ is the token layer and @\\mathbf{h}_{k, j}^{L M} = [\\overrightarrow{\\mathbf{h}}_{k, j}^{L M}; \\overleftarrow{\\mathbf{h}}_{k, j}^{L M}]@. Basically, EMLo concatenates hidden representation of forward and backward LSTM model by layer. Architecture shown in Figure 1.\nFig. 1 ELMo architecture (illustration form BERT)\nCollapsed ELMo representations are used in downstream NLP tasks. The author adds scale parameters @\\gamma^{\\text{task}}@ and softmax-normalized weights @s^{\\text{task}}@ for different layers:\n$$\r\\mathbf{E L M o}_k^{\\text {task }}=E\\left(R_k ; \\Theta^{\\text {task }}\\right)=\\gamma^{\\text {task }} \\sum_{j=0}^L s_j^{\\text {task }} \\mathbf{h}_{k, j}^{L M}.\r$$ELMo vector could either be added in inputs for enhanced representation @[x_k;\\mathbf{ELMo}_k]@ or be concatenated with output @[h_k;\\mathbf{ELMo}_k]@.\nFor computational requirements, the author cuts hidden dimensions to half (to 512) and incorporate residual connections from the first to second layer. CNN-BIG-LSTM trained for 10 epochs yield 39.7 on average forward and backward perplexity, with 9.7 increase compared with forward CMM-BIG-LSTM\nExperiments Tasks and datasets \u0026amp; mectrics:\nQA: SQuAD, F1 Textual entailment: SNLI, accuracy Semantic role labeling: SRL, F1 (OntoNotes) Conference resolution: OntoNotes coreference annotation, avg. F1 NER: Reuters RCV1 corpus, accuracy SST-5: , F1 Adding ELMo representations yields SOTA results, as illustrated in Figure 2.\nFig. 2 Results by adding ELMo across 6 tasks.\nWhere to add ELMo?: The author add the representation in the lowest layer in this paper yet claims that some tasks may prefer adding representation in the output of the layer.\nDifferences between layers: for tasks like Word Sense Disambiguation, last layer is better than the first layer probably because of semantic meanings in final layer. However, for tasks like POS Tagging, as structural information is needed, the first layer outperforms the last layer.\nEfficiency in sampling: In the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set. Faster convergence by adding \u0026ldquo;offsets\u0026rdquo; to vectors in high dimension space, which helps model be optimized towards optimal points efficiently?\nImproving Language Understanding by Generative Pre-Training Before reading Authors are from OpenAI (Ilya Sutskever!). Cited by 11755 (30/11/2024).\nHow time flies \u0026hellip; 6 years passed and OpenAI has grown into a renowned tech company with $3.4 billion annual revenue. GPT chat bot is well known by people around the globe and everyone can enjoy part of the bonus that AI continues to bring to our society. But Ilya and many other founders left OpenAI with a growing concern about LLM safety; AGI is coming yet it seems like an illusion given the poor performance of current LLM bots. Well, just embrace the changes and move forward and grow up with AI.\nThe paper introduced a semi-supervised approach by combining pre-training and fine-tuning. Authors also introduced a task-specific input adaption startegy for fine-tuning.\nMotivation Training on labeled data has received successful results on NLP tasks, while using unlabeled data is challenging. The optimization objective is unclear and there is no consensus on effective way of transfer learning with learnt representations.\nFramework There are two stages of training procedure: unsupervised pretraining and supervised fine-tuning. For fine-tuning, the paper introduces a task-agnostic approach to better adapt learnt representations to spcific tasks.\nUnsupervised pre-training\nClassic Transformer Decoder next word prediction with multi-head attention, FFN \u0026hellip; Next word prediction objective is given by\n$$\rL_1(\\mathcal{U})=\\sum_i \\log P\\left(u_i \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right)\r$$$$\r\\begin{aligned}\rh_0 \u0026= UW_e + W_p \\\\\rh_l \u0026= \\text{transformer_block}(h_{l - 1}), \\forall \\in [1, n] \\\\\rP(u) \u0026= \\text{softmax}(h_nW_e^{T})\r\\end{aligned}\r$$\nwhere @\\mathcal{U = \\{u_1, \\dots, u_{i - 1}\\}}@ are unsupervised tokens, parameters @\\Theta@, @W_e@ token embedding matrix, @W_p@ position embedding matrix.\nSupervised fine-tuning\nWe get labeled dataset @\\mathcal{C}@ in supervised fine-tuning, in which input tokens @x^{i}, i \\in [1, m]@ are labeled with @y@. @y@ prediction is formulated as\n$$\rP\\left(y \\mid x^1, \\ldots, x^m\\right)=\\operatorname{softmax}\\left(h_l^m W_y\\right).\r$$ Objective is given by\n$$\rL_2(\\mathcal{C})=\\sum_{(x, y)} \\log P\\left(y \\mid x^1, \\ldots, x^m\\right).\r$$In order to improve generalization and to speed up convergence, the objective is given by\n$$\rL_3(\\mathcal{C}) = L_2(\\mathcal{C}) + \\lambda \\cdot L_1(\\mathcal{C}).\r$$ Task-specific input transformations\nThe paper also introduced a task-specific strategy in fine-tuning so as to aviod making extensive changes to the model architecture across tasks. Startegy is illustrated in Figure 3. Fig. 3: (left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks. We convert all structured inputs into tokensequences to be processed by our pre-trained model, followed by a linear+softmax layer.\nSounds like structured prompt input.\nExperiments Setups: For pre-training, the paper use BooksCorpus dataset \u0026amp; 1B Word Benchmark (used by ELMo), because both datasets contains long and contigious contexts. For fine-tuning, parameters are learning rate 6.25e-5, batchsize 32, linear learning rate decay with 0.2% training warm up.\nResults: 4 downstream NLP tasks in fine-tuning: Natural Language Inference (recognizing textual entailment), Question answering and commonsense reasoning, Semantic Similarity, Classification. The approach achieved SOTA in 9 out of 12 datasets and works well on both small and large datasets\nAnalysis\nImpact of the number of transferred layers on overall performance: all layers are useful and each layer adds approx. 9% of performance increase on datasets RACE and Mutlti NLI. Zero-shot performance of pretraining models on NLP tasks: performance steadily increases as over pretraining, which suggests that generative pretraining supports the learning of a wide variety of task relevant functionality. Ablation studies: Performance without auxuliary LM objective @L_1(\\mathcal{C})@:larger dataset benefit from @L_1(\\mathcal{C})@ while smaller dataset not. Importance of using Transformer: the author compares Transformer with LSTM using the same framework. Transformer outperforms LLMs on most tasks. Importance of pretraining: performance drops when the model is trained directly on tasks without pertraining BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Before Reading Authors are from Google AI Language. Citation 120133 (30/11/2024). Paper accepted by NAACL 2019, awarded with Best Long Paper.\nThe paper introduces BERT, a bidirectional pretraining method using Transformer. The representations are learnt from left to right and right to left, which provides better representation.\nMotivation There are two strategies in applying pre-trained language representations: feature-based methods (ELMo) and fine-tuning (OpenAI GPT). However, the current approached limited power of representation bacause of the nature of learning from left to right. BERT uses bidirectional training and applied two novel pretraining objectives, namely MLM and NSP, to get pretraininig representations of both token-level and sentence-level. BERT also reduce the need of task-specific archituctures.\nBERT Training: There are two steps of BERT: pre-training and fine-tuning. During pre-training, BERT utilize two objectives to get pre-training representations. Fine-tuning is firstly initialized with pre-trained parameters and all of the parameters are fine-tuned.\nArchitecture: BERT is basically a multi-layer bidirectional Transformer encoder, which is different from GPT constrained by left-to-right nature. (In my opinion, bidirectional is mostly illustrated by attention mechanism in encoder)\nInput/Output Representations: In BERT, the input sequence might be a single sentence or a pack of two sentences. The first token is always [CLS] and seperation token between two sentences is [SEP]. For two sentences, the author add learned segment embedding @E_A, E_B@ to mark tokens in two sentences @A@ and @B@. The final input is the summation of token, segment embedding and position embedding, as illustrated in Figure 4.\nFig. 4 BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\nPre-training:\nMasked LM (MLM): Because of bi-directional feature of BERT, each word would see itself, therefore we need a new pre-training objective. Inspired by Cloze task, the author decided to randomly mask out 15% tokens in each sequence by replacing it with token [MASK]. However, since token [MASK] will not appear in fine-tuning stage after pre-training, there is a mismatch between pre-training and fine-tuning tokens. The authoer then elaborates on the detailed approach of setting [MASK]: within 15% tokens, 80% tokens are replaced by [MASK], 10% tokens are replaced by a random token and the rest stay unchanged. Next Sentence Prediction (NSP): A simple binarized task of deciding whether the sentence @B@ is the next sentence of sentence @A@ in sentence pair @[A, B]@. BERT thus learned sentence-level information. Pre-training data: document-level literature with long contexts, such as BooksCorpus and English Wikipedia in order ot extract contiguous sequences.\nFine-tuning: Plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. Sentence @[A, B]@ could be interpreted as different meanings like QA, hypothesis-premise pairs etc..\nExperiments Tested on four different tasks:\nGLUE: last hidden state + classification weights + softmax SQuAD v1.1: QA pairs, predict on the answer span index @[i, j]@ SQuAD v2.0: The answer probably does not exists in contexts. No answer -\u0026gt; span from [CLS] to [CLS]. The rule of SQuAD also applies. SWAG: Given a sentence, the task is to choose the most plausible continuation among four choices. Ablations Effect of Pre-training tasks: experiments on No NSP, LTR(left2right)\u0026amp;No NSP removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1 The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. Effect of Model Size: scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. We hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small. Ablation w/o fine-tuning Pre-computed representations lower down the costs BERT is effective for both finetuning and feature-based approaches (only pre-training). RoBERTa: A Robustly Optimized BERT Pretraining Approach Before Reading 17176 citation up to 01/09/2025. It\u0026rsquo;s a follow-up work of BERT, in which the author introduced better settings for BERT model training.\nMotivation The RoBERTa proposed an improved receipe for training BERT models. Main settings are training time, size of batches, elimination of NSP training, longer sequence and dynamic masking patterns. The results showed improved performance on metrics in BERT.\nTraining Settings of BERT BERT is optimized with Adam with @\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon=1e-6@ and @L_2@ weight decay of @0.01@ (warm up 10000 steps to 1e-4 and linearly decayed). Dropout rate 0.1 on all layers. GELU activation. Models trained for 1000000 updates with 256 as batchsize and max-length 512. Models are trained with mixed precision floating point, 8xV100.\nTraining data includes BOOKCORPUS, CC-NEWS, OPENWEBTEXT and STORIES.\nTraining Analysis Dynamic masking and static masking: To avoid using the same mask for each epoch, the training data were duplicated 10 times and were masked with different ways for each epoch. This was introduced in BERT and called static masking. While for dynamic masking, masking patterns are generated every time we feed a sequence to the model. And \u0026hellip; as the results presented, we indeed see the increase though being marginal.\nNext Sentence Prediction: NSP loss was questioned by replication experiments. The authors found:\nUsing individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies. Removing the NSP loss matches or slightly improves downstream task performance. Restricting sequences to come from a single document performs slightly better than packing sequences from multiple documents Training with large batches: Training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training.\nText Encoding: Train BERT model using Byte-Pair Encoding. RoBERTa Training settings Three points: Dynamic masking, trained with FULL-SENTENCES dataset without NSP loss, large mini-batches and byte-level BPE. More settings revolves around data used for pretraining and number of passes through the data.\nThe author further conbimed three datasets for training (160GB) and trained the model from 100K to 500K steps.\nEvaluations Models are evaluated on GLUE, SQuAD and RACE.\nGLUE\nThere are 2 types of tasks: single-task and emsembled task in GLUE. RoBERTa was finetuned for single task on each training dataset based on pretrained model. And for ensembled task, RoBERTa did not depend on multi-task finetuning. Instead, for RTE, STS and MRPC, the model was fine-tuned on MNLI single-task model.\nSQuAD\nRoBERTa finetuned only on SQuAD training data without data augmentation like previous works. The single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation.\nRACE\nEach candidate answer was concatenated with the corresponding question and passage. The total length is at most 512 tokens.\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators Before Reading Accepted by ICLR 2020. Authors are from Stanford and Google Brain (Manning!). Citation 4424 up to 01/23/2025.\nMajor improvements on pretraining with MASK (Masked language modeling task, MLM) in BERT. Instead of training with fixed [MASK] token, ELECTRA predicts whether the word is replaced by generator or not, which means all tokens in the input will be considered as prediction objectives.\nMotivation In MLM task, only 15% of the tokens are learnt by the model as the number of masked tokens are limited. In this work, the author proposed a pretraining task replaced token detection, in which the model learns to distinguish real input tokens from generated replacements. The side-product of this setting is that it solves mismatch between training and testing in BERT. (Remember 15%-80%-10%-10%?) Note that the model is seen as a generator that predicts the original identity of corrupted tokens. Moreover, the ELECTRA also features with compute-efficiency and parameter-efficiency in pretraining stage.\nMethod There are two NNs in this work, namely Generator and Discriminator. The generator is in charge of putting mask on input sentence and generating corrupted sentence by replacing masks with other words. The discriminator then tries to distinguish which word in the corrupted sentence is replaced by Generator. In Generator mask words @m_i@, which follows uniform distribution:\n$$\rm_i \\sim \\operatorname{unif}\\{1, n\\} \\text{ for } i=1 \\text{ to } k \\quad \\mathbf{x}^{\\text {masked }}=\\operatorname{REPLACE}(\\mathbf{x}, \\mathbf{m},[ \\text{MASK} ])\r$$\nwhere @\\text{REPLACE(\\mathbf{x}, \\mathbf{m}, p)}@ means replace masked elements in @\\mathbf{x}@ with p using @\\mathbf{m}@ as mask. @h@ are hidden representations and @e@ are embeddings of generator encoder. Then the masked elements @m_i@ are replaced with new words @\\hat x_i@, which follows the distribution given by softmax normalization:\n$$\r\\begin{aligned}\r\\hat{x}_i \u0026\\sim p_G\\left(x_i \\mid \\mathbf{x}^{\\text { masked }}\\right)\\text{ for } i \\in \\mathbf{m} \\\\\rp_G\\left(x_t \\mid \\mathbf{x}\\right)\u0026=\\exp \\left(e\\left(x_t\\right)^T h_G(\\mathbf{x})_t\\right) / \\sum_{x^{\\prime}} \\exp \\left(e\\left(x^{\\prime}\\right)^T h_G(\\mathbf{x})_t\\right) \\\\\r\\mathbf{x}^{\\text {corrupt }}\u0026=\\operatorname{REPLACE}(\\mathbf{x}, \\mathbf{m}, \\hat{\\mathbf{x}}) \\end{aligned}\r$$The corrupted input @\\mathbf{x}^{\\text{corrupt}}@ is the input of Discriminator, which tries to distinguish the word replaced by Generator. The possibility for each word is given by:\n$$\rD(\\mathbf{x}^{\\text {corrupt}}, t) = \\text{sigmoid}(w^{T}h_{D}(\\mathbf{x}^{\\text {corrupt }})_t)\r$$\nThe loss function is the combination of MLM task and discrimination task. Figure 4 illustrates ELECTRA using an example.\nFig 4. An overview of replaced token detection. The generator can be any model that producesan output distribution over tokens, but we usually use a small masked language model that is trainedjointly with the discriminator. Although the models are structured like in a GAN, we train thegenerator with maximum likelihood rather than adversarially due to the difficulty of applying GANsto text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks.\n$$\r\\begin{aligned}\r\u0026\\min _{\\theta_G, \\theta_D} \\sum_{\\mathbf{x} \\in \\mathcal{X}} \\mathcal{L}_{\\mathrm{MLM}}\\left(\\mathbf{x}, \\theta_G\\right)+\\lambda \\mathcal{L}_{\\text {Disc }}\\left(\\mathbf{x}, \\theta_D\\right) \\\\\r\u0026 \\mathcal{L}_{\\mathrm{MLM}}\\left(\\mathbf{x}, \\theta_G\\right)=\\mathbb{E}\\left(\\sum_{i \\in \\mathbf{m}}-\\log p_G\\left(x_i \\mid \\mathbf{x}^{\\text {masked }}\\right)\\right) \\\\ \u0026 \\mathcal{L}_{\\mathrm{Disc}}\\left(\\mathbf{x}, \\theta_D\\right)=\\mathbb{E}\\left(\\sum_{t=1}^n-\\mathbb{1}\\left(x_t^{\\mathrm{corrupt}}=x_t\\right) \\log D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)-\\mathbb{1}\\left(x_t^{\\text {corrupt }}f \\neq x_t\\right) \\log \\left(1-D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)\\right)\\right)\r\\end{aligned}\r$$ Disc loss: cross-entropy loss for discrimination.\nDifference with GAN If the generated token happens to ben correct, the token will be considered \u0026ldquo;real\u0026rdquo; instead of \u0026ldquo;fake\u0026rdquo;. The generator is trained with maximum likelihood rather than being trained adversarially to fool the\ndiscriminator. (Ad training is challenging because of it is impossible to backpropergate through sampling from generator.) Experiments Datasets are GLUE, SQuAD. EM and F1 scores.\nModel extension: Some techiques used in initialization and training. Weight sharing: share all/parts of parameters between generator and discriminator Smaller generators: large models generates challenging tasks for discriminator, and sometimes being too hard to answer by discriminator. Smaller generator works effectively. (small model here: keep some of params in generator constant without updating in BP) Training algorithms: Two stage procedure: training MLM task for n steps; initialize params in discriminator using params in trained generator. Train the discriminator with generator frozen. The author also discusses about the small and large ELECTRA models using weaker training hyperparameters. Further discussions are about the efficiency of ELECTRA. Thr author designed three variations to test token learning in ELECTRA. Results suggest that a large amount of ELECTRA’s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. (btw proves 10% random replacement in BERT is insufficient to solve the issue).\nReferences [1] Tsang, S. (2022, January 8). Review — ELMO: Deep Contextualized Word Representations. Medium. https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c\n[2] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., \u0026amp; Zettlemoyer, L. (2018, February 15). Deep contextualized word representations. arXiv.org. https://arxiv.org/abs/1802.05365\n[3] Radford, A. (2018). Improving language understanding by generative pre-training.\n[4] Devlin, J., Chang, M., Lee, K., \u0026amp; Toutanova, K. (2018, October 11). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv.org. https://arxiv.org/abs/1810.04805\n[5] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., \u0026amp; Stoyanov, V. (2019, July 26). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv.org. https://arxiv.org/abs/1907.11692\n[6] Clark, K., Luong, M., Le, Q., V., \u0026amp; Manning, C. D. (2020, March 23). ELECTRA: Pre-training text encoders as discriminators rather than generators. arXiv.org. https:// arxiv.org/abs/2003.10555\n","permalink":"http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/","summary":"\u003cp\u003e\u003ca href=\"https://www.cs.princeton.edu/courses/archive/fall22/cos597G/\"\u003eHomepage\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"elmo-deep-contextualized-word-representations\"\u003e(ELMo) Deep contextualized word representations\u003c/h2\u003e\n\u003ch3 id=\"before-reading\"\u003eBefore Reading\u003c/h3\u003e\n\u003cp\u003eAuthors are from \u003ca href=\"https://allenai.org/\"\u003eAI2\u003c/a\u003e and \u003ca href=\"https://www.cs.washington.edu/\"\u003eUW\u003c/a\u003e. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation.\u003c/p\u003e","title":"COS597G 22 Encoder Only Models"},{"content":" Homepage\nHuman Language Understanding \u0026amp; Reasoning Introductory reading authored by Christopher D. Manning.\nBrief introduction of NLP history The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms.\nThe second era was from 1970 to 1992 and systems were able to deal with syntax and reference in human language. The new generation of hand-built systems had a clear separation between declarative linguistic knowledge and its procedural processing and which benefited from the development of a range of more modern linguistic theories.\nNLP dramatically changed in the third era, 1993 - 2012 because of the emergence of digital text. At the beginning, researchers tend to extract certain model from a large corpus of data by counting certain facts. Early attempts to learn language structure from text collections were fairly unsuccessful, which led most of the field to concentrate on constructing annotated linguistic resources. Supervised machine learning dominates NLP techniques.\nThe last era features with deep learning and growing artificial intelligence methods. Word \u0026amp; sentence embedding went viral. From 2013 to 2018, deep learning promotes the advantages of embedding thus leading NLP techiques to vector spaces. In 2018, very large scale self-supervised neural network succeeded in learning an enormous amount of knowledge by simly exposed to large contexts. Representative tasks are net word prediction and filling masked words or phrases.\nNow-dominant neural network Since 2018, the dominant neural network model for NLP applications has been the transformer neural network. The dominant idea is one of attention, by which a representation at a position is computed as a weighted combination of representations from other positions. Masked word prediction turns out to be very powerful because it is universal: every form of linguistic and world knowledge, from sentence structure, word connotations, and facts about the world, help one to do this task better. As a result, these models assemble a broad general knowledge of the language and world to which they are exposed.\nWhat can we do with LPLMs? Multilingual machine translation trained on all languages simutaneously; for other tasks like QA, sentiment classification, NER and fluent text generation, LPLMs turns out to be the best solution.\nProspects What\u0026rsquo;s the meaning in contexts? The dominant approach to describing meaning is a denotational semantics approach or a theory of reference: the meaning of a word, phrase, or sentence is the set of objects or situations in the world that it describes. This contrasts with the simple distributional semantics (or use theory of meaning) of modern empirical work in NLP, whereby the meaning of a word is simply a description of the contexts in which it appears. Manning claims that meaning arises from understanding the network of connections between a linguistic form and other things, whether they be objects in the world or other linguistic forms. Using this definition whereby understanding meaning consists of understanding networks of connections of linguistic forms, there can be no doubt that pretrained language models learn meanings. As well as word meanings, they learn much about the world.\nOne of the exciting prospects is learning from multi modal data, such as vision, robotics, knowledge graphs, bioinformatics, and multimodal data. Manning also mentions external database as the source of model while he still addresses the importance of multi-modal learning.\nWe will witness the comming of foundation models, with its specializations handling most information processing and analysis tasks. There might be concerns of risks that foundation models are controlled by several powerful and influencial groups and somehow it will\nbe difficult to tell if models are safe to use in particular contexts because the models and their training data are so large. Manning believes in the limitation of models while also gives postive comments on their utility and foresees the future that models are widly deployed.\nAttention is All You Need Transformer architecture is firstly introduced in this work.\nBefore Reading Attention is All You Need is well known for its contribution of Transformer architecture and therefore probably be seen as the inception of LLM era. 14k citations well demonstrate its significance. Authors are from Google Brain team or UofT. All of the authors shared the same contribution. Paper was accepted by NIPS 2017.\nIntroduction RNNs established SOTA approaches in sequence modeling while fell short of efficiency because of its non-parallizable computation. Previous attention mechanisms attempted to solving the problem yet only in few cases or by combining RNNs. The author proposed Transformer architecture to deal with parallelization by relying entirely on an attention mechanism.\nBackground Previous attempts on reducing sequential computation is to use convolutional neural networks. Though Convs proved efficient, the operations to relate signals from different positions grows either linearly or logarithmically. Transformer coinstrains the number of operations to constant and counteract resolution cost by applying Multi-head Attention. Self-attention performs well on a wide range of tasks and Transformer relies on self-attention without combining with RNNs.\nModel Architecture Key concepts: encoder-decoder structure, stacked self-attention, fully connected layers\nThe Transformer - model architecture.\nEncoder\nEncoder is composed of 6 identical stacked layers which contains 2 sub-layers. Residual connection are employed around each of the two sub-layers, followed by layer normalization. Sub-layers are multi-head self-attention layer and a simple, position-wise fully connected feed-forward network. Output dimension @d_{\\text{model}} = 512@\nDecoder\nDecoder is composed of 6 identical stacked layers. Self-attention sub-layer is modified by applying mask, which prevent the model from attending to subsquent positions.\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nScaled Dot-Product Attention\nTwo common attention functions: additive attention and dot-product attention. The difference is compatibility function. Additive attention use a feed-forward network with a single hidden layer while dot-product attention use Query and Key, which resembles to attention mechanism introduced in the paper. The two are similar in complexity but the latter could be optimized by matrix multiplicatoin, thus being more space-efficient in practice.\nScaled Dot-Product Attention:\n$$\r\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V\r$$For larger values of @d_k@, result of @QK^T@ is going to be large, which falls into @\\text{softmax}@ regions where its has extremely small gradients. Therefore, scaling is added. Notice that the scaling factor is the dimension of Key: @1 / \\sqrt{d_k}@\nMulti-Head Attention Instead of performing a single attention function with @d_{\\text{model}}@-dimensional keys, values and queries, it is better to linearly project the queries, keys and values @h@ times with different, learned linear projections to @d_k@, @d_k@ and @d_v@ dimensions, respectively. Computation is carried out in parallel and output values are concatenated and once again projected.\n(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\nPerform linear output transformation after concatenation.\n$$\r\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_i)W^O\r$$where @\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^{K}, VW_i^{V})@. Dimensions (Sequence length @n@): @K \\in \\mathbb{R}^{n \\times d_{\\text{model}}}, W_{i}^K \\in \\mathbb{R}^{d_{\\text{model}}\\times d_k},@ @ W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}@. In the paper @h = 8, d_k = d_v = d_{\\text{model}} / h = 64@. @d_{\\text{model}}@ could be seen as dimension of embedding.\nApplications of Attention In deocoder, query comes from last layer while key and values comes from encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Mask is implemented inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\nPosition-wise FFN FFN consists of two linear transformation with ReLU activation n between.\n$$\r\\text{FFN} = \\text{ReLU}(xW_1 +b_1)W_2 + b_2\r$$Dimensions: inner dimension 2048, output dimension 512.\nEmbeddings and Softmax The implemetation share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by @\\sqrt{d_{\\text{model}}}@.\nPositional Embedding Sine and cosine functions:\n$$\r\\begin{aligned}\rPE_{(pos, 2i)} \u0026= \\sin(pos / 10000^{2i / d_{\\text{model}}}) \\\\\rPE_{(pos, 2i + 1)} \u0026= \\cos(pos / 10000^{2i / d_{\\text{model}}})\r\\end{aligned}\r$$\nwhere @pos@ is the position and @i@ is the dimension. The author also tries positional embeddings, which yields similar results. But sin/cos PE could extrapolate to sequence longer inputs.\nWhy Self-Attention Total computational complexity per layer Type Complexity per Layer Self-Attention @O(n^2 \\cdot d)@ Self-Attention(restricted) @O(r \\cdot n \\cdot d)@ Recurrent @O(n \\cdot d^2)@ Conv @O(k \\cdot n \\cdot d^2)@ where @d@ is the representation dimension and @n@ is the length of sequence. Self-attention does not perform well when @n \u003e d@ compared with RNNs. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size @r@ in the input sequence centered around the respective output position.\nThe amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The path length between long-range dependencies in the network. More interpretable models by investigating attention values. Training Experiments on machine translation: WMT 2014 English-German/French. 8xP100 GPU, Adam optimizaer, regularization: residual dropout \u0026amp; label smoothing. Metrics: BLEU, Trainig cost(FLOPs)\nResults BLEU: EN-DE 28.4 EN-FR 41.8\nEnglish constituency parsing for checking ability of task generalization.\nAblation Number of attention heads Attention key size @d_k@ Model size Other positional embeddings Blog Post: The Illustrated Transformer Visualizing QK In encoder, @QK@ matrix mutiplication could be seen as quries multiplying keys, then get summation.\nVisualization of Encoder self-attention.\nEncoder-Decoder Attention Detailed version of encoder-decoder architecture:\nVisualization of encoder-decoder architecture\nWhat happened between encoder and decoder: encoder-decoder architecture gif:\nEncoder-decoder attention\nPosition Embedding Visualization Position Embedding Visualization\nx-axis: Embedding dimension; y-axis: Token position.\nAbout model outputs Because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Parameter temperature can affect output as well by adding a scaling factor: @\\text{softmax(x / T)}@.\nReferences [1] C. D. Manning, \u0026ldquo;Human Language Understanding \u0026amp; Reasoning,\u0026rdquo; journal-article, 2022. [Online]. Available: https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf\n[2] A. Vaswani et al., \u0026ldquo;Attention Is All You Need,\u0026rdquo; arXiv.org, Jun. 12, 2017. https://arxiv.org/abs/1706.03762\n[3] J. Alammar, \u0026ldquo;The Illustrated Transformer.\u0026rdquo; https://jalammar.github.io/illustrated-transformer/\n","permalink":"http://localhost:1313/learning/llm/cos597g-22-introduction/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://www.cs.princeton.edu/courses/archive/fall22/cos597G/\"\u003eHomepage\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"human-language-understanding--reasoning\"\u003eHuman Language Understanding \u0026amp; Reasoning\u003c/h2\u003e\n\u003cp\u003eIntroductory reading authored by \u003ca href=\"https://nlp.stanford.edu/~manning/\"\u003eChristopher D. Manning\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"brief-introduction-of-nlp-history\"\u003eBrief introduction of NLP history\u003c/h3\u003e\n\u003cp\u003eThe NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms.\u003c/p\u003e","title":"COS597G 22 Introduction"},{"content":"Overview An inspiring book in changing the personal finance view of the readers. The wealth never comes easy and especially tougher in the era of Rat Race where individuals exploit themselves to get the salary as the rewards of their \u0026ldquo;diligence\u0026rdquo;. The deeply rooted view held by majorities and me, before I encountered with the book, is subverted by Robert T.Kiyosaki in Rich Dad Poor Dad. Working in Rat Race provides us with stable or growing income and, at the same time, turns the master of money \u0026ndash; us \u0026ndash; into slaves who is fettered by conventional mindset of personal finance. The book enlightened readers with the way out of Rat Race and secrets of building personal wealth.\nCurrently I am at my transition from an college student detached from society to an adult who confronts with real world problems. And the primary concern of my first step out of college comfort zone is the topic of money. Unfortunately, I did not ever have any common sense with personal finace and it was actually kind of overwhelming when I received salary from my first internship. Therefore, out of concerns and curiosity, I searched for books and videos for finance management online and that is when I came across with Rich Dad Poor Dad. And I was glad the book comes with a plain yet powerful style in writting, which is accessible even to students in high school, nevertheless presents us with insights into personal wealth.\nThe book starts with education from \u0026ldquo;Rich\u0026rdquo; dad and \u0026ldquo;Poor\u0026rdquo; dad, which represents two types of views on money and wealth. The poor and middle class stick to salary and liabilities and reluctant to take the risk of investments on assets while the rich generate money with better arranged cash flow between income and assets, which is described as \u0026ldquo;Each dollar is employeed by you and work for you 24 hour 7 days a week\u0026rdquo;. The stocks, bonds, real states and the business run by your agents work for the owner without their presence onsite. Though simple as it may sounds, coming up with the way of making the money work for you is never easy and that is exactly where finacial intelligence works. Finacial Literacy and Finacial Intelligence are both the prerequisites for obtaining wealth or being finacially independent. The author suggests accounting, investing, understanding the market and learning the law are four technical pillars of financial intelligence and their synergy will be of great help in the pursuit of wealth. Apart from technical skills, psychology of investing is another important factor in personal finance action. Until we have overcome fear, cynicism, laziness, arrogance and get bad habits kicked off, we could develop our assets columns which will bring us with large cash flow.\nLots of bloggers and youtubers recommand the book as the best introductory reading material of personal finance and my reading experience attest to their applause as an accessible, powerful and thought-provoking book designed for financial novices.\nReview by Chapter:\nLesson 1: The Rich Don\u0026rsquo;t Work For Money The poor and the middle class work for money. The rich have money work for them. People\u0026rsquo;s lives are forever controlled by two emotions: fear and greed. So many people say, \u0026ldquo;Oh, I\u0026rsquo;m not interested in money.\u0026rdquo; Yet they\u0026rsquo;ll work at a job for eight hours a day. Lesson 2: Why Teach Financial Literacy It\u0026rsquo;s not how much the money you make. It\u0026rsquo;s how much money you keep. Rich people acquire assets. The poor and middle class acquire liabilities that they think are assets. You must know the difference between an asset and a liability, and buy assets. Cash flow tells the story of how a person handles money. Observations: The rich buy assets. The poor only have expenses. The middle class buy liabilities they think are assets. Rule #1: You must know the difference between an asset and a liability, and buy assets.\ncash-flow pattern of an asset:\nCash-flow pattern of an asset\nCash-flow pattern of liability\nCash-flow pattern of the poor\nCash-flow pattern of the middle class\nFinancial statements\nLesson 3: Mind Your Own Bussiness The rich focus on their asset columns while everyone else focuses on their income statements. Financial struggle is often the result of people working all their lives for someone else. Keep your daytime job, but start buying real assets, not liabilities or personal effects that have no real value once you get them home. Real assets fall into the following categories: Businesses that do not require my presence Stocks Bonds Income-generating real state Notes(IOUs) Businesses that do not require my presence Royalties from intellectual property such as music, scripts, and patents Anything else that has value,produces income or appreciates, and has a ready market Lesson 4: The History of Taxes and the Power of Corperations My rich dad just played the game smart, and he did it through corporations—the biggest secret of the rich. My rich dad did not see Robin Hood as a hero.He called Robin Hood a crook. If you work for money, you give the power to you employer. If money works for you, you keep the power and control it. If you work for money, you give the power to you employer. If money works for you, you keep the power and control it. Financial IQ is made up of knowledge from four broad areas of expertise: Accounting Investing Understanding markets The law Tax advantages Protection from lawsuits Corporate structure\nLesson 5: The Rich Invent Money Games reflect behavior. They are instant feedback systems. The single most powerful asset we all have is our mind. If it is trained well, it can create enormous wealth. The single most powerful asset we all have is our mind. If it is trained well, it can create enormous wealth. It is not gambling if you know what you’re doing. It is gambling if you’re just throwing money into a deal and praying. Great opportunities are not seen with your eyes. They are seen with your mind. If you want to be the investor that creates invenstments, you need to develop three main skills: If you want to be the second type of investor,you need to develop three main skills. Raise money. Organize smart people. The single most powerful asset we all have is our mind. If it is trained well, it can create enormous wealth.\nLesson 6: Work to Learn - Don\u0026rsquo;t Work for Money Job security meant everything to my educated dad. Learning meant everything to my rich dad. \u0026ldquo;You want to know a little about a lot\u0026rdquo; was rich dad’s suggestion. Job is an acronym for \u0026ldquo;Just Over Broke.\u0026rdquo; The main management skills needed for success are: Management of cash flow Management of systems Management of people Lesson 7 Overcoming Obstacles Fear: The primary difference between a rich person and a poor person is how they manage fear. For most people, the reason they don\u0026rsquo;t win financially is because the pain of losing money is far greater than the joy of being rich. Failure inspires winners. Failure defeats losers. FOCUS: Follow One Course Until Successful. Cynicism: Doubt is expensive. Laziness So what is the cure for laziness? The answer is—a little greed. Rich dad believed that the words \u0026lsquo;I can’t afford it\u0026rsquo; shut down your brain. \u0026lsquo;How can I afford it?\u0026rsquo; opens up possibilities, excitement, and dreams. Bad Habits Arrogance Lesson 8 Getitng Started There is gold everywhere. Most people are not trained to see it. 10 steps as a process to develop your God-given powers, powers over which only you have control. Find a reason greater than reality: the power of spirit Make daily choices: the power of choice Choose friends carefully: the power of association Master a formula and then learn a new one: the power of learning quickly Pay yourself first: the power of self-discipline To successfully pay yourself first, keep the following in mind: Don’t get into large debt positions that you have to pay for. When you come up short, let the pressure build and don’t dip into your savings or investments. Pay your brokers well: the power of good advice Be an Indian giver: the power of getting something for nothing The sophisticated investor’s first question is:“How fast do I get my money back?” They also want to know what they get for free, also called a “piece of the action.” That is why the ROI, or return on investment, is so important. I move a sizable amount of money into the stock of a company that he feels is just about to make a move that will add value to the stock, like announcing a new product. I will move my money in for a week to a month while the stock moves up.Then I pull my initial dollar amount out, and stop worrying about the fluctuations of the market, because my initial money is back and ready to work on another asset. So my money goes in, and then it comes out, and I own an asset that was technically free. Use assets to buy luxuries: the power of focus Choose heroes: the power of myth But heroes do more than simply inspire us.Heroes make things look easy. Making it look easy convinces us to want to be just like them. If they can do it, so can I. Teach and you shall receive: the power of giving Still Want More? Here Are Some To Do\u0026rsquo;s Stop doing what you’re doing. Stop doing what is not working, and look for something new. Look for new ideas. Find someone who has done what you want to do. Take them to lunch and ask them for tips and tricks of the trade. Take classes, read, and attend seminars. Make lots of offers. More suggestions:\nFinding a good deal, the right business, the right people, the right investors, or whatever is just like dating. Jog, walk, or drive a certain area once a month for 10 minutes. Shop for bargains in all markets. Look in the right places. Look for people who want to buy first.Then look for someone who wants to sell. Think big. Learn from history. Action always beats inaction. Three forms of income:\nOrdinary earned Portfolio Passive All of you were given two great gifts: your mind and your time. It is up to you to do what you please with both. I wish you great wealth and much happiness with this fabulous gift called life. – Robert Kiyosaki\nMore Reading More books recommanded (credit to Youtuber 艾财说imoneytalk) CashFlow game ","permalink":"http://localhost:1313/ideas/rich-dad-poor-dad-review/","summary":"\u003ch3 id=\"overview\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eAn inspiring book in changing the personal finance view of the readers. The wealth never comes easy and especially tougher in the era of Rat Race where individuals exploit themselves to get the salary as the rewards of their \u0026ldquo;diligence\u0026rdquo;. The deeply rooted view held by majorities and me, before I encountered with the book, is subverted by Robert T.Kiyosaki in \u003cem\u003eRich Dad Poor Dad\u003c/em\u003e. Working in Rat Race provides us with stable or growing income and, at the same time, turns the master of money \u0026ndash; us \u0026ndash; into slaves who is fettered by conventional mindset of personal finance. The book enlightened readers with the way out of Rat Race and secrets of building personal wealth.\u003c/p\u003e","title":"Rich Dad Poor Dad Review"},{"content":"我和 7 这个数字很有缘分。\n初高中时每次考试分发座位表，我都会第一时间去查看自己的班级号和座位号。在我有限的印象里，我的号码总是与 7 这个数字有着各种联系：7？17？14？21？考试时看到这个并不和谐的质数总是会点亮一下我的心情，这个在数字文化里被冷落的符号对我来说有一种天然的亲近感。\n我不太记得哪个网站说过，七年是全身细胞更新的周期，在生理上，我相对于七年之前的自己是一个全新的存在。我知道每一个器官组织的更新速率不同，但是总要选一个平均值的上限来代表一下这个过程，一个带着点宿命味道的数字是再好不过了。我倒是很喜欢这个不长不短的时间，还带着点儿初高中回忆的味道。今年 21 岁，正好经过了三次更新迭代，兢兢业业工作的器官组织们都至少走过了三个轮回。在十多个轮回中，三个七年仅仅是三分之一的时光，在这个节点上去回顾这短短的二十一年未免有些做作，但是简单回头去看看过去的自己倒是一件蛮有趣的事情。\n在第一个七年的节点我在上二三年级，刚刚从一楼带着铁栅栏的教室升入二楼的小教室里。四班在走廊的堵头，挨着小水池倒还有一点乐趣。夏天的时候班上总有几个不怕挨打的小孩拿着灌好的水气球扔来扔去，我是不太去玩这种水气球炸弹，但是也不妨碍我在旁边观摩几员猛将的拼杀，有时候也会被误伤就是了。被告知不好的事情坚决不碰是我小小的脑袋瓜里的唯一哲学，我好像也是家长和老师面前的好孩子。我记得最骄傲的事情是被选上写字课代表，每周抱着全班的练字本在座位上用红笔批改。写字本沉甸甸的，放在桌膛里边整整齐齐，也是一个小小的成就。好孩子的标准是不说脏话不吃辣条不到处乱跑，但是从根本上说这些有点坏的习惯对我来说从来没有存在的必要。每天的生活对我这个地球玩家新手来说仍然很明亮，这个小镇好大，我怎么也探索不完每一个有趣的角落。我记得端午节和家人去山上采带着露珠的艾草，下完雨去找松树墩旁边的小蘑菇，秋天躺在金黄的落叶床垫上闻一闻泥土的腥味，每次回来那种筋疲力竭躺在床上的感觉总是能让我很满足。放学路上我看到路尽头金红色的小山丘披着西斜的阳光，我总是感觉到一股暖流腾起。从林子里回来，我应该也裁了一片小小的阳光留在我心里了吧。学校微机室的大肚子电脑、夏天三十九度的教室里凉凉的红色小水桶、长满齐腰高杂草的神秘小水沟、查理九世的青铜棺和游戏王的三幻神好像都在回忆的黑柔滤镜下变得有点不太真实。\n在第二个七年的节点我已经是初三的学生了。在转学之后的教育体系中，小学只有五年而初中有四年的时间。三年时间对于一个十多岁的毛头小子来说很漫长，我只记得分数从及格线爬到了班级前几，我从一个小透明也似乎变成了有点存在感的小角色了。关于初三一半的记忆是灰色纸的试卷，堆成山的作业试题和每周每一天考试的心惊胆战。我们学校的学习强度还是比较有名的，每一周对于数学英语物理化学必定会有标准考试，不定时语文也会占用时间来考试，我就在这每一周的循环中经历心跳和肾上腺素的波动起伏。我记得初三的我总是有那种莫名的恐惧，生怕被别人超过分数或者名次，于是在作业练习里边疯狂训练，每一周的考试都当作证明自己的机会一样极为重视。结果当然不会辜负这样“自残”一样的努力，我的名字也能和那些被大家誉为“大神”的角色写在一起了，老师和同学们似乎也比以前温和了许多，我好像获得了阶段性的成功。是吗？我好像更加敏感了，那些成绩和排名兜兜转转都离不开周围的评价。至于对我自己我可能自始至终都没有认知，我只知道在半夜一点刷完这本习题册能让我在下次考试当中多一点胜算，分数高一点这样我也可以高兴地度过这周余下的几天，然后再迎接下一周的血压飙升。在成绩起伏大时候我会自己报复自己，不需要老师督促，毕竟我是自诩的“好孩子”嘛。怕落在别人后边是我唯一的想法。关于初三的另一半呢是一位女生。我记得有一段时间是坐在我的斜后方，她平时比较内向，我们的交集也并不是很多。初三学生的喜欢总是停留在对只言片语和微小的动作理解上，有些幼稚的行为现在想想也是颇有趣的。有时候多交谈几句能够开心好几天；悄悄地把她的QQ设为特别关心，在节日打了很多字删掉送上最普通的祝福；给她讲题时略带兴奋但也小心翼翼克制着不想让对方发现\u0026hellip;\u0026hellip;我记得有一次同学打赌，比写练习册的页数，页数少的人给某某表白，我表白的对象正好是她。我怎么都是不好意思的，于是在当天晚上写了八十多页练习册，是对方的八倍多\u0026hellip;\u0026hellip;哈哈哈这些事情真的是幼稚到极点，比安昙小太郎和水野茜的初期发展还要缓慢，但是相信我那时能够每次上学看到她就已经很开心了吧。我不会形容初三学生双向暗恋的感觉，我只知道当时的世界对于七年前的我来说还是太过复杂，这两种记忆交织着描绘了七年前的我的生活，混着铅灰色和玫瑰色的色彩。七年前的我还好吗？\n第三个七年的结尾是在刚上大四的秋天，我带着七年的回忆来到北京开始了很不顺利的实习生活。高中的三年有太多太多情感，我永远不会忘记三年中的很多瞬间，那是第三节人生中最最珍贵的三年。紧接着是高考砸锅、转专业成功、专业学习长征、科研阻力和长时间的焦虑与迷茫。奇怪的是，这七年我感觉被按下了加速键，我找不到在时间轴上被偷走的那几年。那几段时间就那么堂而皇之地溜走了。有留下什么吗？我倒是觉得大学三年所有情绪的重量都留下了，有一小部分开心的，有大部分焦虑的，也有一部分是无所谓的。大三课程结束我终于意识到大学生活这场漫长的跋涉竟然快走到了尽头，那个日夜埋头赶路的自己应该会很欣慰吧。七年旅途的末尾有上低音号的尾音，有实验没有起色的数字，有utown的落日和晚霞。我倒是又捡起了相机去拍拍星轨，拿起标日初级看看泡温泉的小李，看完了之前总是想查但是总忘记的导演的电影，给自己留出近乎奢侈的时间来思考一些天马行空的问题。我也会躺在床上刷上几个小时的 bilibili，在屏幕前对着新老 cp 傻笑，闷头一觉睡到中午。但是也许现在这个处在七年节点的我也可以说，在这第三节里我的生活过的实实在在。\n我昨天看了小坂流加老师作品改编的 餘命10年 电影，如果单从剧情的角度这部电影没有什么新奇，甚至可以说某些情节有一些老套和不合理。但是每次看到茉莉挣扎着在最后的日子里生活时我却总能被感触到，那种在存活百分制概率下生活的希望和动力让我有些触动。小坂流加老师没能看到作品上映，但是我能够想到在病床前写下这些文字的情景，那种强烈的想要延展生活的渴望。在过去两年里，我有些时候觉得我患上了虚无感的绝症，在各种压力的催化下这种症状更加明显，恶性循环之中导致了无语和迷茫。我弄不明白这生活热情到底是什么，跟着环境亦步亦趋也许是折衷之策吧，毕竟大学三年我都是这样。所以看到茉莉我才意识到，生活本身就是一首最朴实且丰富的诗歌，尽全力去想下一刻想要做的事，尽全力去做下一刻想要做的事已经赋予了生活本身最好的意义，哪怕是已经在绝症当中。\n我记得高中的时候老师留了一篇议论文周记，是有关生活与死亡的讨论，我洋洋洒洒地写了很长一篇有理有据的论述死亡意义的文章，但是关于生活的部分只是起到陪衬死亡这个“明显更加重大的话题”的作用，我还清晰地记得老师用了一句论语的句子做批注：\n未知生，焉知死？ ——《论语 · 先进篇》\n我当时是不以为然，毕竟高中生想法的倔劲谁都有过感受，现在回想起来不禁有些感动。海德格尔在《存在与时间》中有过向死而生的表述，在茉莉的生活中，这份因死而生的心情表达的淋漓尽致。有时候我会胡乱瞎想，如果我真的得了什么绝症，那最后存活的那几年一定是我未来人生中最精彩的几年吧，那种生活的色彩一定会超过过去单色生活的总和。我们意识不到过于遥远的终点，在当下似乎漫漫的旅途当中做些无关紧要的小事浪费时间也不会影响下一段旅程。\n所以七年的时间节点突然跳了出来。\n在七年之后我可以说“我”一定会死亡，那么在这之前我想有怎样的生活呢？站在第四个七年的起点处，我希望在七年之后的我可以带着七年充满生命力的生活回忆满足地走入生理周期的坟墓，然后由下一个我开启另一段向死而生的旅程。\n我想稍稍修改一下小坂流加老师的作品名称：\n餘命 7 年。\n","permalink":"http://localhost:1313/ideas/%E9%A4%98%E5%91%BD-7-%E5%B9%B4/","summary":"\u003cp\u003e我和 7 这个数字很有缘分。\u003c/p\u003e\n\u003cp\u003e初高中时每次考试分发座位表，我都会第一时间去查看自己的班级号和座位号。在我有限的印象里，我的号码总是与 7 这个数字有着各种联系：7？17？14？21？考试时看到这个并不和谐的质数总是会点亮一下我的心情，这个在数字文化里被冷落的符号对我来说有一种天然的亲近感。\u003c/p\u003e\n\u003cp\u003e我不太记得哪个网站说过，七年是全身细胞更新的周期，在生理上，我相对于七年之前的自己是一个全新的存在。我知道每一个器官组织的更新速率不同，但是总要选一个平均值的上限来代表一下这个过程，一个带着点宿命味道的数字是再好不过了。我倒是很喜欢这个不长不短的时间，还带着点儿初高中回忆的味道。今年 21 岁，正好经过了三次更新迭代，兢兢业业工作的器官组织们都至少走过了三个轮回。在十多个轮回中，三个七年仅仅是三分之一的时光，在这个节点上去回顾这短短的二十一年未免有些做作，但是简单回头去看看过去的自己倒是一件蛮有趣的事情。\u003c/p\u003e\n\u003cp\u003e在第一个七年的节点我在上二三年级，刚刚从一楼带着铁栅栏的教室升入二楼的小教室里。四班在走廊的堵头，挨着小水池倒还有一点乐趣。夏天的时候班上总有几个不怕挨打的小孩拿着灌好的水气球扔来扔去，我是不太去玩这种水气球炸弹，但是也不妨碍我在旁边观摩几员猛将的拼杀，有时候也会被误伤就是了。被告知不好的事情坚决不碰是我小小的脑袋瓜里的唯一哲学，我好像也是家长和老师面前的好孩子。我记得最骄傲的事情是被选上写字课代表，每周抱着全班的练字本在座位上用红笔批改。写字本沉甸甸的，放在桌膛里边整整齐齐，也是一个小小的成就。好孩子的标准是不说脏话不吃辣条不到处乱跑，但是从根本上说这些有点坏的习惯对我来说从来没有存在的必要。每天的生活对我这个地球玩家新手来说仍然很明亮，这个小镇好大，我怎么也探索不完每一个有趣的角落。我记得端午节和家人去山上采带着露珠的艾草，下完雨去找松树墩旁边的小蘑菇，秋天躺在金黄的落叶床垫上闻一闻泥土的腥味，每次回来那种筋疲力竭躺在床上的感觉总是能让我很满足。放学路上我看到路尽头金红色的小山丘披着西斜的阳光，我总是感觉到一股暖流腾起。从林子里回来，我应该也裁了一片小小的阳光留在我心里了吧。学校微机室的大肚子电脑、夏天三十九度的教室里凉凉的红色小水桶、长满齐腰高杂草的神秘小水沟、查理九世的青铜棺和游戏王的三幻神好像都在回忆的黑柔滤镜下变得有点不太真实。\u003c/p\u003e\n\u003cp\u003e在第二个七年的节点我已经是初三的学生了。在转学之后的教育体系中，小学只有五年而初中有四年的时间。三年时间对于一个十多岁的毛头小子来说很漫长，我只记得分数从及格线爬到了班级前几，我从一个小透明也似乎变成了有点存在感的小角色了。关于初三一半的记忆是灰色纸的试卷，堆成山的作业试题和每周每一天考试的心惊胆战。我们学校的学习强度还是比较有名的，每一周对于数学英语物理化学必定会有标准考试，不定时语文也会占用时间来考试，我就在这每一周的循环中经历心跳和肾上腺素的波动起伏。我记得初三的我总是有那种莫名的恐惧，生怕被别人超过分数或者名次，于是在作业练习里边疯狂训练，每一周的考试都当作证明自己的机会一样极为重视。结果当然不会辜负这样“自残”一样的努力，我的名字也能和那些被大家誉为“大神”的角色写在一起了，老师和同学们似乎也比以前温和了许多，我好像获得了阶段性的成功。是吗？我好像更加敏感了，那些成绩和排名兜兜转转都离不开周围的评价。至于对我自己我可能自始至终都没有认知，我只知道在半夜一点刷完这本习题册能让我在下次考试当中多一点胜算，分数高一点这样我也可以高兴地度过这周余下的几天，然后再迎接下一周的血压飙升。在成绩起伏大时候我会自己报复自己，不需要老师督促，毕竟我是自诩的“好孩子”嘛。怕落在别人后边是我唯一的想法。关于初三的另一半呢是一位女生。我记得有一段时间是坐在我的斜后方，她平时比较内向，我们的交集也并不是很多。初三学生的喜欢总是停留在对只言片语和微小的动作理解上，有些幼稚的行为现在想想也是颇有趣的。有时候多交谈几句能够开心好几天；悄悄地把她的QQ设为特别关心，在节日打了很多字删掉送上最普通的祝福；给她讲题时略带兴奋但也小心翼翼克制着不想让对方发现\u0026hellip;\u0026hellip;我记得有一次同学打赌，比写练习册的页数，页数少的人给某某表白，我表白的对象正好是她。我怎么都是不好意思的，于是在当天晚上写了八十多页练习册，是对方的八倍多\u0026hellip;\u0026hellip;哈哈哈这些事情真的是幼稚到极点，比安昙小太郎和水野茜的初期发展还要缓慢，但是相信我那时能够每次上学看到她就已经很开心了吧。我不会形容初三学生双向暗恋的感觉，我只知道当时的世界对于七年前的我来说还是太过复杂，这两种记忆交织着描绘了七年前的我的生活，混着铅灰色和玫瑰色的色彩。七年前的我还好吗？\u003c/p\u003e\n\u003cp\u003e第三个七年的结尾是在刚上大四的秋天，我带着七年的回忆来到北京开始了很不顺利的实习生活。高中的三年有太多太多情感，我永远不会忘记三年中的很多瞬间，那是第三节人生中最最珍贵的三年。紧接着是高考砸锅、转专业成功、专业学习长征、科研阻力和长时间的焦虑与迷茫。奇怪的是，这七年我感觉被按下了加速键，我找不到在时间轴上被偷走的那几年。那几段时间就那么堂而皇之地溜走了。有留下什么吗？我倒是觉得大学三年所有情绪的重量都留下了，有一小部分开心的，有大部分焦虑的，也有一部分是无所谓的。大三课程结束我终于意识到大学生活这场漫长的跋涉竟然快走到了尽头，那个日夜埋头赶路的自己应该会很欣慰吧。七年旅途的末尾有上低音号的尾音，有实验没有起色的数字，有utown的落日和晚霞。我倒是又捡起了相机去拍拍星轨，拿起标日初级看看泡温泉的小李，看完了之前总是想查但是总忘记的导演的电影，给自己留出近乎奢侈的时间来思考一些天马行空的问题。我也会躺在床上刷上几个小时的 bilibili，在屏幕前对着新老 cp 傻笑，闷头一觉睡到中午。但是也许现在这个处在七年节点的我也可以说，在这第三节里我的生活过的实实在在。\u003c/p\u003e\n\u003cp\u003e我昨天看了小坂流加老师作品改编的 \u003cem\u003e餘命10年\u003c/em\u003e 电影，如果单从剧情的角度这部电影没有什么新奇，甚至可以说某些情节有一些老套和不合理。但是每次看到茉莉挣扎着在最后的日子里生活时我却总能被感触到，那种在存活百分制概率下生活的希望和动力让我有些触动。小坂流加老师没能看到作品上映，但是我能够想到在病床前写下这些文字的情景，那种强烈的想要延展生活的渴望。在过去两年里，我有些时候觉得我患上了虚无感的绝症，在各种压力的催化下这种症状更加明显，恶性循环之中导致了无语和迷茫。我弄不明白这生活热情到底是什么，跟着环境亦步亦趋也许是折衷之策吧，毕竟大学三年我都是这样。所以看到茉莉我才意识到，生活本身就是一首最朴实且丰富的诗歌，尽全力去想下一刻想要做的事，尽全力去做下一刻想要做的事已经赋予了生活本身最好的意义，哪怕是已经在绝症当中。\u003c/p\u003e\n\u003cp\u003e我记得高中的时候老师留了一篇议论文周记，是有关生活与死亡的讨论，我洋洋洒洒地写了很长一篇有理有据的论述死亡意义的文章，但是关于生活的部分只是起到陪衬死亡这个“明显更加重大的话题”的作用，我还清晰地记得老师用了一句论语的句子做批注：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e未知生，焉知死？ ——《论语 · 先进篇》\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e我当时是不以为然，毕竟高中生想法的倔劲谁都有过感受，现在回想起来不禁有些感动。海德格尔在《存在与时间》中有过向死而生的表述，在茉莉的生活中，这份因死而生的心情表达的淋漓尽致。有时候我会胡乱瞎想，如果我真的得了什么绝症，那最后存活的那几年一定是我未来人生中最精彩的几年吧，那种生活的色彩一定会超过过去单色生活的总和。我们意识不到过于遥远的终点，在当下似乎漫漫的旅途当中做些无关紧要的小事浪费时间也不会影响下一段旅程。\u003c/p\u003e\n\u003cp\u003e所以七年的时间节点突然跳了出来。\u003c/p\u003e\n\u003cp\u003e在七年之后我可以说“我”一定会死亡，那么在这之前我想有怎样的生活呢？站在第四个七年的起点处，我希望在七年之后的我可以带着七年充满生命力的生活回忆满足地走入生理周期的坟墓，然后由下一个我开启另一段向死而生的旅程。\u003c/p\u003e\n\u003cp\u003e我想稍稍修改一下小坂流加老师的作品名称：\u003c/p\u003e\n\u003cp\u003e餘命 7 年。\u003c/p\u003e","title":"餘命 7 年"},{"content":"安装与切换语言 安装：（Windows10）设置 -\u0026gt; 时间和语言 -\u0026gt; 语言 -\u0026gt; 首选语言 -\u0026gt; 添加语言 -\u0026gt; 日语 切换语言： Alt + Shift 或者 Windows + Space 切换相关 Ctrl + caps lock： 片假名输入切换为平假名输入 Alt + caps lock：平假名输入切换为片假名输入 F6: 片假名输入过程切换为平假名 F7: 平假名输入过程切换为片假名 F8: 变窄 (ｹｰｷ) 使用 Alt + ` 切换日文与英文 输入 长音 按照字面音打字，片假名长音打减号\n高校（こうこう）koukou コーヒー　ko-hi- 拨音 两个 n\n任務（にんむ）ninnmu 促音 促音后的音的辅音重复\n実際（じっさい）jissai 雑誌（ざっし）zasshi 拗音与外来语特殊拗音\n正常拗音正常输入，小写字符前加 x 或者 l, 记得写片假名切换 Alt + caps lock\n地球（ちきゅう）chikyuu 写真（しゃしん）shashinn/syashinn 女性（じょせい）jyosei/josei ティ texi フェ fuxe ヴ vu 特殊助词 按照五十音\n私は中国人です　ha 家へ行きました he ビールを飲みます wo Reference [1] “win10 日文输入法的安装与使用_win10日文输入法-CSDN博客.” https://blog.csdn.net/qq_40309341/article/details/103823853\n[2] 日语朋友YC老师, “安卓手机/WIN10电脑日语输入法添加安装使用一条龙教学！无法安装时有啥替代输入法？附特殊音节输入方法！,” YouTube. Jan. 19, 2023. [Online]. Available: https://www.youtube.com/watch?v=I0kXEmB4daA\n","permalink":"http://localhost:1313/learning/japanese/%E6%97%A5%E8%AF%AD%E8%BE%93%E5%85%A5%E6%B3%95/","summary":"\u003ch3 id=\"安装与切换语言\"\u003e安装与切换语言\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e安装：（Windows10）设置 -\u0026gt; 时间和语言 -\u0026gt; 语言 -\u0026gt; 首选语言 -\u0026gt; 添加语言 -\u0026gt; 日语\u003c/li\u003e\n\u003cli\u003e切换语言： \u003ccode\u003eAlt + Shift\u003c/code\u003e 或者 \u003ccode\u003eWindows + Space\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"切换相关\"\u003e切换相关\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eCtrl + caps lock\u003c/code\u003e： 片假名输入切换为平假名输入\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAlt + caps lock\u003c/code\u003e：平假名输入切换为片假名输入\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eF6\u003c/code\u003e: 片假名输入过程切换为平假名\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eF7\u003c/code\u003e: 平假名输入过程切换为片假名\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eF8\u003c/code\u003e: 变窄 (ｹｰｷ)\u003c/li\u003e\n\u003cli\u003e使用 Alt + ` 切换日文与英文\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"输入\"\u003e输入\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e长音\u003c/strong\u003e 按照字面音打字，片假名长音打减号\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e高校（こうこう）koukou\u003c/li\u003e\n\u003cli\u003eコーヒー　ko-hi-\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e拨音\u003c/strong\u003e 两个 n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e任務（にんむ）ninnmu\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e促音\u003c/strong\u003e 促音后的音的辅音重复\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e実際（じっさい）jissai\u003c/li\u003e\n\u003cli\u003e雑誌（ざっし）zasshi\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e拗音与外来语特殊拗音\u003c/strong\u003e\u003cbr\u003e\n正常拗音正常输入，小写字符前加 \u003ccode\u003ex\u003c/code\u003e 或者 \u003ccode\u003el\u003c/code\u003e, 记得写片假名切换 \u003ccode\u003eAlt + caps lock\u003c/code\u003e\u003c/p\u003e","title":"日语输入法"},{"content":"Text This is a plain text. Text with Stress, Italic, Del line. Text with inline item.\nGIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nKatex Inline MathJax: @\\sum_{i}a_i@\nBlock MathJax:\n$$\r\\int_{a}^{b}f(t)\\text{d}t\r$$\\mathbb, \\mathcal test\n$$\r\\mathbb{ABCDEFG}\\mathcal{ABCDEFG}\r$$Environment test\n$$\r\\begin{aligned}\rA \u0026= aaa \\\\\r\u0026= bbb \\rightarrow\r\\end{aligned}\r$$Coding Test for Python\ndef test(param): \u0026#34;\u0026#34;\u0026#34;Test for hugo website Args: param: parameter of function Return: type of param \u0026#34;\u0026#34;\u0026#34; return type(param) Test for C++\n#include \u0026lt;iostream\u0026gt; using namespace std; int test(int param){ // Test comment return true; } line number:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; highlights\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Image Tables A B C D a b c d ","permalink":"http://localhost:1313/learning/hugo/hugo-feature-test/","summary":"\u003ch3 id=\"text\"\u003eText\u003c/h3\u003e\n\u003cp\u003eThis is a plain text. Text with \u003cstrong\u003eStress\u003c/strong\u003e, \u003cem\u003eItalic\u003c/em\u003e, \u003cdel\u003eDel line\u003c/del\u003e. Text with \u003ccode\u003einline item\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003eGIF\u003c!-- raw HTML omitted --\u003e is a bitmap image format.\u003c/p\u003e\n\u003cp\u003eH\u003c!-- raw HTML omitted --\u003e2\u003c!-- raw HTML omitted --\u003eO\u003c/p\u003e\n\u003cp\u003eX\u003c!-- raw HTML omitted --\u003en\u003c!-- raw HTML omitted --\u003e + Y\u003c!-- raw HTML omitted --\u003en\u003c!-- raw HTML omitted --\u003e = Z\u003c!-- raw HTML omitted --\u003en\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e","title":"Hugo Feature Test"}]