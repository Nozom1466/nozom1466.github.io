<!doctype html><html lang=en-US data-theme=dark><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.152.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="COS597G 22 Introduction"><meta itemprop=description content="Keep it simple, keep it powerful."><meta name=description content="Keep it simple, keep it powerful."><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="http://localhost:1313/profileMode/avatar.jpg"><meta itemprop=keywords content="LLM,COS597G:2022"><meta property="og:type" content="article"><meta property="og:title" content="COS597G 22 Introduction"><meta property="og:description" content="Keep it simple, keep it powerful."><meta property="og:image" content="/profileMode/avatar.jpg"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="http://localhost:1313/learning/llm/cos597g-22-introduction/"><meta property="og:site_name" content="Ryan's Blog"><meta property="og:locale" content="en-US"><meta property="article:author" content="NexT Theme"><meta property="article:published_time" content="2024-11-15 22:37:35 +0800 +0800"><meta property="article:modified_time" content="2024-11-15 22:37:35 +0800 +0800"><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/font-awesome/6.7.2/css/all.min.css><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/viewerjs/1.11.6/viewer.min.css><link rel=stylesheet href="/css/main.css?=1765088039"><style type=text/css>.post-footer hr:after{content:"~ End Line ~"}.flinks-list-footer hr:after{content:"~ End Line ~"}</style><link rel=stylesheet type=text/css href="/css/custom_style.css?=1765088039"><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return 0[0];const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),0[0]):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>COS597G 22 Introduction - Ryan's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Ryan's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Build for Hugo Theme</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>Home</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>Archives
<span class=badge>14</span></a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-tags hvr-icon"></i>Tags</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>Search</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=Searching... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>TOC</li><li class=sidebar-nav-overview>Overview</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#human-language-understanding--reasoning>Human Language Understanding & Reasoning</a><ul><li><a href=#brief-introduction-of-nlp-history>Brief introduction of NLP history</a></li><li><a href=#now-dominant-neural-network>Now-dominant neural network</a></li><li><a href=#what-can-we-do-with-lplms>What can we do with LPLMs?</a></li><li><a href=#prospects>Prospects</a></li></ul></li><li><a href=#attention-is-all-you-need>Attention is All You Need</a><ul><li><a href=#before-reading>Before Reading</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#background>Background</a></li><li><a href=#model-architecture>Model Architecture</a><ul><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#applications-of-attention>Applications of Attention</a></li><li><a href=#position-wise-ffn>Position-wise FFN</a></li><li><a href=#embeddings-and-softmax>Embeddings and Softmax</a></li><li><a href=#positional-embedding>Positional Embedding</a></li><li><a href=#why-self-attention>Why Self-Attention</a></li></ul></li><li><a href=#training>Training</a><ul><li><a href=#results>Results</a></li><li><a href=#ablation>Ablation</a></li></ul></li></ul></li><li><a href=#blog-post-the-illustrated-transformer>Blog Post: The Illustrated Transformer</a><ul><li><a href=#visualizing-qk>Visualizing QK</a></li><li><a href=#encoder-decoder-attention>Encoder-Decoder Attention</a></li><li><a href=#position-embedding-visualization>Position Embedding Visualization</a></li><li><a href=#about-model-outputs>About model outputs</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt="NexT Theme" src=/imgs/img-lazy-loading.gif data-src=/profileMode/avatar.jpg><p class=site-author-name itemprop=name>NexT Theme</p><div class=site-description itemprop=description>Keep it simple, keep it powerful.</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>14</span>
<span class=site-state-item-name>Posts</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>Categories</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>14</span>
<span class=site-state-item-name>Tags</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/Nozom1466 title="Github → https://github.com/Nozom1466" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>
Github
</a></span><span class=links-of-social-item><a href=https://x.com/NozomiKasa1466 title="Twitter → https://x.com/NozomiKasa1466" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-twitter fa-fw hvr-icon"></i>
Twitter</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en class=cc-opacity rel=noopener target=_blank title="Creative Commons"><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt="Creative Commons"></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
Links</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>Web Status</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>Running:</div><div class=item-count id=runTimes data-publishdate="2024-09-22 14:40:51 +0800 +0800"></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>Visitors:</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>Views:</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>Words:</div><div class=item-count id=wordsCount data-count=23874></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>ReadTime:</div><div class=item-count id=readTimes data-times=74></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>Last Update:</div><div class=item-count id=last-push-date data-lastpushdate="2025-05-27 22:59:47 +0800 +0800"></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=goto-i18n-translate class=button title="Multilingual translation"><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title="Change Theme"><i class="fas fa-adjust"></i></div></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=http://localhost:1313/learning/llm/cos597g-22-introduction/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/profileMode/avatar.jpg"><meta itemprop=name content="NexT Theme"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="NexT Theme"><meta itemprop=description content="Keep it simple, keep it powerful."></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="COS597G 22 Introduction"><meta itemprop=description content="
    

    Homepage
    
    
    


  
Human Language Understanding & Reasoning

Introductory reading authored by 

    Christopher D. Manning
    
    
    
.
Brief introduction of NLP history

The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms."></span><header class=post-header><h1 class=post-title itemprop="name headline">COS597G 22 Introduction</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="fas fa-solid fa-calendar"></i>
</span><span class=post-meta-item-text title="Publish on">Publish on:
</span><time title="Create Time:2024/11/15 22:37:35 +08:00" itemprop="dateCreated datePublished" datetime="2024-11-15 22:37:35 +0800 +0800">2024/11/15</time></span></div><div class=post-meta-items></div></div></header><div class="post-body autonumber" itemprop=articleBody><blockquote><p><a href=https://www.cs.princeton.edu/courses/archive/fall22/cos597G/ title=Homepage rel="noopener external nofollow noreferrer" target=_blank class=exturl>Homepage
<i class="fa fa-external-link-alt"></i></a></p></blockquote><h2 id=human-language-understanding--reasoning>Human Language Understanding & Reasoning
<a class=header-anchor href=#human-language-understanding--reasoning></a></h2><p>Introductory reading authored by
<a href=https://nlp.stanford.edu/~manning/ title="Christopher D. Manning" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Christopher D. Manning
<i class="fa fa-external-link-alt"></i>
</a>.</p><h3 id=brief-introduction-of-nlp-history>Brief introduction of NLP history
<a class=header-anchor href=#brief-introduction-of-nlp-history></a></h3><p>The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms.</p><a id=more></a><p>The second era was from 1970 to 1992 and systems were able to deal with syntax and reference in human language. The new generation of hand-built systems had a clear separation between declarative linguistic knowledge and its procedural processing and which benefited from the development of a range of more modern linguistic theories.</p><p>NLP dramatically changed in the third era, 1993 - 2012 because of the emergence of digital text. At the beginning, researchers tend to extract certain model from a large corpus of data by counting certain facts. Early attempts to learn language structure from text collections were fairly unsuccessful, which led most of the field to concentrate on constructing annotated linguistic resources. Supervised machine learning dominates NLP techniques.</p><p>The last era features with deep learning and growing artificial intelligence methods. Word & sentence embedding went viral. From 2013 to 2018, deep learning promotes the advantages of embedding thus leading NLP techiques to vector spaces. In 2018, very large scale self-supervised neural network succeeded in learning an enormous amount of knowledge by simly exposed to large contexts. Representative tasks are net word prediction and filling masked words or phrases.</p><h3 id=now-dominant-neural-network>Now-dominant neural network
<a class=header-anchor href=#now-dominant-neural-network></a></h3><p>Since 2018, the dominant neural network model for NLP applications has been the transformer neural network. The dominant idea is one of attention, by which a representation at a position is computed as a weighted combination of representations from other positions. Masked word prediction turns out to be very powerful because it is universal: every form of linguistic and world knowledge, from sentence structure, word connotations, and facts about the world, help one to do this task better. As a result, these models assemble a broad general knowledge of the language and world to which they are exposed.</p><h3 id=what-can-we-do-with-lplms>What can we do with LPLMs?
<a class=header-anchor href=#what-can-we-do-with-lplms></a></h3><p>Multilingual machine translation trained on all languages simutaneously; for other tasks like QA, sentiment classification, NER and fluent text generation, LPLMs turns out to be the best solution.</p><h3 id=prospects>Prospects
<a class=header-anchor href=#prospects></a></h3><p>What&rsquo;s the meaning in contexts? The dominant approach to describing meaning is a denotational semantics approach or a theory of reference: the meaning of a word, phrase, or sentence is the set of objects or situations in the world that it describes. This contrasts with the simple distributional semantics (or use theory of meaning) of modern empirical work in NLP, whereby the meaning of a word is simply a description of the contexts in which it appears. Manning claims that meaning arises from understanding the network of connections between a linguistic form and other things, whether they be objects in the world or other linguistic forms. Using this definition whereby understanding meaning consists of understanding networks of connections of linguistic forms, there can be no doubt that pretrained language models learn meanings. As well as word meanings, they learn much about the world.</p><p>One of the exciting prospects is learning from multi modal data, such as vision, robotics, knowledge graphs, bioinformatics, and multimodal data. Manning also mentions external database as the source of model while he still addresses the importance of multi-modal learning.</p><p>We will witness the comming of foundation models, with its specializations handling most information processing and analysis tasks. There might be concerns of risks that foundation models are controlled by several powerful and influencial groups and somehow it will
be difficult to tell if models are safe to use in particular contexts because the models and their training data are so large. Manning believes in the limitation of models while also gives postive comments on their utility and foresees the future that models are widly deployed.</p><h2 id=attention-is-all-you-need>Attention is All You Need
<a class=header-anchor href=#attention-is-all-you-need></a></h2><p>Transformer architecture is firstly introduced in this work.</p><h3 id=before-reading>Before Reading
<a class=header-anchor href=#before-reading></a></h3><p><em>Attention is All You Need</em> is well known for its contribution of Transformer architecture and therefore probably be seen as the inception of LLM era. 14k citations well demonstrate its significance. Authors are from Google Brain team or UofT. All of the authors shared the same contribution. Paper was accepted by NIPS 2017.</p><h3 id=introduction>Introduction
<a class=header-anchor href=#introduction></a></h3><p>RNNs established SOTA approaches in sequence modeling while fell short of efficiency because of its non-parallizable computation. Previous attention mechanisms attempted to solving the problem yet only in few cases or by combining RNNs. The author proposed Transformer architecture to deal with parallelization by relying entirely on an attention mechanism.</p><h3 id=background>Background
<a class=header-anchor href=#background></a></h3><p>Previous attempts on reducing sequential computation is to use convolutional neural networks. Though Convs proved efficient, the operations to relate signals from different positions grows either linearly or logarithmically. Transformer coinstrains the number of operations to constant and counteract resolution cost by applying Multi-head Attention. Self-attention performs well on a wide range of tasks and Transformer relies on self-attention without combining with RNNs.</p><h3 id=model-architecture>Model Architecture
<a class=header-anchor href=#model-architecture></a></h3><p>Key concepts: encoder-decoder structure, stacked self-attention, fully connected layers</p><figure><img src=/COS597G-introduction/transformer_architecture.png alt="The Transformer - model architecture." width=350><figcaption><p>The Transformer - model architecture.</p></figcaption></figure><ol><li><p>Encoder
Encoder is composed of 6 identical stacked layers which contains 2 sub-layers. Residual connection are employed around each of the two sub-layers, followed by layer normalization. Sub-layers are multi-head self-attention layer and a simple, position-wise fully connected feed-forward network. Output dimension @d_{\text{model}} = 512@</p></li><li><p>Decoder
Decoder is composed of 6 identical stacked layers. Self-attention sub-layer is modified by applying mask, which prevent the model from attending to subsquent positions.</p></li><li><p>Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a <em>compatibility function</em> of the query with the corresponding key.</p></li><li><p>Scaled Dot-Product Attention
Two common attention functions: additive attention and dot-product attention. The difference is <em>compatibility function</em>. Additive attention use a feed-forward network with a single hidden layer while dot-product attention use <em>Query</em> and <em>Key</em>, which resembles to attention mechanism introduced in the paper. The two are similar in complexity but the latter could be optimized by matrix multiplicatoin, thus being more space-efficient in practice.</p></li></ol><p>Scaled Dot-Product Attention:</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
$$</p><p>For larger values of @d_k@, result of @QK^T@ is going to be large, which falls into @\text{softmax}@ regions where its has extremely small gradients. Therefore, scaling is added. Notice that the scaling factor is the dimension of <em>Key</em>: @1 / \sqrt{d_k}@</p><h4 id=multi-head-attention>Multi-Head Attention
<a class=header-anchor href=#multi-head-attention></a></h4><p>Instead of performing a single attention function with @d_{\text{model}}@-dimensional keys, values and queries, it is better to linearly project the queries, keys and values @h@ times with different, learned linear projections to @d_k@, @d_k@ and @d_v@ dimensions, respectively. Computation is carried out in parallel and output values are concatenated and once again projected.</p><figure><img src=/COS597G-introduction/dot-product-attention.png alt="(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel." width=500><figcaption><p>(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.</p></figcaption></figure><p>Perform linear output transformation after concatenation.</p><p>$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_i)W^O
$$</p><p>where @\text{head}<em>i = \text{Attention}(QW_i^Q, KW_i^{K}, VW_i^{V})@. Dimensions (Sequence length @n@): @K \in \mathbb{R}^{n \times d</em>{\text{model}}}, W_{i}^K \in \mathbb{R}^{d_{\text{model}}\times d_k},@ @ W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}@. In the paper @h = 8, d_k = d_v = d_{\text{model}} / h = 64@. @d_{\text{model}}@ could be seen as dimension of embedding.</p><h4 id=applications-of-attention>Applications of Attention
<a class=header-anchor href=#applications-of-attention></a></h4><p>In deocoder, query comes from last layer while key and values comes from encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Mask is implemented inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.</p><h4 id=position-wise-ffn>Position-wise FFN
<a class=header-anchor href=#position-wise-ffn></a></h4><p>FFN consists of two linear transformation with ReLU activation n between.</p><p>$$
\text{FFN} = \text{ReLU}(xW_1 +b_1)W_2 + b_2
$$</p><p>Dimensions: inner dimension 2048, output dimension 512.</p><h4 id=embeddings-and-softmax>Embeddings and Softmax
<a class=header-anchor href=#embeddings-and-softmax></a></h4><p>The implemetation share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by @\sqrt{d_{\text{model}}}@.</p><h4 id=positional-embedding>Positional Embedding
<a class=header-anchor href=#positional-embedding></a></h4><p>Sine and cosine functions:</p><p>$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin(pos / 10000^{2i / d_{\text{model}}}) \
PE_{(pos, 2i + 1)} &= \cos(pos / 10000^{2i / d_{\text{model}}})
\end{aligned}
$$
where @pos@ is the position and @i@ is the dimension. The author also tries positional embeddings, which yields similar results. But sin/cos PE could extrapolate to sequence longer inputs.</p><h4 id=why-self-attention>Why Self-Attention
<a class=header-anchor href=#why-self-attention></a></h4><ol><li>Total computational complexity per layer</li></ol><table><thead><tr><th style=text-align:left>Type</th><th style=text-align:center>Complexity per Layer</th></tr></thead><tbody><tr><td style=text-align:left>Self-Attention</td><td style=text-align:center>@O(n^2 \cdot d)@</td></tr><tr><td style=text-align:left>Self-Attention(restricted)</td><td style=text-align:center>@O(r \cdot n \cdot d)@</td></tr><tr><td style=text-align:left>Recurrent</td><td style=text-align:center>@O(n \cdot d^2)@</td></tr><tr><td style=text-align:left>Conv</td><td style=text-align:center>@O(k \cdot n \cdot d^2)@</td></tr></tbody></table><p>where @d@ is the representation dimension and @n@ is the length of sequence. Self-attention does not perform well when @n > d@ compared with RNNs. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size @r@ in the input sequence centered around the respective output position.</p><ol start=2><li>The amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.</li><li>The path length between long-range dependencies in the network.</li><li>More interpretable models by investigating attention values.</li></ol><h3 id=training>Training
<a class=header-anchor href=#training></a></h3><p>Experiments on machine translation: WMT 2014 English-German/French. 8xP100 GPU, Adam optimizaer, regularization: residual dropout & label smoothing. Metrics: BLEU, Trainig cost(FLOPs)</p><h4 id=results>Results
<a class=header-anchor href=#results></a></h4><p>BLEU: EN-DE 28.4 EN-FR 41.8
<a href=https://paperswithcode.com/task/constituency-parsing title="English constituency parsing" rel="noopener external nofollow noreferrer" target=_blank class=exturl>English constituency parsing
<i class="fa fa-external-link-alt"></i>
</a>for checking ability of task generalization.</p><h4 id=ablation>Ablation
<a class=header-anchor href=#ablation></a></h4><ol><li>Number of attention heads</li><li>Attention key size @d_k@</li><li>Model size</li><li>Other positional embeddings</li></ol><h2 id=blog-post-the-illustrated-transformer>Blog Post: The Illustrated Transformer
<a class=header-anchor href=#blog-post-the-illustrated-transformer></a></h2><h3 id=visualizing-qk>Visualizing QK
<a class=header-anchor href=#visualizing-qk></a></h3><p>In encoder, @QK@ matrix mutiplication could be seen as quries multiplying keys, then get summation.</p><figure><img src=https://jalammar.github.io/images/t/self-attention-output.png alt="Visualization of Encoder self-attention." width=400><figcaption><p>Visualization of Encoder self-attention.</p></figcaption></figure><h3 id=encoder-decoder-attention>Encoder-Decoder Attention
<a class=header-anchor href=#encoder-decoder-attention></a></h3><p>Detailed version of encoder-decoder architecture:</p><figure><img src=https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png alt="Visualization of encoder-decoder architecture" width=600><figcaption><p>Visualization of encoder-decoder architecture</p></figcaption></figure><p>What happened between encoder and decoder: encoder-decoder architecture gif:</p><figure><img src=https://jalammar.github.io/images/t/transformer_decoding_1.gif alt="Encoder-decoder attention" width=600><figcaption><p>Encoder-decoder attention</p></figcaption></figure><h3 id=position-embedding-visualization>Position Embedding Visualization
<a class=header-anchor href=#position-embedding-visualization></a></h3><p><figure><img src=https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png alt="Position Embedding Visualization" width=500><figcaption><p>Position Embedding Visualization</p></figcaption></figure>x-axis: Embedding dimension; y-axis: Token position.</p><h3 id=about-model-outputs>About model outputs
<a class=header-anchor href=#about-model-outputs></a></h3><p>Because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Parameter <code>temperature</code> can affect output as well by adding a scaling factor: @\text{softmax(x / T)}@.</p><h2 id=references>References
<a class=header-anchor href=#references></a></h2><p>[1] C. D. Manning, &ldquo;Human Language Understanding & Reasoning,&rdquo; journal-article, 2022. [Online]. Available:
<a href=https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf title=https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf
<i class="fa fa-external-link-alt"></i>
</a>[2] A. Vaswani et al., &ldquo;Attention Is All You Need,&rdquo; arXiv.org, Jun. 12, 2017.
<a href=https://arxiv.org/abs/1706.03762 title=https://arxiv.org/abs/1706.03762 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/abs/1706.03762
<i class="fa fa-external-link-alt"></i>
</a>[3] J. Alammar, &ldquo;The Illustrated Transformer.&rdquo;
<a href=https://jalammar.github.io/illustrated-transformer/ title=https://jalammar.github.io/illustrated-transformer/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://jalammar.github.io/illustrated-transformer/
<i class="fa fa-external-link-alt"></i></a></p></div><footer class=post-footer><div class=post-tags><a href=/tags/llm/>LLM
</a><a href=/tags/cos597g2022/>COS597G:2022</a></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/learning/infra/%E4%BB%8E0%E5%8D%95%E6%8E%92ai_sys/ rel=next title=从0单排AI_Sys><i class="fa fa-chevron-left"></i> 从0单排AI_Sys</a></div><div class="post-nav-prev post-nav-item"><a href=/learning/overseas/%E6%B5%B7%E5%A4%96%E5%B7%A5%E4%BD%9C-%E8%BA%AB%E4%BB%BD%E7%AF%87/ rel=prev title="海外工作 身份篇">海外工作 身份篇
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div id=i18n-translate class=i18n-translate><i class="fa fa-language"></i><div id=lang-select class=lang-select><div id=lang-selected class=selected-option><span class="flag-icon flag-icon-en-us"></span>
<span class=selected-language>English</span>
<i class="fa fa-chevron-down"></i></div><div id=lang-options class=lang-options><div class=lang-option lang-code=en-us lang-name=English lang-url=/learning/llm/cos597g-22-introduction/><span class="flag-icon flag-icon-en-us"></span>
<span class=lang-name>English</span></div></div></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>NexT Theme</span></div></div></footer><script class=next-config data-name=page type=application/json>{"comments":false,"expired":false,"isHome":false,"isPage":true,"path":"cos597g-22-introduction","permalink":"http://localhost:1313/learning/llm/cos597g-22-introduction/","title":"COS597G 22 Introduction","toc":true,"waline":{"commentcnt":{"alias":"@waline/client","alias_name":"waline","file":"dist/comment.js","name":"comment","version":"2.15.8"}}}</script><script type=text/javascript src=http://localhost:1313/js/3rd/animejs/3.2.2/anime.min.js crossorigin=anonymous defer></script><script type=text/javascript src=http://localhost:1313/js/3rd/viewerjs/1.11.6/viewer.min.js crossorigin=anonymous defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":true,"hostname":"http://localhost:1313/","i18n":{"ds_day":" Day Ago","ds_days":" Day ","ds_hour":" Hour Ago","ds_hours":" Hour ","ds_just":"Just","ds_min":" Min Ago","ds_mins":" Min","ds_month":" Month Ago","ds_years":" Year ","empty":"We didn't find any results for the search: ${query}","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms","placeholder":"Searching..."},"isMultiLang":true,"lang":"en-US","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"slideInRight","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":true,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Mist","share":{"addtoany":{"js":"https://static.addtoany.com/menu/page.js","locale":"zh-CN","num":8},"enable":false},"sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"local","router":{"name":"local","type":"modern","url":"http://localhost:1313/js/3rd"}},"version":"4.8.3","waline":{"cfg":{"emoji":false,"imguploader":false,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"@waline/client","file":"dist/waline.css","name":"waline","version":"2.15.8"},"js":{"alias":"@waline/client","file":"dist/waline.js","name":"waline","version":"2.15.8"}}}</script><script type=text/javascript src="/js/main.js?=1765088039" defer></script></body></html>