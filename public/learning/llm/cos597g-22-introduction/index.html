<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>COS597G 22 Introduction | Ryan's Blog</title>
<meta name=keywords content="LLM,COS597G:2022"><meta name=description content="
Homepage

Human Language Understanding & Reasoning
Introductory reading authored by Christopher D. Manning.
Brief introduction of NLP history
The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms."><meta name=author content="Ryan Ming"><link rel=canonical href=http://localhost:1313/learning/llm/cos597g-22-introduction/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.4cf5c3a283f3912b9dbe1361188b2604ac9a8707aa9a89d3cec382f0561a5be5.css integrity="sha256-TPXDooPzkSudvhNhGIsmBKyahweqmonTzsOC8FYaW+U=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/meta/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/meta/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/meta/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/meta/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/meta/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/learning/llm/cos597g-22-introduction/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="COS597G 22 Introduction"><meta property="og:description" content="
Homepage

Human Language Understanding & Reasoning
Introductory reading authored by Christopher D. Manning.
Brief introduction of NLP history
The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/learning/llm/cos597g-22-introduction/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="learning"><meta property="article:published_time" content="2024-11-15T22:37:35+08:00"><meta property="article:modified_time" content="2024-11-15T22:37:35+08:00"><meta property="og:site_name" content="Ryan's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="COS597G 22 Introduction"><meta name=twitter:description content="
Homepage

Human Language Understanding & Reasoning
Introductory reading authored by Christopher D. Manning.
Brief introduction of NLP history
The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Learnings","item":"http://localhost:1313/learning/"},{"@type":"ListItem","position":2,"name":"COS597G 22 Introduction","item":"http://localhost:1313/learning/llm/cos597g-22-introduction/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"COS597G 22 Introduction","name":"COS597G 22 Introduction","description":" Homepage\nHuman Language Understanding \u0026amp; Reasoning Introductory reading authored by Christopher D. Manning.\nBrief introduction of NLP history The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms.\n","keywords":["LLM","COS597G:2022"],"articleBody":" Homepage\nHuman Language Understanding \u0026 Reasoning Introductory reading authored by Christopher D. Manning.\nBrief introduction of NLP history The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms.\nThe second era was from 1970 to 1992 and systems were able to deal with syntax and reference in human language. The new generation of hand-built systems had a clear separation between declarative linguistic knowledge and its procedural processing and which benefited from the development of a range of more modern linguistic theories.\nNLP dramatically changed in the third era, 1993 - 2012 because of the emergence of digital text. At the beginning, researchers tend to extract certain model from a large corpus of data by counting certain facts. Early attempts to learn language structure from text collections were fairly unsuccessful, which led most of the field to concentrate on constructing annotated linguistic resources. Supervised machine learning dominates NLP techniques.\nThe last era features with deep learning and growing artificial intelligence methods. Word \u0026 sentence embedding went viral. From 2013 to 2018, deep learning promotes the advantages of embedding thus leading NLP techiques to vector spaces. In 2018, very large scale self-supervised neural network succeeded in learning an enormous amount of knowledge by simly exposed to large contexts. Representative tasks are net word prediction and filling masked words or phrases.\nNow-dominant neural network Since 2018, the dominant neural network model for NLP applications has been the transformer neural network. The dominant idea is one of attention, by which a representation at a position is computed as a weighted combination of representations from other positions. Masked word prediction turns out to be very powerful because it is universal: every form of linguistic and world knowledge, from sentence structure, word connotations, and facts about the world, help one to do this task better. As a result, these models assemble a broad general knowledge of the language and world to which they are exposed.\nWhat can we do with LPLMs? Multilingual machine translation trained on all languages simutaneously; for other tasks like QA, sentiment classification, NER and fluent text generation, LPLMs turns out to be the best solution.\nProspects What’s the meaning in contexts? The dominant approach to describing meaning is a denotational semantics approach or a theory of reference: the meaning of a word, phrase, or sentence is the set of objects or situations in the world that it describes. This contrasts with the simple distributional semantics (or use theory of meaning) of modern empirical work in NLP, whereby the meaning of a word is simply a description of the contexts in which it appears. Manning claims that meaning arises from understanding the network of connections between a linguistic form and other things, whether they be objects in the world or other linguistic forms. Using this definition whereby understanding meaning consists of understanding networks of connections of linguistic forms, there can be no doubt that pretrained language models learn meanings. As well as word meanings, they learn much about the world.\nOne of the exciting prospects is learning from multi modal data, such as vision, robotics, knowledge graphs, bioinformatics, and multimodal data. Manning also mentions external database as the source of model while he still addresses the importance of multi-modal learning.\nWe will witness the comming of foundation models, with its specializations handling most information processing and analysis tasks. There might be concerns of risks that foundation models are controlled by several powerful and influencial groups and somehow it will\nbe difficult to tell if models are safe to use in particular contexts because the models and their training data are so large. Manning believes in the limitation of models while also gives postive comments on their utility and foresees the future that models are widly deployed.\nAttention is All You Need Transformer architecture is firstly introduced in this work.\nBefore Reading Attention is All You Need is well known for its contribution of Transformer architecture and therefore probably be seen as the inception of LLM era. 14k citations well demonstrate its significance. Authors are from Google Brain team or UofT. All of the authors shared the same contribution. Paper was accepted by NIPS 2017.\nIntroduction RNNs established SOTA approaches in sequence modeling while fell short of efficiency because of its non-parallizable computation. Previous attention mechanisms attempted to solving the problem yet only in few cases or by combining RNNs. The author proposed Transformer architecture to deal with parallelization by relying entirely on an attention mechanism.\nBackground Previous attempts on reducing sequential computation is to use convolutional neural networks. Though Convs proved efficient, the operations to relate signals from different positions grows either linearly or logarithmically. Transformer coinstrains the number of operations to constant and counteract resolution cost by applying Multi-head Attention. Self-attention performs well on a wide range of tasks and Transformer relies on self-attention without combining with RNNs.\nModel Architecture Key concepts: encoder-decoder structure, stacked self-attention, fully connected layers\nThe Transformer - model architecture.\nEncoder\nEncoder is composed of 6 identical stacked layers which contains 2 sub-layers. Residual connection are employed around each of the two sub-layers, followed by layer normalization. Sub-layers are multi-head self-attention layer and a simple, position-wise fully connected feed-forward network. Output dimension @d_{\\text{model}} = 512@\nDecoder\nDecoder is composed of 6 identical stacked layers. Self-attention sub-layer is modified by applying mask, which prevent the model from attending to subsquent positions.\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nScaled Dot-Product Attention\nTwo common attention functions: additive attention and dot-product attention. The difference is compatibility function. Additive attention use a feed-forward network with a single hidden layer while dot-product attention use Query and Key, which resembles to attention mechanism introduced in the paper. The two are similar in complexity but the latter could be optimized by matrix multiplicatoin, thus being more space-efficient in practice.\nScaled Dot-Product Attention:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V $$For larger values of @d_k@, result of @QK^T@ is going to be large, which falls into @\\text{softmax}@ regions where its has extremely small gradients. Therefore, scaling is added. Notice that the scaling factor is the dimension of Key: @1 / \\sqrt{d_k}@\nMulti-Head Attention Instead of performing a single attention function with @d_{\\text{model}}@-dimensional keys, values and queries, it is better to linearly project the queries, keys and values @h@ times with different, learned linear projections to @d_k@, @d_k@ and @d_v@ dimensions, respectively. Computation is carried out in parallel and output values are concatenated and once again projected.\n(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\nPerform linear output transformation after concatenation.\n$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_i)W^O $$where @\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^{K}, VW_i^{V})@. Dimensions (Sequence length @n@): @K \\in \\mathbb{R}^{n \\times d_{\\text{model}}}, W_{i}^K \\in \\mathbb{R}^{d_{\\text{model}}\\times d_k},@ @ W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}@. In the paper @h = 8, d_k = d_v = d_{\\text{model}} / h = 64@. @d_{\\text{model}}@ could be seen as dimension of embedding.\nApplications of Attention In deocoder, query comes from last layer while key and values comes from encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Mask is implemented inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\nPosition-wise FFN FFN consists of two linear transformation with ReLU activation n between.\n$$ \\text{FFN} = \\text{ReLU}(xW_1 +b_1)W_2 + b_2 $$Dimensions: inner dimension 2048, output dimension 512.\nEmbeddings and Softmax The implemetation share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by @\\sqrt{d_{\\text{model}}}@.\nPositional Embedding Sine and cosine functions:\n$$ \\begin{aligned} PE_{(pos, 2i)} \u0026= \\sin(pos / 10000^{2i / d_{\\text{model}}}) \\\\ PE_{(pos, 2i + 1)} \u0026= \\cos(pos / 10000^{2i / d_{\\text{model}}}) \\end{aligned} $$\nwhere @pos@ is the position and @i@ is the dimension. The author also tries positional embeddings, which yields similar results. But sin/cos PE could extrapolate to sequence longer inputs.\nWhy Self-Attention Total computational complexity per layer Type Complexity per Layer Self-Attention @O(n^2 \\cdot d)@ Self-Attention(restricted) @O(r \\cdot n \\cdot d)@ Recurrent @O(n \\cdot d^2)@ Conv @O(k \\cdot n \\cdot d^2)@ where @d@ is the representation dimension and @n@ is the length of sequence. Self-attention does not perform well when @n \u003e d@ compared with RNNs. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size @r@ in the input sequence centered around the respective output position.\nThe amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The path length between long-range dependencies in the network. More interpretable models by investigating attention values. Training Experiments on machine translation: WMT 2014 English-German/French. 8xP100 GPU, Adam optimizaer, regularization: residual dropout \u0026 label smoothing. Metrics: BLEU, Trainig cost(FLOPs)\nResults BLEU: EN-DE 28.4 EN-FR 41.8\nEnglish constituency parsing for checking ability of task generalization.\nAblation Number of attention heads Attention key size @d_k@ Model size Other positional embeddings Blog Post: The Illustrated Transformer Visualizing QK In encoder, @QK@ matrix mutiplication could be seen as quries multiplying keys, then get summation.\nVisualization of Encoder self-attention.\nEncoder-Decoder Attention Detailed version of encoder-decoder architecture:\nVisualization of encoder-decoder architecture\nWhat happened between encoder and decoder: encoder-decoder architecture gif:\nEncoder-decoder attention\nPosition Embedding Visualization Position Embedding Visualization\nx-axis: Embedding dimension; y-axis: Token position.\nAbout model outputs Because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Parameter temperature can affect output as well by adding a scaling factor: @\\text{softmax(x / T)}@.\nReferences [1] C. D. Manning, “Human Language Understanding \u0026 Reasoning,” journal-article, 2022. [Online]. Available: https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf\n[2] A. Vaswani et al., “Attention Is All You Need,” arXiv.org, Jun. 12, 2017. https://arxiv.org/abs/1706.03762\n[3] J. Alammar, “The Illustrated Transformer.” https://jalammar.github.io/illustrated-transformer/\n","wordCount":"1757","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-11-15T22:37:35+08:00","dateModified":"2024-11-15T22:37:35+08:00","author":{"@type":"Person","name":"Ryan Ming"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/learning/llm/cos597g-22-introduction/"},"publisher":{"@type":"Organization","name":"Ryan's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/meta/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["@","@"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/learning/ title=Learning><span>Learning</span></a></li><li><a href=http://localhost:1313/ideas/ title=Ideas><span>Ideas</span></a></li><li><a href=http://localhost:1313/ryanming-portfolio/ title=Academic><span>Academic</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/learning/>Learnings</a></div><h1 class="post-title entry-hint-parent">COS597G 22 Introduction</h1><div class=post-meta><span title='2024-11-15 22:37:35 +0800 CST'>November 15, 2024</span>&nbsp;·&nbsp;Ryan Ming</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#human-language-understanding--reasoning>Human Language Understanding & Reasoning</a><ul><li><a href=#brief-introduction-of-nlp-history>Brief introduction of NLP history</a></li><li><a href=#now-dominant-neural-network>Now-dominant neural network</a></li><li><a href=#what-can-we-do-with-lplms>What can we do with LPLMs?</a></li><li><a href=#prospects>Prospects</a></li></ul></li><li><a href=#attention-is-all-you-need>Attention is All You Need</a><ul><li><a href=#before-reading>Before Reading</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#background>Background</a></li><li><a href=#model-architecture>Model Architecture</a></li><li><a href=#training>Training</a></li></ul></li><li><a href=#blog-post-the-illustrated-transformer>Blog Post: The Illustrated Transformer</a><ul><li><a href=#visualizing-qk>Visualizing QK</a></li><li><a href=#encoder-decoder-attention>Encoder-Decoder Attention</a></li><li><a href=#position-embedding-visualization>Position Embedding Visualization</a></li><li><a href=#about-model-outputs>About model outputs</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p><a href=https://www.cs.princeton.edu/courses/archive/fall22/cos597G/>Homepage</a></p></blockquote><h2 id=human-language-understanding--reasoning>Human Language Understanding & Reasoning<a hidden class=anchor aria-hidden=true href=#human-language-understanding--reasoning>#</a></h2><p>Introductory reading authored by <a href=https://nlp.stanford.edu/~manning/>Christopher D. Manning</a>.</p><h3 id=brief-introduction-of-nlp-history>Brief introduction of NLP history<a hidden class=anchor aria-hidden=true href=#brief-introduction-of-nlp-history>#</a></h3><p>The NLP history is divided into four sections, running from the middle of last century to 2 years ago. NLP starts with machine translation in Cold War 1950 - 1969, when researchers on both sides sought to develop systems capable of translating the scientific output of the other nations. The NLP system provided little more the word-level lookups and some simple principle-based mechanisms.</p><p>The second era was from 1970 to 1992 and systems were able to deal with syntax and reference in human language. The new generation of hand-built systems had a clear separation between declarative linguistic knowledge and its procedural processing and which benefited from the development of a range of more modern linguistic theories.</p><p>NLP dramatically changed in the third era, 1993 - 2012 because of the emergence of digital text. At the beginning, researchers tend to extract certain model from a large corpus of data by counting certain facts. Early attempts to learn language structure from text collections were fairly unsuccessful, which led most of the field to concentrate on constructing annotated linguistic resources. Supervised machine learning dominates NLP techniques.</p><p>The last era features with deep learning and growing artificial intelligence methods. Word & sentence embedding went viral. From 2013 to 2018, deep learning promotes the advantages of embedding thus leading NLP techiques to vector spaces. In 2018, very large scale self-supervised neural network succeeded in learning an enormous amount of knowledge by simly exposed to large contexts. Representative tasks are net word prediction and filling masked words or phrases.</p><h3 id=now-dominant-neural-network>Now-dominant neural network<a hidden class=anchor aria-hidden=true href=#now-dominant-neural-network>#</a></h3><p>Since 2018, the dominant neural network model for NLP applications has been the transformer neural network. The dominant idea is one of attention, by which a representation at a position is computed as a weighted combination of representations from other positions. Masked word prediction turns out to be very powerful because it is universal: every form of linguistic and world knowledge, from sentence structure, word connotations, and facts about the world, help one to do this task better. As a result, these models assemble a broad general knowledge of the language and world to which they are exposed.</p><h3 id=what-can-we-do-with-lplms>What can we do with LPLMs?<a hidden class=anchor aria-hidden=true href=#what-can-we-do-with-lplms>#</a></h3><p>Multilingual machine translation trained on all languages simutaneously; for other tasks like QA, sentiment classification, NER and fluent text generation, LPLMs turns out to be the best solution.</p><h3 id=prospects>Prospects<a hidden class=anchor aria-hidden=true href=#prospects>#</a></h3><p>What&rsquo;s the meaning in contexts? The dominant approach to describing meaning is a denotational semantics approach or a theory of reference: the meaning of a word, phrase, or sentence is the set of objects or situations in the world that it describes. This contrasts with the simple distributional semantics (or use theory of meaning) of modern empirical work in NLP, whereby the meaning of a word is simply a description of the contexts in which it appears. Manning claims that meaning arises from understanding the network of connections between a linguistic form and other things, whether they be objects in the world or other linguistic forms. Using this definition whereby understanding meaning consists of understanding networks of connections of linguistic forms, there can be no doubt that pretrained language models learn meanings. As well as word meanings, they learn much about the world.</p><p>One of the exciting prospects is learning from multi modal data, such as vision, robotics, knowledge graphs, bioinformatics, and multimodal data. Manning also mentions external database as the source of model while he still addresses the importance of multi-modal learning.</p><p>We will witness the comming of foundation models, with its specializations handling most information processing and analysis tasks. There might be concerns of risks that foundation models are controlled by several powerful and influencial groups and somehow it will<br>be difficult to tell if models are safe to use in particular contexts because the models and their training data are so large. Manning believes in the limitation of models while also gives postive comments on their utility and foresees the future that models are widly deployed.</p><h2 id=attention-is-all-you-need>Attention is All You Need<a hidden class=anchor aria-hidden=true href=#attention-is-all-you-need>#</a></h2><p>Transformer architecture is firstly introduced in this work.</p><h3 id=before-reading>Before Reading<a hidden class=anchor aria-hidden=true href=#before-reading>#</a></h3><p><em>Attention is All You Need</em> is well known for its contribution of Transformer architecture and therefore probably be seen as the inception of LLM era. 14k citations well demonstrate its significance. Authors are from Google Brain team or UofT. All of the authors shared the same contribution. Paper was accepted by NIPS 2017.</p><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>RNNs established SOTA approaches in sequence modeling while fell short of efficiency because of its non-parallizable computation. Previous attention mechanisms attempted to solving the problem yet only in few cases or by combining RNNs. The author proposed Transformer architecture to deal with parallelization by relying entirely on an attention mechanism.</p><h3 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h3><p>Previous attempts on reducing sequential computation is to use convolutional neural networks. Though Convs proved efficient, the operations to relate signals from different positions grows either linearly or logarithmically. Transformer coinstrains the number of operations to constant and counteract resolution cost by applying Multi-head Attention. Self-attention performs well on a wide range of tasks and Transformer relies on self-attention without combining with RNNs.</p><h3 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h3><p>Key concepts: encoder-decoder structure, stacked self-attention, fully connected layers</p><figure class=align-center><img loading=lazy src=/COS597G-introduction/transformer_architecture.png#center alt="The Transformer - model architecture." width=350><figcaption><p>The Transformer - model architecture.</p></figcaption></figure><ol><li><p>Encoder<br>Encoder is composed of 6 identical stacked layers which contains 2 sub-layers. Residual connection are employed around each of the two sub-layers, followed by layer normalization. Sub-layers are multi-head self-attention layer and a simple, position-wise fully connected feed-forward network. Output dimension @d_{\text{model}} = 512@</p></li><li><p>Decoder<br>Decoder is composed of 6 identical stacked layers. Self-attention sub-layer is modified by applying mask, which prevent the model from attending to subsquent positions.</p></li><li><p>Attention<br>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a <em>compatibility function</em> of the query with the corresponding key.</p></li><li><p>Scaled Dot-Product Attention<br>Two common attention functions: additive attention and dot-product attention. The difference is <em>compatibility function</em>. Additive attention use a feed-forward network with a single hidden layer while dot-product attention use <em>Query</em> and <em>Key</em>, which resembles to attention mechanism introduced in the paper. The two are similar in complexity but the latter could be optimized by matrix multiplicatoin, thus being more space-efficient in practice.</p></li></ol><p>Scaled Dot-Product Attention:</p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
$$<p>For larger values of @d_k@, result of @QK^T@ is going to be large, which falls into @\text{softmax}@ regions where its has extremely small gradients. Therefore, scaling is added. Notice that the scaling factor is the dimension of <em>Key</em>: @1 / \sqrt{d_k}@</p><h4 id=multi-head-attention>Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h4><p>Instead of performing a single attention function with @d_{\text{model}}@-dimensional keys, values and queries, it is better to linearly project the queries, keys and values @h@ times with different, learned linear projections to @d_k@, @d_k@ and @d_v@ dimensions, respectively. Computation is carried out in parallel and output values are concatenated and once again projected.</p><figure class=align-center><img loading=lazy src=/COS597G-introduction/dot-product-attention.png#center alt="(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel." width=500><figcaption><p>(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.</p></figcaption></figure><p>Perform linear output transformation after concatenation.</p>$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_i)W^O
$$<p>where @\text{head}_i = \text{Attention}(QW_i^Q, KW_i^{K}, VW_i^{V})@. Dimensions (Sequence length @n@): @K \in \mathbb{R}^{n \times d_{\text{model}}}, W_{i}^K \in \mathbb{R}^{d_{\text{model}}\times d_k},@ @ W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}@. In the paper @h = 8, d_k = d_v = d_{\text{model}} / h = 64@. @d_{\text{model}}@ could be seen as dimension of embedding.</p><h4 id=applications-of-attention>Applications of Attention<a hidden class=anchor aria-hidden=true href=#applications-of-attention>#</a></h4><p>In deocoder, query comes from last layer while key and values comes from encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Mask is implemented inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.</p><h4 id=position-wise-ffn>Position-wise FFN<a hidden class=anchor aria-hidden=true href=#position-wise-ffn>#</a></h4><p>FFN consists of two linear transformation with ReLU activation n between.</p>$$
\text{FFN} = \text{ReLU}(xW_1 +b_1)W_2 + b_2
$$<p>Dimensions: inner dimension 2048, output dimension 512.</p><h4 id=embeddings-and-softmax>Embeddings and Softmax<a hidden class=anchor aria-hidden=true href=#embeddings-and-softmax>#</a></h4><p>The implemetation share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by @\sqrt{d_{\text{model}}}@.</p><h4 id=positional-embedding>Positional Embedding<a hidden class=anchor aria-hidden=true href=#positional-embedding>#</a></h4><p>Sine and cosine functions:</p>$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin(pos / 10000^{2i / d_{\text{model}}}) \\
PE_{(pos, 2i + 1)} &= \cos(pos / 10000^{2i / d_{\text{model}}})
\end{aligned}
$$<p><br>where @pos@ is the position and @i@ is the dimension. The author also tries positional embeddings, which yields similar results. But sin/cos PE could extrapolate to sequence longer inputs.</p><h4 id=why-self-attention>Why Self-Attention<a hidden class=anchor aria-hidden=true href=#why-self-attention>#</a></h4><ol><li>Total computational complexity per layer</li></ol><table><thead><tr><th style=text-align:left>Type</th><th style=text-align:center>Complexity per Layer</th></tr></thead><tbody><tr><td style=text-align:left>Self-Attention</td><td style=text-align:center>@O(n^2 \cdot d)@</td></tr><tr><td style=text-align:left>Self-Attention(restricted)</td><td style=text-align:center>@O(r \cdot n \cdot d)@</td></tr><tr><td style=text-align:left>Recurrent</td><td style=text-align:center>@O(n \cdot d^2)@</td></tr><tr><td style=text-align:left>Conv</td><td style=text-align:center>@O(k \cdot n \cdot d^2)@</td></tr></tbody></table><p>where @d@ is the representation dimension and @n@ is the length of sequence. Self-attention does not perform well when @n > d@ compared with RNNs. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size @r@ in the input sequence centered around the respective output position.</p><ol start=2><li>The amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.</li><li>The path length between long-range dependencies in the network.</li><li>More interpretable models by investigating attention values.</li></ol><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><p>Experiments on machine translation: WMT 2014 English-German/French. 8xP100 GPU, Adam optimizaer, regularization: residual dropout & label smoothing. Metrics: BLEU, Trainig cost(FLOPs)</p><h4 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h4><p>BLEU: EN-DE 28.4 EN-FR 41.8<br><a href=https://paperswithcode.com/task/constituency-parsing>English constituency parsing</a> for checking ability of task generalization.</p><h4 id=ablation>Ablation<a hidden class=anchor aria-hidden=true href=#ablation>#</a></h4><ol><li>Number of attention heads</li><li>Attention key size @d_k@</li><li>Model size</li><li>Other positional embeddings</li></ol><h2 id=blog-post-the-illustrated-transformer>Blog Post: The Illustrated Transformer<a hidden class=anchor aria-hidden=true href=#blog-post-the-illustrated-transformer>#</a></h2><h3 id=visualizing-qk>Visualizing QK<a hidden class=anchor aria-hidden=true href=#visualizing-qk>#</a></h3><p>In encoder, @QK@ matrix mutiplication could be seen as quries multiplying keys, then get summation.</p><figure class=align-center><img loading=lazy src=https://jalammar.github.io/images/t/self-attention-output.png#center alt="Visualization of Encoder self-attention." width=400><figcaption><p>Visualization of Encoder self-attention.</p></figcaption></figure><h3 id=encoder-decoder-attention>Encoder-Decoder Attention<a hidden class=anchor aria-hidden=true href=#encoder-decoder-attention>#</a></h3><p>Detailed version of encoder-decoder architecture:</p><figure class=align-center><img loading=lazy src=https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png#center alt="Visualization of encoder-decoder architecture" width=600><figcaption><p>Visualization of encoder-decoder architecture</p></figcaption></figure><p>What happened between encoder and decoder: encoder-decoder architecture gif:</p><figure class=align-center><img loading=lazy src=https://jalammar.github.io/images/t/transformer_decoding_1.gif#center alt="Encoder-decoder attention" width=600><figcaption><p>Encoder-decoder attention</p></figcaption></figure><h3 id=position-embedding-visualization>Position Embedding Visualization<a hidden class=anchor aria-hidden=true href=#position-embedding-visualization>#</a></h3><p><figure class=align-center><img loading=lazy src=https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png#center alt="Position Embedding Visualization" width=500><figcaption><p>Position Embedding Visualization</p></figcaption></figure><br>x-axis: Embedding dimension; y-axis: Token position.</p><h3 id=about-model-outputs>About model outputs<a hidden class=anchor aria-hidden=true href=#about-model-outputs>#</a></h3><p>Because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Parameter <code>temperature</code> can affect output as well by adding a scaling factor: @\text{softmax(x / T)}@.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] C. D. Manning, &ldquo;Human Language Understanding & Reasoning,&rdquo; journal-article, 2022. [Online]. Available: <a href=https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf>https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_09_Manning.pdf</a><br>[2] A. Vaswani et al., &ldquo;Attention Is All You Need,&rdquo; arXiv.org, Jun. 12, 2017. <a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a><br>[3] J. Alammar, &ldquo;The Illustrated Transformer.&rdquo; <a href=https://jalammar.github.io/illustrated-transformer/>https://jalammar.github.io/illustrated-transformer/</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llm/>LLM</a></li><li><a href=http://localhost:1313/tags/cos597g2022/>COS597G:2022</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Ryan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script defer src=https://use.fontawesome.com/releases/v5.15.4/js/all.js integrity=sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc crossorigin=anonymous></script><a class=busuanzi-counter><span id=busuanzi_container_value_site_pv><i class="far fa-eye fa-fw"></i>
<span id=busuanzi_value_site_pv></span>
</span>&nbsp;|&nbsp;
<span id=busuanzi_container_value_site_uv><i class="fa fa-user"></i>
<span id=busuanzi_value_site_uv></span></span></a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>