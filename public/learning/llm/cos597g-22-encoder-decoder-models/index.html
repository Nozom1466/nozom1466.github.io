<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>COS597G 22 Encoder Decoder Models | Ryan's Blog</title>
<meta name=keywords content><meta name=description content="Homepage
For BART, T5, mT5 and AlexaTM 20B
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series."><meta name=author content="Ryan Ming"><link rel=canonical href=http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.14155b8a746586b00d33630c5068133df953647db869fbe663e748f852eb81e4.css integrity="sha256-FBVbinRlhrANM2MMUGgTPflTZH24afvmY+dI+FLrgeQ=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/meta/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/meta/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/meta/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/meta/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/meta/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="COS597G 22 Encoder Decoder Models"><meta property="og:description" content="Homepage
For BART, T5, mT5 and AlexaTM 20B
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="learning"><meta property="article:published_time" content="2025-04-22T23:45:33+08:00"><meta property="article:modified_time" content="2025-04-22T23:45:33+08:00"><meta property="og:site_name" content="Ryan's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="COS597G 22 Encoder Decoder Models"><meta name=twitter:description content="Homepage
For BART, T5, mT5 and AlexaTM 20B
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Learnings","item":"http://localhost:1313/learning/"},{"@type":"ListItem","position":2,"name":"COS597G 22 Encoder Decoder Models","item":"http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"COS597G 22 Encoder Decoder Models","name":"COS597G 22 Encoder Decoder Models","description":"Homepage\nFor BART, T5, mT5 and AlexaTM 20B\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series.\n","keywords":[],"articleBody":"Homepage\nFor BART, T5, mT5 and AlexaTM 20B\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series.\nFigure 1. LLMs evolutionary tree.\nContributions Combination of bidirectional encider and autoregressive decoder: BERT(bi-directional) + GPT(uni-direct auto-regressive) Pretraining with better noising approaches: shuffling + in-filling scheme Approach Architecture: BART use seq2seq Transformer architecture. Mofidications: ReLUs are modified as GeLUs, with initialization from @\\mathcal{N}(0, 0.02)@. 6 layers in encoder and 12 layers in decoder (following BERT) Remove Feed-Forward network before word prediction Pretraining: Trained by corrupting documents and then optimizing a reconstriction loss. Corruptions are introduced in Figure 2. Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed.\nFinetuning: Representations produced by BART are used in Seq/token classification, seq generationa and MT tasks. In MT, the author replace BART’s encoder embedding layer with a new randomly initialized encoder, as illustarted in Figure 3. Figure 3. Fine tuning BART for classification and translation.\nPretraining Objective Comparison Models: LM(GPT), Permuted LM(XLNet), Masked LM(BERT), Multitask Masked LM(UniLM), Masked seq2seq(MASS) Tasks: SQuAD(context + question -\u003e context span) MNLI(con + q -\u003e relation) ELI5, XSum, CNN/DM(con + q -\u003e abstraction) ConvAI2(dialogue gen) Insights: Performance of pre-training methods varies significantly across tasks Token masking is crucial Left-to-right pre-training improves generation Bidirectional encoders are crucial for SQuAD The pre-training objective is not the only important factor Pure language models perform best on ELI5 BART achieves the most consistently strong performance. Large-scale Pre-training Experiments Pretraining with large batchsize (8000 a batch. 500k steps following RoBERTa):\nDiscriminative Tasks: BART’s improvements on generation tasks do not come at the expense of classification performance. Generation Tasks: Summarization, Dialogue and Abstrctive QA. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer This work explores the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. And the model proposed in this paper, known as T5, is trained on unified text2text framework, where all of outputs are regarded as texts. (“3.8”, “5”)\nMotivation Previously the training in ML is amenable to downstream learning tasks and the knowledge required for the learning is learned as part of auxiliary task.(word vectors). Later the scheme shifted to pretraining on data-rich tasks using unlabeled data (like Common Crawl project). The work focus on the understanding of burgeoning transfer learning techniques.\nBasic idea: treat every text processing problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output.\nArchitecture of T5 Model To summarize, T5 is roughly equivalent to the original Transformer with the exception of\nremoving the Layer Norm bias：activations are only rescaled and no additive bias is applied. placing the layer normalization outside the residual path using a different position embedding scheme.: We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model. Check out Transformers without Normalization from meta…. Layer Norm is replaced by @tanh(\\cdot)@，\nDataset - The Colossal Clean Crawled Corpus Adopted from Common Crawl dataset, using cleaning up techniques:\nend in . page \u003e 3 sentences; lines \u003e= 5 words remove bad words lorem ipsum placeholder removed curly bracket removed citation markers removed policy \u0026 cookies removed leave only one in 3-span sentence sliding window pages not written in English Downstream Tasks Indeed an in-depth tech report …\n750 GB dataset\nmachine translation: WMT question answering: SQuAD abstractive summarization: CNN/Daily Mail text classification: GLUE, SuperGLUE Baselines Goal: Pre-train a standard Transformer using a simple denoising objective and then separately fine-tune on each of our downstream tasks. Model: encoder and decoder are each similar in size and configuration to a BERT_BASE. Each has 12 blocks, FF layer @d_{ff}=3072@, @d_{kv} = 64@, multi-attention with 12 heads. Dropout prob 0.1. Training: As all tasks are regarded as t2t, loss f: teacher forcing and cross-entropy loss. AdaFactor as optimizor. Learning rate schedule: inverse square root (@1 / \\sqrt{\\max{(n, k)}}@), with n as iteration index and k as number of warm-up steps. Vocabulary: SentencePiece to encode text as WordPiece tokens. Unsupervised Objective: (Denoising) An objective that randomly samples and then drops out 15% of tokens in the input sequence, as shown in Figure 4. Figure 4.Schematic of the objective we use in our baseline model. In this example, we process the sentence “Thank you for inviting me to your party last week.” The words “for”, “inviting” and “last” (marked with an ×) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as and ) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel . The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token .\nThe following sections are discussing model performance from Architectures, Unsupervised Objectives, Pre-training Datasets, Training strategy and Scaling.\nArchitectures Another classification other than encoder/decoder: look into attention mask adopted by the model. There are 3 types of mask patterns: Fulli-visible, Casual and Casual with prefix, as shown in Figure 5.\nFigure 5. Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from “the future”. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\nStandard Encoder-Decoder architecture: All-visible mask(Encoder) + Casual mask(Decoder) Language Model: Casual mask, Language models are typically used for compression or sequence generation Prefix LM: Casual with Prefix mask, could be considered as encoder+decoder combined. Architectures as shown in Figure 6.\nPrefix LMs resembles BERT for classification tasks when you feeding the model with tasks like: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target: (Of course! Didn’t see why the author are adding this paragraph …). Attention masking seems to be an interesting topic, which involves many other memory-efficient or computing-efficient methods (paged attention for vllm?). Figure 6. Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use “.” to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fullyvisible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input.\nHere the authors mentioned criterions in selecting models, condidering these models are in different architectures and parameters. We suppose two models are equivalent if they have the same parameter @P@ or the same computational cost @C@. Consider an encoder-decoder model with @L + L@ layers, @P + P@ parameters and a language model(decoder) with @2L@ layers and @2P@ parameters. The parameters are approximately the same for these models but the computation cost of language model is approx. twice of that in encoder-decoder model. Because the latter has to deal with both input squence and output sequence but the former deal with inputs and outputs separately. (has lots to do with sequence length …). Theresfore, they select:\ne-d, L + L -\u003e 2P, M flops\ne-d, shared params -\u003e P, M flops\ne-d, L/2 + L/2 -\u003e P, M/2 flops\nd, L -\u003e P, M flops\nd, prefix -\u003e P, M flops\nwhere e-d for encoder and decoder and L for layers, P for parameters, M for computational cost.\nResults (for different architecture): Denoising task (metioned in previous section) and language modeling task (predicting the whole sentence for language model and predicting the second half of the sentence given the first half). Sharing the params across e-d performed very well and halfing params hurts the performace.\nUnsupervised Objectives Examples of common unsupervised objectives (Figure 7). Models are fisrt pretrained based on these unsupervised objectives then evaluated on downstream tasks (GLUE, CNNDM, SQuAD, SGLUE, EnDe, EnFr and EnRo). This section extends in: 3 common unsupervised objectives -\u003e variants of BERT objective (MLM) -\u003e exploration of corruption rate -\u003e exploration of corrupting spans.\nFigure 7. Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text “Thank you for inviting me to your party last week .” Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. denotes a shared mask token and , , and denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple.\nHigh Level Approaches: Tha author evaluated 3 types of objectives: Prefix language modeling, BERT-syle(MLM) and deshuffuling (as illustarted in Figure 7). BERT objective stands out (signifigantly over Deshuffling).\nVariants of BERT Objective: Purpose: better performance and better efficiency.\nVARIANT 1: MASS-style, reconstruct the original uncorrupted sequence(4th in Figure 7); VARIANT 2: Unique mask token, predict token prefixed by special token(5th in Figure 7).; VARIANT 3: Drop Corrupted Tokens, concatenate predicted tokens(6th in Figure 7). All these variants performs similarly. Notice that performace of dropping corrupted tokens fluctuates on several metrics. Dropping is still attractive because it reduces the input length thus making training faster. Corruption Rate: Limited effect on performance. Larger corruption rate -\u003e more inference time.\nCorrupting Spans: BERT mask follows i.i.d., masking tokens independently. While in some cases we need consecutive corruption. Number of corruption span and span lengths are determined by parameters. Again, this trick has limited effect on downstream task performance. While span corruption slightly speeds up training because it produces shorter sequence on average.\nConclusions:\nDenoising objectives(BERT MLM) outperforms language modeling and deshuffling. Choosing among the denoising objectives we considered here should mainly be done according to their computational cost(since similar approaches yields slight improvements). It may be fortuitous to explore entirely different ways of leveraging unlabeled data. For conclusion 2, the paper seems only to explore BERT variants. What about the other two?\nPretraining Data set The effects of pretraininig dataset. There are these fun facts:\nC4 dataset proposed by this paper invloves a heuristic filtering strategy, which proved to be helpful in pretraining. Pretraining on in-domain unlabeled data can improve performance on downstream tasks. (not superising? like SFT?) But it’s not good if we want our model to adapt to language tasks from arbitraray domains. BUT the dataset gathered for specific domains are much smaller. Pretraining dataset size is also a key factor. The performance of pretrained model degrades as the size of dataset getting smaller. (model trys to memorize the dataset rather than learning.) Use large dataset as possible. Repeated dataset will degrade the performance while its ok if the repeated time are smaller than 64 (som metrics are even better) ","wordCount":"2057","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-04-22T23:45:33+08:00","dateModified":"2025-04-22T23:45:33+08:00","author":{"@type":"Person","name":"Ryan Ming"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/"},"publisher":{"@type":"Organization","name":"Ryan's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/meta/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["@","@"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/learning/ title=Learning><span>Learning</span></a></li><li><a href=http://localhost:1313/ideas/ title=Ideas><span>Ideas</span></a></li><li><a href=http://localhost:1313/travel/ title=Travel><span>Travel</span></a></li><li><a href=http://localhost:1313/ryanming-portfolio/ title=Academic><span>Academic</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/learning/>Learnings</a></div><h1 class="post-title entry-hint-parent">COS597G 22 Encoder Decoder Models</h1><div class=post-meta><span title='2025-04-22 23:45:33 +0800 CST'>April 22, 2025</span>&nbsp;·&nbsp;Ryan Ming</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a><ul><li><a href=#contributions>Contributions</a></li><li><a href=#approach>Approach</a></li><li><a href=#pretraining-objective-comparison>Pretraining Objective Comparison</a></li><li><a href=#large-scale-pre-training-experiments>Large-scale Pre-training Experiments</a></li></ul></li><li><a href=#exploring-the-limits-of-transfer-learning-with-a-unified--text-to-text-transformer>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a><ul><li><a href=#motivation>Motivation</a></li><li><a href=#architecture-of-t5-model>Architecture of T5 Model</a></li><li><a href=#dataset---the-colossal-clean-crawled-corpus>Dataset - The Colossal Clean Crawled Corpus</a></li><li><a href=#downstream-tasks>Downstream Tasks</a></li><li><a href=#baselines>Baselines</a></li><li><a href=#architectures>Architectures</a></li><li><a href=#unsupervised-objectives>Unsupervised Objectives</a></li><li><a href=#pretraining-data-set>Pretraining Data set</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p><a href=https://www.cs.princeton.edu/courses/archive/fall22/cos597G/>Homepage</a></p><p>For BART, T5, mT5 and AlexaTM 20B</p><h2 id=bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension<a hidden class=anchor aria-hidden=true href=#bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension>#</a></h2><p>Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series.</p><figure class=align-center><img loading=lazy src="https://media.licdn.com/dms/image/v2/D4D22AQFNeG_WIY7WOQ/feedshare-shrink_1280/feedshare-shrink_1280/0/1683649150911?e=1746057600&amp;v=beta&amp;t=J7YdPUI60vDiuhDTK7UtMVQrfHrLeVddhkINUVwM3gs#center" alt="Figure 1. LLMs evolutionary tree." width=800><figcaption><p>Figure 1. LLMs evolutionary tree.</p></figcaption></figure><h3 id=contributions>Contributions<a hidden class=anchor aria-hidden=true href=#contributions>#</a></h3><ol><li><strong>Combination of bidirectional encider and autoregressive decoder</strong>: BERT(bi-directional) + GPT(uni-direct auto-regressive)</li><li><strong>Pretraining with better noising approaches</strong>: shuffling + in-filling scheme</li></ol><h3 id=approach>Approach<a hidden class=anchor aria-hidden=true href=#approach>#</a></h3><ul><li><strong>Architecture</strong>: BART use seq2seq Transformer architecture. Mofidications:</li></ul><ol><li>ReLUs are modified as GeLUs, with initialization from @\mathcal{N}(0, 0.02)@.</li><li>6 layers in encoder and 12 layers in decoder (following BERT)</li><li>Remove Feed-Forward network before word prediction</li></ol><ul><li><strong>Pretraining</strong>: Trained by corrupting documents and then optimizing a reconstriction loss. Corruptions are introduced in Figure 2.</li></ul><figure class=align-center><img loading=lazy src=https://myblog-1316371247.cos.ap-shanghai.myqcloud.com/myblog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-04-23%20013045.png#center alt="Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed." width=800><figcaption><p>Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed.</p></figcaption></figure><ul><li><strong>Finetuning</strong>: Representations produced by BART are used in Seq/token classification, seq generationa and MT tasks. In MT, the author replace BART&rsquo;s encoder embedding layer with a new randomly initialized encoder, as illustarted in Figure 3.</li></ul><figure class=align-center><img loading=lazy src=https://myblog-1316371247.cos.ap-shanghai.myqcloud.com/myblog/20250423013642552.png#center alt="Figure 3. Fine tuning BART for classification and translation." width=1000><figcaption><p>Figure 3. Fine tuning BART for classification and translation.</p></figcaption></figure><h3 id=pretraining-objective-comparison>Pretraining Objective Comparison<a hidden class=anchor aria-hidden=true href=#pretraining-objective-comparison>#</a></h3><ul><li><strong>Models</strong>: LM(GPT), Permuted LM(XLNet), Masked LM(BERT), Multitask Masked LM(UniLM), Masked seq2seq(MASS)</li><li><strong>Tasks</strong>:</li></ul><ol><li>SQuAD(context + question -> context span)</li><li>MNLI(con + q -> relation)</li><li>ELI5, XSum, CNN/DM(con + q -> abstraction)</li><li>ConvAI2(dialogue gen)</li></ol><ul><li><strong>Insights</strong>:</li></ul><ol><li>Performance of pre-training methods varies significantly across tasks</li><li>Token masking is crucial</li><li>Left-to-right pre-training improves generation</li><li>Bidirectional encoders are crucial for SQuAD</li><li>The pre-training objective is not the only important factor</li><li>Pure language models perform best on ELI5</li><li>BART achieves the most consistently strong performance.</li></ol><h3 id=large-scale-pre-training-experiments>Large-scale Pre-training Experiments<a hidden class=anchor aria-hidden=true href=#large-scale-pre-training-experiments>#</a></h3><p>Pretraining with large batchsize (8000 a batch. 500k steps following RoBERTa):</p><ul><li><strong>Discriminative Tasks</strong>: BART’s improvements on generation tasks do not come at the expense of classification performance.</li><li><strong>Generation Tasks</strong>: Summarization, Dialogue and Abstrctive QA.</li></ul><h2 id=exploring-the-limits-of-transfer-learning-with-a-unified--text-to-text-transformer>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer<a hidden class=anchor aria-hidden=true href=#exploring-the-limits-of-transfer-learning-with-a-unified--text-to-text-transformer>#</a></h2><p>This work explores the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. And the model proposed in this paper, known as T5, is trained on unified text2text framework, where all of outputs are regarded as texts. (&ldquo;3.8&rdquo;, &ldquo;5&rdquo;)</p><h3 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h3><p>Previously the training in ML is amenable to downstream learning tasks and the knowledge required for the learning is learned as part of auxiliary task.(word vectors). Later the scheme shifted to pretraining on data-rich tasks using unlabeled data (like Common Crawl project). The work focus on the understanding of burgeoning transfer learning techniques.</p><p>Basic idea: treat every text processing problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output.</p><h3 id=architecture-of-t5-model>Architecture of T5 Model<a hidden class=anchor aria-hidden=true href=#architecture-of-t5-model>#</a></h3><p>To summarize, T5 is roughly equivalent to the original Transformer with the exception of</p><ol><li><strong>removing the Layer Norm bias</strong>：activations are only rescaled and no additive bias is applied.</li><li><strong>placing the layer normalization outside the residual path</strong></li><li><strong>using a different position embedding scheme.</strong>: We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model.</li></ol><blockquote><p>Check out <a href=https://arxiv.org/abs/2503.10622>Transformers without Normalization</a> from meta&mldr;. Layer Norm is replaced by @tanh(\cdot)@，</p></blockquote><h3 id=dataset---the-colossal-clean-crawled-corpus>Dataset - The Colossal Clean Crawled Corpus<a hidden class=anchor aria-hidden=true href=#dataset---the-colossal-clean-crawled-corpus>#</a></h3><p>Adopted from Common Crawl dataset, using cleaning up techniques:</p><ul><li>end in .</li><li>page > 3 sentences; lines >= 5 words</li><li>remove bad words</li><li>lorem ipsum placeholder removed</li><li>curly bracket removed</li><li>citation markers removed</li><li>policy & cookies removed</li><li>leave only one in 3-span sentence sliding window</li><li>pages not written in English</li></ul><h3 id=downstream-tasks>Downstream Tasks<a hidden class=anchor aria-hidden=true href=#downstream-tasks>#</a></h3><blockquote><p>Indeed an in-depth tech report &mldr;</p></blockquote><p>750 GB dataset</p><ul><li>machine translation: WMT</li><li>question answering: SQuAD</li><li>abstractive summarization: CNN/Daily Mail</li><li>text classification: GLUE, SuperGLUE</li></ul><h3 id=baselines>Baselines<a hidden class=anchor aria-hidden=true href=#baselines>#</a></h3><ul><li><strong>Goal</strong>: Pre-train a standard Transformer using a simple denoising objective and then separately fine-tune on each of our downstream tasks.</li><li><strong>Model</strong>: encoder and decoder are each similar in size and configuration to a BERT_BASE. Each has 12 blocks, FF layer @d_{ff}=3072@, @d_{kv} = 64@, multi-attention with 12 heads. Dropout prob 0.1.</li><li><strong>Training</strong>: As all tasks are regarded as t2t, loss f: <a href=https://blog.csdn.net/qq_30219017/article/details/89090690>teacher forcing</a> and cross-entropy loss. AdaFactor as optimizor. Learning rate schedule: inverse square root (@1 / \sqrt{\max{(n, k)}}@), with n as iteration index and k as number of warm-up steps.</li><li><strong>Vocabulary</strong>: SentencePiece to encode text as WordPiece tokens.</li><li><strong>Unsupervised Objective</strong>: (Denoising) An objective that randomly samples and then drops out 15% of tokens in the input sequence, as shown in Figure 4.</li></ul><figure class=align-center><img loading=lazy src=https://miro.medium.com/v2/resize:fit:988/format:webp/1*9yFICqDlfprn-I_VZ5RHgw.png#center alt="Figure 4.Schematic of the objective we use in our baseline model. In this example, we process the sentence “Thank you for inviting me to your party last week.” The words “for”, “inviting” and “last” (marked with an ×) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as and ) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel . The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token ." width=800><figcaption><p>Figure 4.Schematic of the objective we use in our baseline model. In this example, we process the sentence “Thank you for inviting me to your party last week.” The words “for”, “inviting” and “last” (marked with an ×) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as and ) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel . The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token .</p></figcaption></figure><blockquote><p>The following sections are discussing model performance from Architectures, Unsupervised Objectives, Pre-training Datasets, Training strategy and Scaling.</p></blockquote><h3 id=architectures>Architectures<a hidden class=anchor aria-hidden=true href=#architectures>#</a></h3><p>Another classification other than encoder/decoder: look into attention mask adopted by the model. There are 3 types of mask patterns: Fulli-visible, Casual and Casual with prefix, as shown in Figure 5.</p><figure class=align-center><img loading=lazy src=https://blog.codescv.com/images/t5-masks.png#center alt="Figure 5. Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from “the future”. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence." width=800><figcaption><p>Figure 5. Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from “the future”. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.</p></figcaption></figure><ul><li><strong>Standard Encoder-Decoder architecture</strong>: All-visible mask(Encoder) + Casual mask(Decoder)</li><li><strong>Language Model</strong>: Casual mask, Language models are typically used for compression or sequence generation</li><li><strong>Prefix LM</strong>: Casual with Prefix mask, could be considered as encoder+decoder combined. Architectures as shown in Figure 6.<br>Prefix LMs resembles BERT for classification tasks when you feeding the model with tasks like: <code>I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target:</code> (Of course! Didn&rsquo;t see why the author are adding this paragraph &mldr;). Attention masking seems to be an interesting topic, which involves many other memory-efficient or computing-efficient methods (paged attention for vllm?).</li></ul><figure class=align-center><img loading=lazy src=https://dkharazi.github.io/88459ae93dd4af11a69cab297fec5dbd/t5training.png#center alt="Figure 6. Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use “.” to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fullyvisible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input." width=800><figcaption><p>Figure 6. Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use “.” to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fullyvisible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input.</p></figcaption></figure><p>Here the authors mentioned criterions in selecting models, condidering these models are in different architectures and parameters. We suppose two models are equivalent if they have the same parameter @P@ or the same computational cost @C@. Consider an <em>encoder-decoder model</em> with @L + L@ layers, @P + P@ parameters and a <em>language model</em>(decoder) with @2L@ layers and @2P@ parameters. The parameters are approximately the same for these models but the <strong>computation cost</strong> of <em>language model</em> is approx. twice of that in <em>encoder-decoder</em> model. Because the latter has to deal with both input squence and output sequence but the former deal with inputs and outputs separately. (has lots to do with sequence length &mldr;). Theresfore, they select:</p><ul><li><p>e-d, L + L -> 2P, M flops</p></li><li><p>e-d, shared params -> P, M flops</p></li><li><p>e-d, L/2 + L/2 -> P, M/2 flops</p></li><li><p>d, L -> P, M flops</p></li><li><p>d, prefix -> P, M flops<br>where e-d for encoder and decoder and L for layers, P for parameters, M for computational cost.</p></li><li><p><strong>Results (for different architecture)</strong>: Denoising task (metioned in previous section) and language modeling task (predicting the whole sentence for language model and predicting the second half of the sentence given the first half). Sharing the params across e-d performed very well and halfing params hurts the performace.</p></li></ul><h3 id=unsupervised-objectives>Unsupervised Objectives<a hidden class=anchor aria-hidden=true href=#unsupervised-objectives>#</a></h3><p>Examples of common unsupervised objectives (Figure 7). Models are fisrt pretrained based on these unsupervised objectives then evaluated on downstream tasks (GLUE, CNNDM, SQuAD, SGLUE, EnDe, EnFr and EnRo). This section extends in: 3 common unsupervised objectives -> variants of BERT objective (MLM) -> exploration of corruption rate -> exploration of corrupting spans.</p><figure class=align-center><img loading=lazy src=https://myblog-1316371247.cos.ap-shanghai.myqcloud.com/myblog/20250423235343386.png#center alt="Figure 7. Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text “Thank you for inviting me to your party last week .” Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. denotes a shared mask token and , , and denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple." width=1000><figcaption><p>Figure 7. Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text “Thank you for inviting me to your party last week .” Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. denotes a shared mask token and , , and denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple.</p></figcaption></figure><ul><li><p><strong>High Level Approaches</strong>: Tha author evaluated 3 types of objectives: Prefix language modeling, BERT-syle(MLM) and deshuffuling (as illustarted in Figure 7). BERT objective stands out (signifigantly over Deshuffling).</p></li><li><p><strong>Variants of BERT Objective</strong>: Purpose: better performance and better efficiency.</p><ul><li><em>VARIANT 1: MASS-style</em>, reconstruct the original uncorrupted sequence(4th in Figure 7);</li><li><em>VARIANT 2: Unique mask token</em>, predict token prefixed by special token(5th in Figure 7).;</li><li><em>VARIANT 3: Drop Corrupted Tokens</em>, concatenate predicted tokens(6th in Figure 7). All these variants performs similarly.</li><li>Notice that performace of dropping corrupted tokens fluctuates on several metrics. Dropping is still attractive because it <em>reduces the input length thus making training faster</em>.</li></ul></li><li><p><strong>Corruption Rate</strong>: Limited effect on performance. Larger corruption rate -> more inference time.</p></li><li><p><strong>Corrupting Spans</strong>: BERT mask follows i.i.d., masking tokens independently. While in some cases we need consecutive corruption. Number of corruption span and span lengths are determined by parameters. Again, this trick has limited effect on downstream task performance. While span corruption slightly speeds up training because it produces shorter sequence on average.</p></li><li><p><strong>Conclusions</strong>:</p></li></ul><ol><li>Denoising objectives(BERT MLM) outperforms language modeling and deshuffling.</li><li>Choosing among the denoising objectives we considered here should mainly be done <strong>according to their computational cost</strong>(since similar approaches yields slight improvements). It may be fortuitous to explore entirely different ways of leveraging unlabeled data.</li></ol><blockquote><p>For conclusion 2, the paper seems only to explore BERT variants. What about the other two?</p></blockquote><h3 id=pretraining-data-set>Pretraining Data set<a hidden class=anchor aria-hidden=true href=#pretraining-data-set>#</a></h3><p>The effects of pretraininig dataset. There are these fun facts:</p><ol><li><code>C4</code> dataset proposed by this paper invloves a heuristic filtering strategy, which proved to be helpful in pretraining.</li><li>Pretraining on in-domain unlabeled data can improve performance on downstream tasks. (not superising? like SFT?) But it&rsquo;s not good if we want our model to adapt to language tasks from arbitraray domains. BUT the dataset gathered for specific domains are much smaller.</li><li>Pretraining dataset size is also a key factor. The performance of pretrained model degrades as the size of dataset getting smaller. (model trys to memorize the dataset rather than learning.)</li><li>Use large dataset as possible. Repeated dataset will degrade the performance while its ok if the repeated time are smaller than 64 (som metrics are even better)</li></ol><blockquote></blockquote></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Ryan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script defer src=https://use.fontawesome.com/releases/v5.15.4/js/all.js integrity=sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc crossorigin=anonymous></script><a class=busuanzi-counter><span id=busuanzi_container_value_site_pv><i class="far fa-eye fa-fw"></i>
<span id=busuanzi_value_site_pv></span>
</span>&nbsp;|&nbsp;
<span id=busuanzi_container_value_site_uv><i class="fa fa-user"></i>
<span id=busuanzi_value_site_uv></span></span></a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>