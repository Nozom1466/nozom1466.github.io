<!doctype html><html lang=en-US data-theme=dark><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.152.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="COS597G 22 Encoder Decoder Models"><meta itemprop=description content="Keep it simple, keep it powerful."><meta name=description content="Keep it simple, keep it powerful."><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="http://localhost:1313/profileMode/avatar.jpg"><meta itemprop=keywords content><meta property="og:type" content="article"><meta property="og:title" content="COS597G 22 Encoder Decoder Models"><meta property="og:description" content="Keep it simple, keep it powerful."><meta property="og:image" content="/profileMode/avatar.jpg"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/"><meta property="og:site_name" content="Ryan's Blog"><meta property="og:locale" content="en-US"><meta property="article:author" content="NexT Theme"><meta property="article:published_time" content="2025-04-22 23:45:33 +0800 +0800"><meta property="article:modified_time" content="2025-04-22 23:45:33 +0800 +0800"><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/font-awesome/6.7.2/css/all.min.css><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/viewerjs/1.11.6/viewer.min.css><link rel=stylesheet href="/css/main.css?=1765088257"><style type=text/css>.post-footer hr:after{content:"~ End Line ~"}.flinks-list-footer hr:after{content:"~ End Line ~"}</style><link rel=stylesheet type=text/css href="/css/custom_style.css?=1765088257"><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return 0[0];const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),0[0]):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>COS597G 22 Encoder Decoder Models - Ryan's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Ryan's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Build for Hugo Theme</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>Home</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>Archives
<span class=badge>7</span></a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-tags hvr-icon"></i>Tags</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>Search</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=Searching... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>TOC</li><li class=sidebar-nav-overview>Overview</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a><ul><li><a href=#contributions>Contributions</a></li><li><a href=#approach>Approach</a></li><li><a href=#pretraining-objective-comparison>Pretraining Objective Comparison</a></li><li><a href=#large-scale-pre-training-experiments>Large-scale Pre-training Experiments</a></li></ul></li><li><a href=#exploring-the-limits-of-transfer-learning-with-a-unified--text-to-text-transformer>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a><ul><li><a href=#motivation>Motivation</a></li><li><a href=#architecture-of-t5-model>Architecture of T5 Model</a></li><li><a href=#dataset---the-colossal-clean-crawled-corpus>Dataset - The Colossal Clean Crawled Corpus</a></li><li><a href=#downstream-tasks>Downstream Tasks</a></li><li><a href=#baselines>Baselines</a></li><li><a href=#architectures>Architectures</a></li><li><a href=#unsupervised-objectives>Unsupervised Objectives</a></li><li><a href=#pretraining-data-set>Pretraining Data set</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt="NexT Theme" src=/imgs/img-lazy-loading.gif data-src=/profileMode/avatar.jpg><p class=site-author-name itemprop=name>NexT Theme</p><div class=site-description itemprop=description>Keep it simple, keep it powerful.</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>7</span>
<span class=site-state-item-name>Posts</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>Categories</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>8</span>
<span class=site-state-item-name>Tags</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/Nozom1466 title="Github → https://github.com/Nozom1466" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>
Github
</a></span><span class=links-of-social-item><a href=https://x.com/NozomiKasa1466 title="Twitter → https://x.com/NozomiKasa1466" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-twitter fa-fw hvr-icon"></i>
Twitter</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en class=cc-opacity rel=noopener target=_blank title="Creative Commons"><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt="Creative Commons"></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
Links</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>Web Status</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>Running:</div><div class=item-count id=runTimes data-publishdate="2024-09-28 19:59:00 +0800 +0800"></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>Visitors:</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>Views:</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>Words:</div><div class=item-count id=wordsCount data-count=17593></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>ReadTime:</div><div class=item-count id=readTimes data-times=43></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>Last Update:</div><div class=item-count id=last-push-date data-lastpushdate="2025-05-27 22:59:47 +0800 +0800"></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=goto-i18n-translate class=button title="Multilingual translation"><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title="Change Theme"><i class="fas fa-adjust"></i></div></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/profileMode/avatar.jpg"><meta itemprop=name content="NexT Theme"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="NexT Theme"><meta itemprop=description content="Keep it simple, keep it powerful."></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="COS597G 22 Encoder Decoder Models"><meta itemprop=description content="Encoder Decoder Models"></span><header class=post-header><h1 class=post-title itemprop="name headline">COS597G 22 Encoder Decoder Models</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="fas fa-solid fa-calendar"></i>
</span><span class=post-meta-item-text title="Publish on">Publish on:
</span><time title="Create Time:2025/04/22 23:45:33 +08:00" itemprop="dateCreated datePublished" datetime="2025-04-22 23:45:33 +0800 +0800">2025/04/22</time></span></div><div class=post-meta-items></div></div></header><div class="post-body autonumber" itemprop=articleBody>Encoder Decoder Models
<a id=more></a><p><a href=https://www.cs.princeton.edu/courses/archive/fall22/cos597G/ title=Homepage rel="noopener external nofollow noreferrer" target=_blank class=exturl>Homepage
<i class="fa fa-external-link-alt"></i></a></p><p>For BART, T5, mT5 and AlexaTM 20B</p><h2 id=bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
<a class=header-anchor href=#bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension></a></h2><p>Encoder-decoder model was not actually a popular architecture between 2019 and 2020. As you can observe from Figure 1, during 2019-2020 slot, lots of tech companies bid on Encoder-only models, including BERT, RoBERTa, ALBERT. Encoder-decoder and decoder-only models are not yet well explored. But if we further look at the end of branches, Encoder-decoder models still take a place in model zoo, such as Flan series.</p><figure><img src="https://media.licdn.com/dms/image/v2/D4D22AQFNeG_WIY7WOQ/feedshare-shrink_1280/feedshare-shrink_1280/0/1683649150911?e=1746057600&amp;v=beta&amp;t=J7YdPUI60vDiuhDTK7UtMVQrfHrLeVddhkINUVwM3gs" alt="Figure 1. LLMs evolutionary tree." width=800><figcaption><p>Figure 1. LLMs evolutionary tree.</p></figcaption></figure><h3 id=contributions>Contributions
<a class=header-anchor href=#contributions></a></h3><ol><li><strong>Combination of bidirectional encider and autoregressive decoder</strong>: BERT(bi-directional) + GPT(uni-direct auto-regressive)</li><li><strong>Pretraining with better noising approaches</strong>: shuffling + in-filling scheme</li></ol><h3 id=approach>Approach
<a class=header-anchor href=#approach></a></h3><ul><li><strong>Architecture</strong>: BART use seq2seq Transformer architecture. Mofidications:</li></ul><ol><li>ReLUs are modified as GeLUs, with initialization from @\mathcal{N}(0, 0.02)@.</li><li>6 layers in encoder and 12 layers in decoder (following BERT)</li><li>Remove Feed-Forward network before word prediction</li></ol><ul><li><strong>Pretraining</strong>: Trained by corrupting documents and then optimizing a reconstriction loss. Corruptions are introduced in Figure 2.</li></ul><figure><img src=https://myblog-1316371247.cos.ap-shanghai.myqcloud.com/myblog/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-04-23%20013045.png alt="Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed." width=800><figcaption><p>Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed.</p></figcaption></figure><ul><li><strong>Finetuning</strong>: Representations produced by BART are used in Seq/token classification, seq generationa and MT tasks. In MT, the author replace BART&rsquo;s encoder embedding layer with a new randomly initialized encoder, as illustarted in Figure 3.</li></ul><figure><img src=https://myblog-1316371247.cos.ap-shanghai.myqcloud.com/myblog/20250423013642552.png alt="Figure 3. Fine tuning BART for classification and translation." width=1000><figcaption><p>Figure 3. Fine tuning BART for classification and translation.</p></figcaption></figure><h3 id=pretraining-objective-comparison>Pretraining Objective Comparison
<a class=header-anchor href=#pretraining-objective-comparison></a></h3><ul><li><strong>Models</strong>: LM(GPT), Permuted LM(XLNet), Masked LM(BERT), Multitask Masked LM(UniLM), Masked seq2seq(MASS)</li><li><strong>Tasks</strong>:</li></ul><ol><li>SQuAD(context + question -> context span)</li><li>MNLI(con + q -> relation)</li><li>ELI5, XSum, CNN/DM(con + q -> abstraction)</li><li>ConvAI2(dialogue gen)</li></ol><ul><li><strong>Insights</strong>:</li></ul><ol><li>Performance of pre-training methods varies significantly across tasks</li><li>Token masking is crucial</li><li>Left-to-right pre-training improves generation</li><li>Bidirectional encoders are crucial for SQuAD</li><li>The pre-training objective is not the only important factor</li><li>Pure language models perform best on ELI5</li><li>BART achieves the most consistently strong performance.</li></ol><h3 id=large-scale-pre-training-experiments>Large-scale Pre-training Experiments
<a class=header-anchor href=#large-scale-pre-training-experiments></a></h3><p>Pretraining with large batchsize (8000 a batch. 500k steps following RoBERTa):</p><ul><li><strong>Discriminative Tasks</strong>: BART’s improvements on generation tasks do not come at the expense of classification performance.</li><li><strong>Generation Tasks</strong>: Summarization, Dialogue and Abstrctive QA.</li></ul><h2 id=exploring-the-limits-of-transfer-learning-with-a-unified--text-to-text-transformer>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
<a class=header-anchor href=#exploring-the-limits-of-transfer-learning-with-a-unified--text-to-text-transformer></a></h2><p>This work explores the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. And the model proposed in this paper, known as T5, is trained on unified text2text framework, where all of outputs are regarded as texts. (&ldquo;3.8&rdquo;, &ldquo;5&rdquo;)</p><h3 id=motivation>Motivation
<a class=header-anchor href=#motivation></a></h3><p>Previously the training in ML is amenable to downstream learning tasks and the knowledge required for the learning is learned as part of auxiliary task.(word vectors). Later the scheme shifted to pretraining on data-rich tasks using unlabeled data (like Common Crawl project). The work focus on the understanding of burgeoning transfer learning techniques.</p><p>Basic idea: treat every text processing problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output.</p><h3 id=architecture-of-t5-model>Architecture of T5 Model
<a class=header-anchor href=#architecture-of-t5-model></a></h3><p>To summarize, T5 is roughly equivalent to the original Transformer with the exception of</p><ol><li><strong>removing the Layer Norm bias</strong>：activations are only rescaled and no additive bias is applied.</li><li><strong>placing the layer normalization outside the residual path</strong></li><li><strong>using a different position embedding scheme.</strong>: We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model.</li></ol><blockquote><p>Check out
<a href=https://arxiv.org/abs/2503.10622 title="Transformers without Normalization" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Transformers without Normalization
<i class="fa fa-external-link-alt"></i>
</a>from meta&mldr;. Layer Norm is replaced by @tanh(\cdot)@，</p></blockquote><h3 id=dataset---the-colossal-clean-crawled-corpus>Dataset - The Colossal Clean Crawled Corpus
<a class=header-anchor href=#dataset---the-colossal-clean-crawled-corpus></a></h3><p>Adopted from Common Crawl dataset, using cleaning up techniques:</p><ul><li>end in .</li><li>page > 3 sentences; lines >= 5 words</li><li>remove bad words</li><li>lorem ipsum placeholder removed</li><li>curly bracket removed</li><li>citation markers removed</li><li>policy & cookies removed</li><li>leave only one in 3-span sentence sliding window</li><li>pages not written in English</li></ul><h3 id=downstream-tasks>Downstream Tasks
<a class=header-anchor href=#downstream-tasks></a></h3><blockquote><p>Indeed an in-depth tech report &mldr;</p></blockquote><p>750 GB dataset</p><ul><li>machine translation: WMT</li><li>question answering: SQuAD</li><li>abstractive summarization: CNN/Daily Mail</li><li>text classification: GLUE, SuperGLUE</li></ul><h3 id=baselines>Baselines
<a class=header-anchor href=#baselines></a></h3><ul><li><strong>Goal</strong>: Pre-train a standard Transformer using a simple denoising objective and then separately fine-tune on each of our downstream tasks.</li><li><strong>Model</strong>: encoder and decoder are each similar in size and configuration to a BERT_BASE. Each has 12 blocks, FF layer @d_{ff}=3072@, @d_{kv} = 64@, multi-attention with 12 heads. Dropout prob 0.1.</li><li><strong>Training</strong>: As all tasks are regarded as t2t, loss f:
<a href=https://blog.csdn.net/qq_30219017/article/details/89090690 title="teacher forcing" rel="noopener external nofollow noreferrer" target=_blank class=exturl>teacher forcing
<i class="fa fa-external-link-alt"></i>
</a>and cross-entropy loss. AdaFactor as optimizor. Learning rate schedule: inverse square root (@1 / \sqrt{\max{(n, k)}}@), with n as iteration index and k as number of warm-up steps.</li><li><strong>Vocabulary</strong>: SentencePiece to encode text as WordPiece tokens.</li><li><strong>Unsupervised Objective</strong>: (Denoising) An objective that randomly samples and then drops out 15% of tokens in the input sequence, as shown in Figure 4.</li></ul><figure><img src=https://miro.medium.com/v2/resize:fit:988/format:webp/1*9yFICqDlfprn-I_VZ5RHgw.png alt="Figure 4.Schematic of the objective we use in our baseline model. In this example, we process the sentence “Thank you for inviting me to your party last week.” The words “for”, “inviting” and “last” (marked with an ×) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as and ) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel . The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token ." width=800><figcaption><p>Figure 4.Schematic of the objective we use in our baseline model. In this example, we process the sentence “Thank you for inviting me to your party last week.” The words “for”, “inviting” and “last” (marked with an ×) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as <x>and <y>) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel <x>. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token <z>.</p></figcaption></figure><blockquote><p>The following sections are discussing model performance from Architectures, Unsupervised Objectives, Pre-training Datasets, Training strategy and Scaling.</p></blockquote><h3 id=architectures>Architectures
<a class=header-anchor href=#architectures></a></h3><p>Another classification other than encoder/decoder: look into attention mask adopted by the model. There are 3 types of mask patterns: Fulli-visible, Casual and Casual with prefix, as shown in Figure 5.</p><figure><img src=https://blog.codescv.com/images/t5-masks.png alt="Figure 5. Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from “the future”. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence." width=800><figcaption><p>Figure 5. Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from “the future”. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.</p></figcaption></figure><ul><li><strong>Standard Encoder-Decoder architecture</strong>: All-visible mask(Encoder) + Casual mask(Decoder)</li><li><strong>Language Model</strong>: Casual mask, Language models are typically used for compression or sequence generation</li><li><strong>Prefix LM</strong>: Casual with Prefix mask, could be considered as encoder+decoder combined. Architectures as shown in Figure 6.
Prefix LMs resembles BERT for classification tasks when you feeding the model with tasks like: <code>I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target:</code> (Of course! Didn&rsquo;t see why the author are adding this paragraph &mldr;). Attention masking seems to be an interesting topic, which involves many other memory-efficient or computing-efficient methods (paged attention for vllm?).</li></ul><figure><img src=https://dkharazi.github.io/88459ae93dd4af11a69cab297fec5dbd/t5training.png alt="Figure 6. Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use “.” to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fullyvisible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input." width=800><figcaption><p>Figure 6. Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use “.” to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fullyvisible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input.</p></figcaption></figure><p>Here the authors mentioned criterions in selecting models, condidering these models are in different architectures and parameters. We suppose two models are equivalent if they have the same parameter @P@ or the same computational cost @C@. Consider an <em>encoder-decoder model</em> with @L + L@ layers, @P + P@ parameters and a <em>language model</em>(decoder) with @2L@ layers and @2P@ parameters. The parameters are approximately the same for these models but the <strong>computation cost</strong> of <em>language model</em> is approx. twice of that in <em>encoder-decoder</em> model. Because the latter has to deal with both input squence and output sequence but the former deal with inputs and outputs separately. (has lots to do with sequence length &mldr;). Theresfore, they select:</p><ul><li><p>e-d, L + L -> 2P, M flops</p></li><li><p>e-d, shared params -> P, M flops</p></li><li><p>e-d, L/2 + L/2 -> P, M/2 flops</p></li><li><p>d, L -> P, M flops</p></li><li><p>d, prefix -> P, M flops
where e-d for encoder and decoder and L for layers, P for parameters, M for computational cost.</p></li><li><p><strong>Results (for different architecture)</strong>: Denoising task (metioned in previous section) and language modeling task (predicting the whole sentence for language model and predicting the second half of the sentence given the first half). Sharing the params across e-d performed very well and halfing params hurts the performace.</p></li></ul><h3 id=unsupervised-objectives>Unsupervised Objectives
<a class=header-anchor href=#unsupervised-objectives></a></h3><p>Examples of common unsupervised objectives (Figure 7). Models are fisrt pretrained based on these unsupervised objectives then evaluated on downstream tasks (GLUE, CNNDM, SQuAD, SGLUE, EnDe, EnFr and EnRo). This section extends in: 3 common unsupervised objectives -> variants of BERT objective (MLM) -> exploration of corruption rate -> exploration of corrupting spans.</p><figure><img src=https://myblog-1316371247.cos.ap-shanghai.myqcloud.com/myblog/20250423235343386.png alt="Figure 7. Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text “Thank you for inviting me to your party last week .” Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. denotes a shared mask token and , , and denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple." width=1000><figcaption><p>Figure 7. Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text “Thank you for inviting me to your party last week .” Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. <m>denotes a shared mask token and <x>, <y>, and <z>denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple.</p></figcaption></figure><ul><li><p><strong>High Level Approaches</strong>: Tha author evaluated 3 types of objectives: Prefix language modeling, BERT-syle(MLM) and deshuffuling (as illustarted in Figure 7). BERT objective stands out (signifigantly over Deshuffling).</p></li><li><p><strong>Variants of BERT Objective</strong>: Purpose: better performance and better efficiency.</p><ul><li><em>VARIANT 1: MASS-style</em>, reconstruct the original uncorrupted sequence(4th in Figure 7);</li><li><em>VARIANT 2: Unique mask token</em>, predict token prefixed by special token(5th in Figure 7).;</li><li><em>VARIANT 3: Drop Corrupted Tokens</em>, concatenate predicted tokens(6th in Figure 7). All these variants performs similarly.</li><li>Notice that performace of dropping corrupted tokens fluctuates on several metrics. Dropping is still attractive because it <em>reduces the input length thus making training faster</em>.</li></ul></li><li><p><strong>Corruption Rate</strong>: Limited effect on performance. Larger corruption rate -> more inference time.</p></li><li><p><strong>Corrupting Spans</strong>: BERT mask follows i.i.d., masking tokens independently. While in some cases we need consecutive corruption. Number of corruption span and span lengths are determined by parameters. Again, this trick has limited effect on downstream task performance. While span corruption slightly speeds up training because it produces shorter sequence on average.</p></li><li><p><strong>Conclusions</strong>:</p></li></ul><ol><li>Denoising objectives(BERT MLM) outperforms language modeling and deshuffling.</li><li>Choosing among the denoising objectives we considered here should mainly be done <strong>according to their computational cost</strong>(since similar approaches yields slight improvements). It may be fortuitous to explore entirely different ways of leveraging unlabeled data.</li></ol><blockquote><p>For conclusion 2, the paper seems only to explore BERT variants. What about the other two?</p></blockquote><h3 id=pretraining-data-set>Pretraining Data set
<a class=header-anchor href=#pretraining-data-set></a></h3><p>The effects of pretraininig dataset. There are these fun facts:</p><ol><li><code>C4</code> dataset proposed by this paper invloves a heuristic filtering strategy, which proved to be helpful in pretraining.</li><li>Pretraining on in-domain unlabeled data can improve performance on downstream tasks. (not superising? like SFT?) But it&rsquo;s not good if we want our model to adapt to language tasks from arbitraray domains. BUT the dataset gathered for specific domains are much smaller.</li><li>Pretraining dataset size is also a key factor. The performance of pretrained model degrades as the size of dataset getting smaller. (model trys to memorize the dataset rather than learning.)</li><li>Use large dataset as possible. Repeated dataset will degrade the performance while its ok if the repeated time are smaller than 64 (som metrics are even better)</li></ol><blockquote></blockquote></div><footer class=post-footer><hr><div class=post-nav><div class="post-nav-next post-nav-item"></div><div class="post-nav-prev post-nav-item"><a href=/learning/japanese/%E6%97%A5%E8%AF%AD%E8%BE%93%E5%85%A5%E6%B3%95/ rel=prev title=日语输入法>日语输入法
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div id=i18n-translate class=i18n-translate><i class="fa fa-language"></i><div id=lang-select class=lang-select><div id=lang-selected class=selected-option><span class="flag-icon flag-icon-en-us"></span>
<span class=selected-language>English</span>
<i class="fa fa-chevron-down"></i></div><div id=lang-options class=lang-options><div class=lang-option lang-code=en-us lang-name=English lang-url=/learning/llm/cos597g-22-encoder-decoder-models/><span class="flag-icon flag-icon-en-us"></span>
<span class=lang-name>English</span></div></div></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>NexT Theme</span></div></div></footer><script class=next-config data-name=page type=application/json>{"comments":false,"expired":false,"isHome":false,"isPage":true,"path":"cos597g-22-encoder-decoder-models","permalink":"http://localhost:1313/learning/llm/cos597g-22-encoder-decoder-models/","title":"COS597G 22 Encoder Decoder Models","toc":true,"waline":{"commentcnt":{"alias":"@waline/client","alias_name":"waline","file":"dist/comment.js","name":"comment","version":"2.15.8"}}}</script><script type=text/javascript src=http://localhost:1313/js/3rd/animejs/3.2.2/anime.min.js crossorigin=anonymous defer></script><script type=text/javascript src=http://localhost:1313/js/3rd/viewerjs/1.11.6/viewer.min.js crossorigin=anonymous defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":true,"hostname":"http://localhost:1313/","i18n":{"ds_day":" Day Ago","ds_days":" Day ","ds_hour":" Hour Ago","ds_hours":" Hour ","ds_just":"Just","ds_min":" Min Ago","ds_mins":" Min","ds_month":" Month Ago","ds_years":" Year ","empty":"We didn't find any results for the search: ${query}","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms","placeholder":"Searching..."},"isMultiLang":true,"lang":"en-US","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"slideInRight","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":true,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Mist","share":{"addtoany":{"js":"https://static.addtoany.com/menu/page.js","locale":"zh-CN","num":8},"enable":false},"sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"local","router":{"name":"local","type":"modern","url":"http://localhost:1313/js/3rd"}},"version":"4.8.3","waline":{"cfg":{"emoji":false,"imguploader":false,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"@waline/client","file":"dist/waline.css","name":"waline","version":"2.15.8"},"js":{"alias":"@waline/client","file":"dist/waline.js","name":"waline","version":"2.15.8"}}}</script><script type=text/javascript src="/js/main.js?=1765088257" defer></script></body></html>