<!doctype html><html lang=en-US data-theme=dark><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.152.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="COS597G 22 Encoder Only Models"><meta itemprop=description content="Keep it simple, keep it powerful."><meta name=description content="Keep it simple, keep it powerful."><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="http://localhost:1313/profileMode/avatar.jpg"><meta itemprop=keywords content="LLM,COS597G:2022"><meta property="og:type" content="article"><meta property="og:title" content="COS597G 22 Encoder Only Models"><meta property="og:description" content="Keep it simple, keep it powerful."><meta property="og:image" content="/profileMode/avatar.jpg"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/"><meta property="og:site_name" content="Ryan's Blog"><meta property="og:locale" content="en-US"><meta property="article:author" content="NexT Theme"><meta property="article:published_time" content="2025-02-13 15:10:32 +0800 +0800"><meta property="article:modified_time" content="2025-02-13 15:10:32 +0800 +0800"><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/font-awesome/6.7.2/css/all.min.css><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=http://localhost:1313/js/3rd/viewerjs/1.11.6/viewer.min.css><link rel=stylesheet href="/css/main.css?=1765088039"><style type=text/css>.post-footer hr:after{content:"~ End Line ~"}.flinks-list-footer hr:after{content:"~ End Line ~"}</style><link rel=stylesheet type=text/css href="/css/custom_style.css?=1765088039"><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return 0[0];const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),0[0]):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>COS597G 22 Encoder Only Models - Ryan's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Ryan's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Build for Hugo Theme</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>Home</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>Archives
<span class=badge>14</span></a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-tags hvr-icon"></i>Tags</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>Search</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=Searching... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>TOC</li><li class=sidebar-nav-overview>Overview</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#elmo-deep-contextualized-word-representations>(ELMo) Deep contextualized word representations</a><ul><li><a href=#before-reading>Before Reading</a></li><li><a href=#motivation>Motivation</a></li><li><a href=#elmo-embedidngs-from-lm>ELMo: Embedidngs from LM</a></li><li><a href=#experiments>Experiments</a></li></ul></li><li><a href=#improving-language-understanding-by-generative-pre-training>Improving Language Understanding by Generative Pre-Training</a><ul><li><a href=#before-reading-1>Before reading</a></li><li><a href=#motivation-1>Motivation</a></li><li><a href=#framework>Framework</a></li><li><a href=#experiments-1>Experiments</a></li></ul></li><li><a href=#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><ul><li><a href=#before-reading-2>Before Reading</a></li><li><a href=#motivation-2>Motivation</a></li><li><a href=#bert>BERT</a></li><li><a href=#experiments-2>Experiments</a></li></ul></li><li><a href=#roberta-a-robustly-optimized-bert-pretraining-approach>RoBERTa: A Robustly Optimized BERT Pretraining Approach</a><ul><li><a href=#before-reading-3>Before Reading</a></li><li><a href=#motivation-3>Motivation</a></li><li><a href=#training-settings-of-bert>Training Settings of BERT</a></li><li><a href=#training-analysis>Training Analysis</a></li><li><a href=#roberta-training-settings>RoBERTa Training settings</a></li><li><a href=#evaluations>Evaluations</a></li></ul></li><li><a href=#electra-pre-training-text-encoders-as-discriminators-rather-than-generators>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a><ul><li><a href=#before-reading-4>Before Reading</a></li><li><a href=#motivation-4>Motivation</a></li><li><a href=#method>Method</a></li><li><a href=#experiments-3>Experiments</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt="NexT Theme" src=/imgs/img-lazy-loading.gif data-src=/profileMode/avatar.jpg><p class=site-author-name itemprop=name>NexT Theme</p><div class=site-description itemprop=description>Keep it simple, keep it powerful.</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>14</span>
<span class=site-state-item-name>Posts</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>Categories</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>14</span>
<span class=site-state-item-name>Tags</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/Nozom1466 title="Github → https://github.com/Nozom1466" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>
Github
</a></span><span class=links-of-social-item><a href=https://x.com/NozomiKasa1466 title="Twitter → https://x.com/NozomiKasa1466" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-twitter fa-fw hvr-icon"></i>
Twitter</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en class=cc-opacity rel=noopener target=_blank title="Creative Commons"><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt="Creative Commons"></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
Links</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>Web Status</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>Running:</div><div class=item-count id=runTimes data-publishdate="2024-09-22 14:40:51 +0800 +0800"></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>Visitors:</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>Views:</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>Words:</div><div class=item-count id=wordsCount data-count=23874></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>ReadTime:</div><div class=item-count id=readTimes data-times=74></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>Last Update:</div><div class=item-count id=last-push-date data-lastpushdate="2025-05-27 22:59:47 +0800 +0800"></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=goto-i18n-translate class=button title="Multilingual translation"><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title="Change Theme"><i class="fas fa-adjust"></i></div></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/profileMode/avatar.jpg"><meta itemprop=name content="NexT Theme"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="NexT Theme"><meta itemprop=description content="Keep it simple, keep it powerful."></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="COS597G 22 Encoder Only Models"><meta itemprop=description content="

    Homepage
    
    
    

(ELMo) Deep contextualized word representations

Before Reading

Authors are from 

    AI2
    
    
    
 and 

    UW
    
    
    
. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.
Motivation

ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation."></span><header class=post-header><h1 class=post-title itemprop="name headline">COS597G 22 Encoder Only Models</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="fas fa-solid fa-calendar"></i>
</span><span class=post-meta-item-text title="Publish on">Publish on:
</span><time title="Create Time:2025/02/13 15:10:32 +08:00" itemprop="dateCreated datePublished" datetime="2025-02-13 15:10:32 +0800 +0800">2025/02/13</time></span></div><div class=post-meta-items></div></div></header><div class="post-body autonumber" itemprop=articleBody><p><a href=https://www.cs.princeton.edu/courses/archive/fall22/cos597G/ title=Homepage rel="noopener external nofollow noreferrer" target=_blank class=exturl>Homepage
<i class="fa fa-external-link-alt"></i></a></p><h2 id=elmo-deep-contextualized-word-representations>(ELMo) Deep contextualized word representations
<a class=header-anchor href=#elmo-deep-contextualized-word-representations></a></h2><h3 id=before-reading>Before Reading
<a class=header-anchor href=#before-reading></a></h3><p>Authors are from
<a href=https://allenai.org/ title=AI2 rel="noopener external nofollow noreferrer" target=_blank class=exturl>AI2
<i class="fa fa-external-link-alt"></i>
</a>and
<a href=https://www.cs.washington.edu/ title=UW rel="noopener external nofollow noreferrer" target=_blank class=exturl>UW
<i class="fa fa-external-link-alt"></i>
</a>. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.</p><h3 id=motivation>Motivation
<a class=header-anchor href=#motivation></a></h3><p>ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation.</p><a id=more></a><h3 id=elmo-embedidngs-from-lm>ELMo: Embedidngs from LM
<a class=header-anchor href=#elmo-embedidngs-from-lm></a></h3><p>ELMo is built on biLM respresentations. BiLM gives prediction of token @t_k@ by combining forward and backward LM. Log likelihood of token @t_k@ is given by:</p><p>$$
\begin{aligned} & \sum_{k=1}^N\left(\log p\left(t_k \mid t_1, \ldots, t_{k-1} ; \Theta_x, \vec{\Theta}<em>{L S T M}, \Theta_s\right)\right. \ & \left.\quad+\log p\left(t_k \mid t</em>{k+1}, \ldots, t_N ; \Theta_x, \overleftarrow{\Theta}_{L S T M}, \Theta_s\right)\right)\end{aligned}
$$</p><p>where @N@ is the number of tokens, @\Theta@ denotes parameters, with subscript @x@ as token representations and @s@ as softmax layer. Note that parameters of forward and backward LM are separately maintained.</p><p>ELMo is the combination of intermediate respresentation in biLSTMs where representation set with @L@-layer biLM is given by:</p><p>$$
\begin{aligned} R_k & =\left{\mathbf{x}<em>k^{L M}, \overrightarrow{\mathbf{h}}</em>{k, j}^{L M}, \overleftarrow{\mathbf{h}}<em>{k, j}^{L M} \mid j=1, \ldots, L\right} \ & =\left{\mathbf{h}</em>{k, j}^{L M} \mid j=0, \ldots, L\right}\end{aligned}
$$</p><p>where @\mathbf{h}<em>{k, 0}^{L M}@ is the token layer and @\mathbf{h}</em>{k, j}^{L M} = [\overrightarrow{\mathbf{h}}<em>{k, j}^{L M}; \overleftarrow{\mathbf{h}}</em>{k, j}^{L M}]@. Basically, EMLo concatenates hidden representation of forward and backward LSTM model by layer. Architecture shown in Figure 1.</p><figure><img src=https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oi2vJesp7L1ElKnQ1nQQUg.png alt="Fig. 1 ELMo architecture (illustration form BERT)" width=600><figcaption><p>Fig. 1 ELMo architecture (illustration form BERT)</p></figcaption></figure><p>Collapsed ELMo representations are used in downstream NLP tasks. The author adds scale parameters @\gamma^{\text{task}}@ and softmax-normalized weights @s^{\text{task}}@ for different layers:</p><p>$$
\mathbf{E L M o}<em>k^{\text {task }}=E\left(R_k ; \Theta^{\text {task }}\right)=\gamma^{\text {task }} \sum</em>{j=0}^L s_j^{\text {task }} \mathbf{h}_{k, j}^{L M}.
$$</p><p>ELMo vector could either be added in inputs for enhanced representation @[x_k;\mathbf{ELMo}_k]@ or be concatenated with output @[h_k;\mathbf{ELMo}_k]@.</p><p>For computational requirements, the author cuts hidden dimensions to half (to 512) and incorporate residual connections from the first to second layer. <code>CNN-BIG-LSTM</code> trained for 10 epochs yield 39.7 on average forward and backward perplexity, with 9.7 increase compared with forward <code>CMM-BIG-LSTM</code></p><h3 id=experiments>Experiments
<a class=header-anchor href=#experiments></a></h3><p>Tasks and datasets & mectrics:</p><ul><li>QA: SQuAD, F1</li><li>Textual entailment: SNLI, accuracy</li><li>Semantic role labeling: SRL, F1 (OntoNotes)</li><li>Conference resolution: OntoNotes coreference annotation, avg. F1</li><li>NER: Reuters RCV1 corpus, accuracy</li><li>SST-5: , F1</li></ul><p>Adding ELMo representations yields SOTA results, as illustrated in Figure 2.</p><figure><img src=https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQb-FD7F33Z_svGr4jTrfg.png alt="Fig. 2 Results by adding ELMo across 6 tasks." width=400><figcaption><p>Fig. 2 Results by adding ELMo across 6 tasks.</p></figcaption></figure><ul><li><p><strong>Where to add ELMo?</strong>: The author add the representation in the lowest layer in this paper yet claims that some tasks may prefer adding representation in the output of the layer.</p></li><li><p><strong>Differences between layers</strong>: for tasks like Word Sense Disambiguation, last layer is better than the first layer probably because of semantic meanings in final layer. However, for tasks like POS Tagging, as structural information is needed, the first layer outperforms the last layer.</p></li><li><p><strong>Efficiency in sampling</strong>: In the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set. Faster convergence by adding &ldquo;offsets&rdquo; to vectors in high dimension space, which helps model be optimized towards optimal points efficiently?</p></li></ul><h2 id=improving-language-understanding-by-generative-pre-training>Improving Language Understanding by Generative Pre-Training
<a class=header-anchor href=#improving-language-understanding-by-generative-pre-training></a></h2><h3 id=before-reading-1>Before reading
<a class=header-anchor href=#before-reading-1></a></h3><p>Authors are from OpenAI (Ilya Sutskever!). Cited by 11755 (30/11/2024).</p><blockquote><p>How time flies &mldr; 6 years passed and OpenAI has grown into a renowned tech company with $3.4 billion annual revenue. GPT chat bot is well known by people around the globe and everyone can enjoy part of the bonus that AI continues to bring to our society. But Ilya and many other founders left OpenAI with a growing concern about LLM safety; AGI is coming yet it seems like an illusion given the poor performance of current LLM bots. Well, just embrace the changes and move forward and grow up with AI.</p></blockquote><p>The paper introduced a semi-supervised approach by combining pre-training and fine-tuning. Authors also introduced a task-specific input adaption startegy for fine-tuning.</p><h3 id=motivation-1>Motivation
<a class=header-anchor href=#motivation-1></a></h3><p>Training on labeled data has received successful results on NLP tasks, while using unlabeled data is challenging. The optimization objective is unclear and there is no consensus on effective way of transfer learning with learnt representations.</p><h3 id=framework>Framework
<a class=header-anchor href=#framework></a></h3><p>There are two stages of training procedure: unsupervised pretraining and supervised fine-tuning. For fine-tuning, the paper introduces a task-agnostic approach to better adapt learnt representations to spcific tasks.</p><ul><li><strong>Unsupervised pre-training</strong>
Classic Transformer Decoder next word prediction with multi-head attention, FFN &mldr;</li></ul><p>Next word prediction objective is given by
$$
L_1(\mathcal{U})=\sum_i \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)
$$</p><p>$$
\begin{aligned}
h_0 &= UW_e + W_p \
h_l &= \text{transformer_block}(h_{l - 1}), \forall \in [1, n] \
P(u) &= \text{softmax}(h_nW_e^{T})
\end{aligned}
$$
where @\mathcal{U = {u_1, \dots, u_{i - 1}}}@ are unsupervised tokens, parameters @\Theta@, @W_e@ token embedding matrix, @W_p@ position embedding matrix.</p><ul><li><strong>Supervised fine-tuning</strong>
We get labeled dataset @\mathcal{C}@ in supervised fine-tuning, in which input tokens @x^{i}, i \in [1, m]@ are labeled with @y@. @y@ prediction is formulated as
$$
P\left(y \mid x^1, \ldots, x^m\right)=\operatorname{softmax}\left(h_l^m W_y\right).
$$</li></ul><p>Objective is given by
$$
L_2(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^1, \ldots, x^m\right).
$$</p><p>In order to improve generalization and to speed up convergence, the objective is given by
$$
L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C}).
$$</p><ul><li><strong>Task-specific input transformations</strong>
The paper also introduced a task-specific strategy in fine-tuning so as to aviod making extensive changes to the model architecture across tasks. Startegy is illustrated in Figure 3.</li></ul><figure><img src=https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ME_kS-46o8zsF2q-Lt1caA.png alt="Fig. 3: (left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks. We convert all structured inputs into tokensequences to be processed by our pre-trained model, followed by a linear+softmax layer." width=700><figcaption><p>Fig. 3: (left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks. We convert all structured inputs into tokensequences to be processed by our pre-trained model, followed by a linear+softmax layer.</p></figcaption></figure><p>Sounds like structured prompt input.</p><h3 id=experiments-1>Experiments
<a class=header-anchor href=#experiments-1></a></h3><ul><li><p><strong>Setups</strong>: For pre-training, the paper use BooksCorpus dataset & 1B Word Benchmark (used by ELMo), because both datasets contains long and contigious contexts. For fine-tuning, parameters are learning rate 6.25e-5, batchsize 32, linear learning rate decay with 0.2% training warm up.</p></li><li><p><strong>Results</strong>: 4 downstream NLP tasks in fine-tuning: Natural Language Inference (recognizing textual entailment), Question answering and commonsense reasoning, Semantic Similarity, Classification. The approach achieved SOTA in 9 out of 12 datasets and works well on both small and large datasets</p></li><li><p><strong>Analysis</strong></p></li></ul><ol><li>Impact of the number of transferred layers on overall performance: all layers are useful and each layer adds approx. 9% of performance increase on datasets RACE and Mutlti NLI.</li><li>Zero-shot performance of pretraining models on NLP tasks: performance steadily increases as over pretraining, which suggests that generative pretraining supports the learning of a wide variety of task relevant functionality.</li><li>Ablation studies:<ol><li>Performance without auxuliary LM objective @L_1(\mathcal{C})@:larger dataset benefit from @L_1(\mathcal{C})@ while smaller dataset not.</li><li>Importance of using Transformer: the author compares Transformer with LSTM using the same framework. Transformer outperforms LLMs on most tasks.</li><li>Importance of pretraining: performance drops when the model is trained directly on tasks without pertraining</li></ol></li></ol><h2 id=bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
<a class=header-anchor href=#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding></a></h2><h3 id=before-reading-2>Before Reading
<a class=header-anchor href=#before-reading-2></a></h3><p>Authors are from Google AI Language. Citation 120133 (30/11/2024). Paper accepted by NAACL 2019, awarded with Best Long Paper.</p><p>The paper introduces BERT, a bidirectional pretraining method using Transformer. The representations are learnt from left to right and right to left, which provides better representation.</p><h3 id=motivation-2>Motivation
<a class=header-anchor href=#motivation-2></a></h3><p>There are two strategies in applying pre-trained language representations: feature-based methods (ELMo) and fine-tuning (OpenAI GPT). However, the current approached limited power of representation bacause of the nature of learning from left to right. BERT uses bidirectional training and applied two novel pretraining objectives, namely MLM and NSP, to get pretraininig representations of both token-level and sentence-level. BERT also reduce the need of task-specific archituctures.</p><h3 id=bert>BERT
<a class=header-anchor href=#bert></a></h3><ul><li><p><strong>Training:</strong> There are two steps of BERT: pre-training and fine-tuning. During pre-training, BERT utilize two objectives to get pre-training representations. Fine-tuning is firstly initialized with pre-trained parameters and all of the parameters are fine-tuned.</p></li><li><p><strong>Architecture:</strong> BERT is basically a multi-layer bidirectional Transformer encoder, which is different from GPT constrained by left-to-right nature. (In my opinion, bidirectional is mostly illustrated by attention mechanism in encoder)</p></li><li><p><strong>Input/Output Representations:</strong> In BERT, the input sequence might be a single sentence or a pack of two sentences. The first token is always [CLS] and seperation token between two sentences is [SEP]. For two sentences, the author add learned segment embedding @E_A, E_B@ to mark tokens in two sentences @A@ and @B@. The final input is the summation of token, segment embedding and position embedding, as illustrated in Figure 4.</p></li></ul><figure><img src=https://upload.wikimedia.org/wikipedia/commons/6/65/BERT_input_embeddings.png alt="Fig. 4 BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings." width=700><figcaption><p>Fig. 4 BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.</p></figcaption></figure><p><strong>Pre-training:</strong></p><ol><li>Masked LM (MLM): Because of bi-directional feature of BERT, each word would see itself, therefore we need a new pre-training objective. Inspired by Cloze task, the author decided to randomly mask out 15% tokens in each sequence by replacing it with token [MASK]. However, since token [MASK] will not appear in fine-tuning stage after pre-training, there is a mismatch between pre-training and fine-tuning tokens. The authoer then elaborates on the detailed approach of setting [MASK]: within 15% tokens, 80% tokens are replaced by [MASK], 10% tokens are replaced by a random token and the rest stay unchanged.</li><li>Next Sentence Prediction (NSP): A simple binarized task of deciding whether the sentence @B@ is the next sentence of sentence @A@ in sentence pair @[A, B]@. BERT thus learned sentence-level information.</li></ol><blockquote><p>Pre-training data: document-level literature with long contexts, such as BooksCorpus and English Wikipedia in order ot extract contiguous sequences.</p></blockquote><p><strong>Fine-tuning:</strong> Plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. Sentence @[A, B]@ could be interpreted as different meanings like QA, hypothesis-premise pairs etc..</p><h3 id=experiments-2>Experiments
<a class=header-anchor href=#experiments-2></a></h3><p>Tested on four different tasks:</p><ol><li>GLUE: last hidden state + classification weights + softmax</li><li>SQuAD v1.1: QA pairs, predict on the answer span index @[i, j]@</li><li>SQuAD v2.0: The answer probably does not exists in contexts. No answer -> span from [CLS] to [CLS]. The rule of SQuAD also applies.</li><li>SWAG: Given a sentence, the task is to choose the most plausible continuation among four choices.</li></ol><ul><li><strong>Ablations</strong></li></ul><ol><li>Effect of Pre-training tasks: experiments on No NSP, LTR(left2right)&amp;No NSP<ol><li>removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1</li><li>The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.</li></ol></li><li>Effect of Model Size:<ol><li>scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.</li><li>We hypothesize that when the model is fine-tuned directly on the downstream tasks and <em>uses only a very small number of randomly initialized additional parameters</em>, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.</li></ol></li><li>Ablation w/o fine-tuning<ol><li>Pre-computed representations lower down the costs</li><li>BERT is effective for both finetuning and feature-based approaches (only pre-training).</li></ol></li></ol><h2 id=roberta-a-robustly-optimized-bert-pretraining-approach>RoBERTa: A Robustly Optimized BERT Pretraining Approach
<a class=header-anchor href=#roberta-a-robustly-optimized-bert-pretraining-approach></a></h2><h3 id=before-reading-3>Before Reading
<a class=header-anchor href=#before-reading-3></a></h3><p>17176 citation up to 01/09/2025. It&rsquo;s a follow-up work of BERT, in which the author introduced better settings for BERT model training.</p><h3 id=motivation-3>Motivation
<a class=header-anchor href=#motivation-3></a></h3><p>The RoBERTa proposed an improved receipe for training BERT models. Main settings are training time, size of batches, elimination of NSP training, longer sequence and dynamic masking patterns. The results showed improved performance on metrics in BERT.</p><h3 id=training-settings-of-bert>Training Settings of BERT
<a class=header-anchor href=#training-settings-of-bert></a></h3><p>BERT is optimized with Adam with @\beta_1 = 0.9, \beta_2 = 0.999, \epsilon=1e-6@ and @L_2@ weight decay of @0.01@ (warm up 10000 steps to 1e-4 and linearly decayed). Dropout rate 0.1 on all layers. GELU activation. Models trained for 1000000 updates with 256 as batchsize and max-length 512. Models are trained with mixed precision floating point, 8xV100.</p><p>Training data includes BOOKCORPUS, CC-NEWS, OPENWEBTEXT and STORIES.</p><h3 id=training-analysis>Training Analysis
<a class=header-anchor href=#training-analysis></a></h3><p><strong>Dynamic masking and static masking</strong>: To avoid using the same mask for each epoch, the training data were duplicated 10 times and were masked with different ways for each epoch. This was introduced in BERT and called static masking. While for dynamic masking, masking patterns are generated every time we feed a sequence to the model. And &mldr; as the results presented, we indeed see the increase though being marginal.</p><p><strong>Next Sentence Prediction</strong>: NSP loss was questioned by replication experiments. The authors found:</p><ol><li>Using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.</li><li>Removing the NSP loss matches or slightly improves downstream task performance.</li><li>Restricting sequences to come from a single document performs slightly better than packing sequences from multiple documents</li></ol><p><strong>Training with large batches</strong>: Training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training.</p><ul><li><strong>Text Encoding</strong>: Train BERT model using Byte-Pair Encoding.</li></ul><h3 id=roberta-training-settings>RoBERTa Training settings
<a class=header-anchor href=#roberta-training-settings></a></h3><p>Three points: Dynamic masking, trained with FULL-SENTENCES dataset without NSP loss, large mini-batches and byte-level BPE. More settings revolves around data used for pretraining and number of passes through the data.</p><p>The author further conbimed three datasets for training (160GB) and trained the model from 100K to 500K steps.</p><h3 id=evaluations>Evaluations
<a class=header-anchor href=#evaluations></a></h3><p>Models are evaluated on GLUE, SQuAD and RACE.</p><ul><li><p><strong>GLUE</strong>
There are 2 types of tasks: single-task and emsembled task in GLUE. RoBERTa was finetuned for single task on each training dataset based on pretrained model. And for ensembled task, RoBERTa did not depend on multi-task finetuning. Instead, for RTE, STS and MRPC, the model was fine-tuned on MNLI single-task model.</p></li><li><p><strong>SQuAD</strong>
RoBERTa finetuned only on SQuAD training data without data augmentation like previous works. The single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation.</p></li><li><p><strong>RACE</strong>
Each candidate answer was concatenated with the corresponding question and passage. The total length is at most 512 tokens.</p></li></ul><h2 id=electra-pre-training-text-encoders-as-discriminators-rather-than-generators>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
<a class=header-anchor href=#electra-pre-training-text-encoders-as-discriminators-rather-than-generators></a></h2><h3 id=before-reading-4>Before Reading
<a class=header-anchor href=#before-reading-4></a></h3><p>Accepted by ICLR 2020. Authors are from Stanford and Google Brain (Manning!). Citation 4424 up to 01/23/2025.</p><p>Major improvements on pretraining with MASK (Masked language modeling task, MLM) in BERT. Instead of training with fixed [MASK] token, ELECTRA predicts whether the word is replaced by generator or not, which means all tokens in the input will be considered as prediction objectives.</p><h3 id=motivation-4>Motivation
<a class=header-anchor href=#motivation-4></a></h3><p>In MLM task, only 15% of the tokens are learnt by the model as the number of masked tokens are limited. In this work, the author proposed a pretraining task <em>replaced token detection</em>, in which the model learns to distinguish real input tokens from generated replacements. The side-product of this setting is that it solves mismatch between training and testing in BERT. (Remember 15%-80%-10%-10%?) Note that the model is seen as a generator that predicts the original identity of corrupted tokens. Moreover, the ELECTRA also features with compute-efficiency and parameter-efficiency in pretraining stage.</p><h3 id=method>Method
<a class=header-anchor href=#method></a></h3><p>There are two NNs in this work, namely <em>Generator</em> and <em>Discriminator</em>. The generator is in charge of putting mask on input sentence and generating corrupted sentence by replacing masks with other words. The discriminator then tries to distinguish which word in the corrupted sentence is replaced by Generator. In Generator mask words @m_i@, which follows uniform distribution:
$$
m_i \sim \operatorname{unif}{1, n} \text{ for } i=1 \text{ to } k \quad \mathbf{x}^{\text {masked }}=\operatorname{REPLACE}(\mathbf{x}, \mathbf{m},[ \text{MASK} ])
$$
where @\text{REPLACE(\mathbf{x}, \mathbf{m}, p)}@ means replace masked elements in @\mathbf{x}@ with p using @\mathbf{m}@ as mask. @h@ are hidden representations and @e@ are embeddings of generator encoder. Then the masked elements @m_i@ are replaced with new words @\hat x_i@, which follows the distribution given by softmax normalization:
$$
\begin{aligned}
\hat{x}_i &\sim p_G\left(x_i \mid \mathbf{x}^{\text { masked }}\right)\text{ for } i \in \mathbf{m} \
p_G\left(x_t \mid \mathbf{x}\right)&=\exp \left(e\left(x_t\right)^T h_G(\mathbf{x})<em>t\right) / \sum</em>{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^T h_G(\mathbf{x})_t\right) \
\mathbf{x}^{\text {corrupt }}&=\operatorname{REPLACE}(\mathbf{x}, \mathbf{m}, \hat{\mathbf{x}})
\end{aligned}
$$</p><p>The corrupted input @\mathbf{x}^{\text{corrupt}}@ is the input of Discriminator, which tries to distinguish the word replaced by Generator. The possibility for each word is given by:</p><p>$$
D(\mathbf{x}^{\text {corrupt}}, t) = \text{sigmoid}(w^{T}h_{D}(\mathbf{x}^{\text {corrupt }})_t)
$$
The loss function is the combination of MLM task and discrimination task. Figure 4 illustrates ELECTRA using an example.</p><figure><img src=https://ar5iv.labs.arxiv.org/html/2207.08141/assets/f222222.png alt="Fig 4. An overview of replaced token detection. The generator can be any model that producesan output distribution over tokens, but we usually use a small masked language model that is trainedjointly with the discriminator. Although the models are structured like in a GAN, we train thegenerator with maximum likelihood rather than adversarially due to the difficulty of applying GANsto text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks." width=500><figcaption><p>Fig 4. An overview of replaced token detection. The generator can be any model that producesan output distribution over tokens, but we usually use a small masked language model that is trainedjointly with the discriminator. Although the models are structured like in a GAN, we train thegenerator with maximum likelihood rather than adversarially due to the difficulty of applying GANsto text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks.</p></figcaption></figure><p>$$
\begin{aligned}
&\min <em>{\theta_G, \theta_D} \sum</em>{\mathbf{x} \in \mathcal{X}} \mathcal{L}<em>{\mathrm{MLM}}\left(\mathbf{x}, \theta_G\right)+\lambda \mathcal{L}</em>{\text {Disc }}\left(\mathbf{x}, \theta_D\right) \
& \mathcal{L}<em>{\mathrm{MLM}}\left(\mathbf{x}, \theta_G\right)=\mathbb{E}\left(\sum</em>{i \in \mathbf{m}}-\log p_G\left(x_i \mid \mathbf{x}^{\text {masked }}\right)\right) \
& \mathcal{L}<em>{\mathrm{Disc}}\left(\mathbf{x}, \theta_D\right)=\mathbb{E}\left(\sum</em>{t=1}^n-\mathbb{1}\left(x_t^{\mathrm{corrupt}}=x_t\right) \log D\left(\mathbf{x}^{\text {corrupt }}, t\right)-\mathbb{1}\left(x_t^{\text {corrupt }}f \neq x_t\right) \log \left(1-D\left(\mathbf{x}^{\text {corrupt }}, t\right)\right)\right)
\end{aligned}
$$</p><blockquote><p>Disc loss: cross-entropy loss for discrimination.</p></blockquote><ul><li><strong>Difference with GAN</strong></li></ul><ol><li>If the generated token happens to ben correct, the token will be considered &ldquo;real&rdquo; instead of &ldquo;fake&rdquo;.</li><li>The generator is trained with maximum likelihood rather than being trained adversarially to fool the
discriminator. (Ad training is challenging because of it is impossible to backpropergate through sampling from generator.)</li></ol><h3 id=experiments-3>Experiments
<a class=header-anchor href=#experiments-3></a></h3><p>Datasets are GLUE, SQuAD. EM and F1 scores.</p><ul><li><strong>Model extension</strong>: Some techiques used in initialization and training.</li></ul><ol><li>Weight sharing: share all/parts of parameters between generator and discriminator</li><li>Smaller generators: large models generates challenging tasks for discriminator, and sometimes being too hard to answer by discriminator. Smaller generator works effectively. (small model here: keep some of params in generator constant without updating in BP)</li><li>Training algorithms: Two stage procedure: training MLM task for n steps; initialize params in discriminator using params in trained generator. Train the discriminator with generator frozen.</li></ol><p>The author also discusses about the small and large ELECTRA models using weaker training hyperparameters. Further discussions are about the efficiency of ELECTRA. Thr author designed three variations to test token learning in ELECTRA. Results suggest that a large amount of ELECTRA’s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. (btw proves 10% random replacement in BERT is insufficient to solve the issue).</p><h3 id=references>References
<a class=header-anchor href=#references></a></h3><p>[1] Tsang, S. (2022, January 8). Review — ELMO: Deep Contextualized Word Representations. Medium.
<a href=https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c title=https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c
<i class="fa fa-external-link-alt"></i>
</a>[2] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018, February 15). Deep contextualized word representations. arXiv.org.
<a href=https://arxiv.org/abs/1802.05365 title=https://arxiv.org/abs/1802.05365 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/abs/1802.05365
<i class="fa fa-external-link-alt"></i>
</a>[3] Radford, A. (2018). Improving language understanding by generative pre-training.
[4] Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2018, October 11). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv.org.
<a href=https://arxiv.org/abs/1810.04805 title=https://arxiv.org/abs/1810.04805 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/abs/1810.04805
<i class="fa fa-external-link-alt"></i>
</a>[5] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019, July 26). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv.org.
<a href=https://arxiv.org/abs/1907.11692 title=https://arxiv.org/abs/1907.11692 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/abs/1907.11692
<i class="fa fa-external-link-alt"></i>
</a>[6] Clark, K., Luong, M., Le, Q., V., & Manning, C. D. (2020, March 23). ELECTRA: Pre-training text encoders as discriminators rather than generators. arXiv.org. https:// arxiv.org/abs/2003.10555</p></div><footer class=post-footer><div class=post-tags><a href=/tags/llm/>LLM
</a><a href=/tags/cos597g2022/>COS597G:2022</a></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/learning/infra/ai-system-zomi/ rel=next title="AI System ZOMI"><i class="fa fa-chevron-left"></i> AI System ZOMI</a></div><div class="post-nav-prev post-nav-item"><a href=/learning/infra/%E4%BB%8E0%E5%8D%95%E6%8E%92ai_sys/ rel=prev title=从0单排AI_Sys>从0单排AI_Sys
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div id=i18n-translate class=i18n-translate><i class="fa fa-language"></i><div id=lang-select class=lang-select><div id=lang-selected class=selected-option><span class="flag-icon flag-icon-en-us"></span>
<span class=selected-language>English</span>
<i class="fa fa-chevron-down"></i></div><div id=lang-options class=lang-options><div class=lang-option lang-code=en-us lang-name=English lang-url=/learning/llm/cos597g-22-encoder-only-models/><span class="flag-icon flag-icon-en-us"></span>
<span class=lang-name>English</span></div></div></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>NexT Theme</span></div></div></footer><script class=next-config data-name=page type=application/json>{"comments":false,"expired":false,"isHome":false,"isPage":true,"path":"cos597g-22-encoder-only-models","permalink":"http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/","title":"COS597G 22 Encoder Only Models","toc":true,"waline":{"commentcnt":{"alias":"@waline/client","alias_name":"waline","file":"dist/comment.js","name":"comment","version":"2.15.8"}}}</script><script type=text/javascript src=http://localhost:1313/js/3rd/animejs/3.2.2/anime.min.js crossorigin=anonymous defer></script><script type=text/javascript src=http://localhost:1313/js/3rd/viewerjs/1.11.6/viewer.min.js crossorigin=anonymous defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":true,"hostname":"http://localhost:1313/","i18n":{"ds_day":" Day Ago","ds_days":" Day ","ds_hour":" Hour Ago","ds_hours":" Hour ","ds_just":"Just","ds_min":" Min Ago","ds_mins":" Min","ds_month":" Month Ago","ds_years":" Year ","empty":"We didn't find any results for the search: ${query}","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms","placeholder":"Searching..."},"isMultiLang":true,"lang":"en-US","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"slideInRight","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":true,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Mist","share":{"addtoany":{"js":"https://static.addtoany.com/menu/page.js","locale":"zh-CN","num":8},"enable":false},"sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"local","router":{"name":"local","type":"modern","url":"http://localhost:1313/js/3rd"}},"version":"4.8.3","waline":{"cfg":{"emoji":false,"imguploader":false,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"@waline/client","file":"dist/waline.css","name":"waline","version":"2.15.8"},"js":{"alias":"@waline/client","file":"dist/waline.js","name":"waline","version":"2.15.8"}}}</script><script type=text/javascript src="/js/main.js?=1765088039" defer></script></body></html>