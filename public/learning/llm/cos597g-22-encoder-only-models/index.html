<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>COS597G 22 Encoder Only Models | Ryan's Blog</title>
<meta name=keywords content="LLM,COS597G:2022"><meta name=description content="Homepage
For BERT, RoBERTa and ELECTRA
(ELMo) Deep contextualized word representations
Before Reading
Authors are from AI2 and UW. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.
Motivation
ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation."><meta name=author content="Ryan Ming"><link rel=canonical href=http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.df798f0835e7b7ffb23438a93f60e84cf9524cfd904162a056783f54ab69ebcd.css integrity="sha256-33mPCDXnt/+yNDipP2DoTPlSTP2QQWKgVng/VKtp680=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/meta/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/meta/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/meta/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/meta/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/meta/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="COS597G 22 Encoder Only Models"><meta property="og:description" content="Homepage
For BERT, RoBERTa and ELECTRA
(ELMo) Deep contextualized word representations
Before Reading
Authors are from AI2 and UW. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.
Motivation
ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="learning"><meta property="article:published_time" content="2025-02-13T15:10:32+08:00"><meta property="article:modified_time" content="2025-02-13T15:10:32+08:00"><meta property="og:site_name" content="Ryan's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="COS597G 22 Encoder Only Models"><meta name=twitter:description content="Homepage
For BERT, RoBERTa and ELECTRA
(ELMo) Deep contextualized word representations
Before Reading
Authors are from AI2 and UW. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.
Motivation
ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Learnings","item":"http://localhost:1313/learning/"},{"@type":"ListItem","position":2,"name":"COS597G 22 Encoder Only Models","item":"http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"COS597G 22 Encoder Only Models","name":"COS597G 22 Encoder Only Models","description":"Homepage\nFor BERT, RoBERTa and ELECTRA\n(ELMo) Deep contextualized word representations Before Reading Authors are from AI2 and UW. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.\nMotivation ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation.\n","keywords":["LLM","COS597G:2022"],"articleBody":"Homepage\nFor BERT, RoBERTa and ELECTRA\n(ELMo) Deep contextualized word representations Before Reading Authors are from AI2 and UW. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.\nMotivation ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation.\nELMo: Embedidngs from LM ELMo is built on biLM respresentations. BiLM gives prediction of token @t_k@ by combining forward and backward LM. Log likelihood of token @t_k@ is given by:\n$$ \\begin{aligned} \u0026 \\sum_{k=1}^N\\left(\\log p\\left(t_k \\mid t_1, \\ldots, t_{k-1} ; \\Theta_x, \\vec{\\Theta}_{L S T M}, \\Theta_s\\right)\\right. \\\\ \u0026 \\left.\\quad+\\log p\\left(t_k \\mid t_{k+1}, \\ldots, t_N ; \\Theta_x, \\overleftarrow{\\Theta}_{L S T M}, \\Theta_s\\right)\\right)\\end{aligned} $$where @N@ is the number of tokens, @\\Theta@ denotes parameters, with subscript @x@ as token representations and @s@ as softmax layer. Note that parameters of forward and backward LM are separately maintained.\nELMo is the combination of intermediate respresentation in biLSTMs where representation set with @L@-layer biLM is given by:\n$$ \\begin{aligned} R_k \u0026 =\\left\\{\\mathbf{x}_k^{L M}, \\overrightarrow{\\mathbf{h}}_{k, j}^{L M}, \\overleftarrow{\\mathbf{h}}_{k, j}^{L M} \\mid j=1, \\ldots, L\\right\\} \\\\ \u0026 =\\left\\{\\mathbf{h}_{k, j}^{L M} \\mid j=0, \\ldots, L\\right\\}\\end{aligned} $$where @\\mathbf{h}_{k, 0}^{L M}@ is the token layer and @\\mathbf{h}_{k, j}^{L M} = [\\overrightarrow{\\mathbf{h}}_{k, j}^{L M}; \\overleftarrow{\\mathbf{h}}_{k, j}^{L M}]@. Basically, EMLo concatenates hidden representation of forward and backward LSTM model by layer. Architecture shown in Figure 1.\nFig. 1 ELMo architecture (illustration form BERT)\nCollapsed ELMo representations are used in downstream NLP tasks. The author adds scale parameters @\\gamma^{\\text{task}}@ and softmax-normalized weights @s^{\\text{task}}@ for different layers:\n$$ \\mathbf{E L M o}_k^{\\text {task }}=E\\left(R_k ; \\Theta^{\\text {task }}\\right)=\\gamma^{\\text {task }} \\sum_{j=0}^L s_j^{\\text {task }} \\mathbf{h}_{k, j}^{L M}. $$ELMo vector could either be added in inputs for enhanced representation @[x_k;\\mathbf{ELMo}_k]@ or be concatenated with output @[h_k;\\mathbf{ELMo}_k]@.\nFor computational requirements, the author cuts hidden dimensions to half (to 512) and incorporate residual connections from the first to second layer. CNN-BIG-LSTM trained for 10 epochs yield 39.7 on average forward and backward perplexity, with 9.7 increase compared with forward CMM-BIG-LSTM\nExperiments Tasks and datasets \u0026 mectrics:\nQA: SQuAD, F1 Textual entailment: SNLI, accuracy Semantic role labeling: SRL, F1 (OntoNotes) Conference resolution: OntoNotes coreference annotation, avg. F1 NER: Reuters RCV1 corpus, accuracy SST-5: , F1 Adding ELMo representations yields SOTA results, as illustrated in Figure 2.\nFig. 2 Results by adding ELMo across 6 tasks.\nWhere to add ELMo?: The author add the representation in the lowest layer in this paper yet claims that some tasks may prefer adding representation in the output of the layer.\nDifferences between layers: for tasks like Word Sense Disambiguation, last layer is better than the first layer probably because of semantic meanings in final layer. However, for tasks like POS Tagging, as structural information is needed, the first layer outperforms the last layer.\nEfficiency in sampling: In the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set. Faster convergence by adding “offsets” to vectors in high dimension space, which helps model be optimized towards optimal points efficiently?\nImproving Language Understanding by Generative Pre-Training Before reading Authors are from OpenAI (Ilya Sutskever!). Cited by 11755 (30/11/2024).\nHow time flies … 6 years passed and OpenAI has grown into a renowned tech company with $3.4 billion annual revenue. GPT chat bot is well known by people around the globe and everyone can enjoy part of the bonus that AI continues to bring to our society. But Ilya and many other founders left OpenAI with a growing concern about LLM safety; AGI is coming yet it seems like an illusion given the poor performance of current LLM bots. Well, just embrace the changes and move forward and grow up with AI.\nThe paper introduced a semi-supervised approach by combining pre-training and fine-tuning. Authors also introduced a task-specific input adaption startegy for fine-tuning.\nMotivation Training on labeled data has received successful results on NLP tasks, while using unlabeled data is challenging. The optimization objective is unclear and there is no consensus on effective way of transfer learning with learnt representations.\nFramework There are two stages of training procedure: unsupervised pretraining and supervised fine-tuning. For fine-tuning, the paper introduces a task-agnostic approach to better adapt learnt representations to spcific tasks.\nUnsupervised pre-training\nClassic Transformer Decoder next word prediction with multi-head attention, FFN … Next word prediction objective is given by\n$$ L_1(\\mathcal{U})=\\sum_i \\log P\\left(u_i \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right) $$$$ \\begin{aligned} h_0 \u0026= UW_e + W_p \\\\ h_l \u0026= \\text{transformer_block}(h_{l - 1}), \\forall \\in [1, n] \\\\ P(u) \u0026= \\text{softmax}(h_nW_e^{T}) \\end{aligned} $$\nwhere @\\mathcal{U = \\{u_1, \\dots, u_{i - 1}\\}}@ are unsupervised tokens, parameters @\\Theta@, @W_e@ token embedding matrix, @W_p@ position embedding matrix.\nSupervised fine-tuning\nWe get labeled dataset @\\mathcal{C}@ in supervised fine-tuning, in which input tokens @x^{i}, i \\in [1, m]@ are labeled with @y@. @y@ prediction is formulated as\n$$ P\\left(y \\mid x^1, \\ldots, x^m\\right)=\\operatorname{softmax}\\left(h_l^m W_y\\right). $$ Objective is given by\n$$ L_2(\\mathcal{C})=\\sum_{(x, y)} \\log P\\left(y \\mid x^1, \\ldots, x^m\\right). $$In order to improve generalization and to speed up convergence, the objective is given by\n$$ L_3(\\mathcal{C}) = L_2(\\mathcal{C}) + \\lambda \\cdot L_1(\\mathcal{C}). $$ Task-specific input transformations\nThe paper also introduced a task-specific strategy in fine-tuning so as to aviod making extensive changes to the model architecture across tasks. Startegy is illustrated in Figure 3. Fig. 3: (left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks. We convert all structured inputs into tokensequences to be processed by our pre-trained model, followed by a linear+softmax layer.\nSounds like structured prompt input.\nExperiments Setups: For pre-training, the paper use BooksCorpus dataset \u0026 1B Word Benchmark (used by ELMo), because both datasets contains long and contigious contexts. For fine-tuning, parameters are learning rate 6.25e-5, batchsize 32, linear learning rate decay with 0.2% training warm up.\nResults: 4 downstream NLP tasks in fine-tuning: Natural Language Inference (recognizing textual entailment), Question answering and commonsense reasoning, Semantic Similarity, Classification. The approach achieved SOTA in 9 out of 12 datasets and works well on both small and large datasets\nAnalysis\nImpact of the number of transferred layers on overall performance: all layers are useful and each layer adds approx. 9% of performance increase on datasets RACE and Mutlti NLI. Zero-shot performance of pretraining models on NLP tasks: performance steadily increases as over pretraining, which suggests that generative pretraining supports the learning of a wide variety of task relevant functionality. Ablation studies: Performance without auxuliary LM objective @L_1(\\mathcal{C})@:larger dataset benefit from @L_1(\\mathcal{C})@ while smaller dataset not. Importance of using Transformer: the author compares Transformer with LSTM using the same framework. Transformer outperforms LLMs on most tasks. Importance of pretraining: performance drops when the model is trained directly on tasks without pertraining BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Before Reading Authors are from Google AI Language. Citation 120133 (30/11/2024). Paper accepted by NAACL 2019, awarded with Best Long Paper.\nThe paper introduces BERT, a bidirectional pretraining method using Transformer. The representations are learnt from left to right and right to left, which provides better representation.\nMotivation There are two strategies in applying pre-trained language representations: feature-based methods (ELMo) and fine-tuning (OpenAI GPT). However, the current approached limited power of representation bacause of the nature of learning from left to right. BERT uses bidirectional training and applied two novel pretraining objectives, namely MLM and NSP, to get pretraininig representations of both token-level and sentence-level. BERT also reduce the need of task-specific archituctures.\nBERT Training: There are two steps of BERT: pre-training and fine-tuning. During pre-training, BERT utilize two objectives to get pre-training representations. Fine-tuning is firstly initialized with pre-trained parameters and all of the parameters are fine-tuned.\nArchitecture: BERT is basically a multi-layer bidirectional Transformer encoder, which is different from GPT constrained by left-to-right nature. (In my opinion, bidirectional is mostly illustrated by attention mechanism in encoder)\nInput/Output Representations: In BERT, the input sequence might be a single sentence or a pack of two sentences. The first token is always [CLS] and seperation token between two sentences is [SEP]. For two sentences, the author add learned segment embedding @E_A, E_B@ to mark tokens in two sentences @A@ and @B@. The final input is the summation of token, segment embedding and position embedding, as illustrated in Figure 4.\nFig. 4 BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\nPre-training:\nMasked LM (MLM): Because of bi-directional feature of BERT, each word would see itself, therefore we need a new pre-training objective. Inspired by Cloze task, the author decided to randomly mask out 15% tokens in each sequence by replacing it with token [MASK]. However, since token [MASK] will not appear in fine-tuning stage after pre-training, there is a mismatch between pre-training and fine-tuning tokens. The authoer then elaborates on the detailed approach of setting [MASK]: within 15% tokens, 80% tokens are replaced by [MASK], 10% tokens are replaced by a random token and the rest stay unchanged. Next Sentence Prediction (NSP): A simple binarized task of deciding whether the sentence @B@ is the next sentence of sentence @A@ in sentence pair @[A, B]@. BERT thus learned sentence-level information. Pre-training data: document-level literature with long contexts, such as BooksCorpus and English Wikipedia in order ot extract contiguous sequences.\nFine-tuning: Plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. Sentence @[A, B]@ could be interpreted as different meanings like QA, hypothesis-premise pairs etc..\nExperiments Tested on four different tasks:\nGLUE: last hidden state + classification weights + softmax SQuAD v1.1: QA pairs, predict on the answer span index @[i, j]@ SQuAD v2.0: The answer probably does not exists in contexts. No answer -\u003e span from [CLS] to [CLS]. The rule of SQuAD also applies. SWAG: Given a sentence, the task is to choose the most plausible continuation among four choices. Ablations Effect of Pre-training tasks: experiments on No NSP, LTR(left2right)\u0026No NSP removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1 The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. Effect of Model Size: scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. We hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small. Ablation w/o fine-tuning Pre-computed representations lower down the costs BERT is effective for both finetuning and feature-based approaches (only pre-training). RoBERTa: A Robustly Optimized BERT Pretraining Approach Before Reading 17176 citation up to 01/09/2025. It’s a follow-up work of BERT, in which the author introduced better settings for BERT model training.\nMotivation The RoBERTa proposed an improved receipe for training BERT models. Main settings are training time, size of batches, elimination of NSP training, longer sequence and dynamic masking patterns. The results showed improved performance on metrics in BERT.\nTraining Settings of BERT BERT is optimized with Adam with @\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon=1e-6@ and @L_2@ weight decay of @0.01@ (warm up 10000 steps to 1e-4 and linearly decayed). Dropout rate 0.1 on all layers. GELU activation. Models trained for 1000000 updates with 256 as batchsize and max-length 512. Models are trained with mixed precision floating point, 8xV100.\nTraining data includes BOOKCORPUS, CC-NEWS, OPENWEBTEXT and STORIES.\nTraining Analysis Dynamic masking and static masking: To avoid using the same mask for each epoch, the training data were duplicated 10 times and were masked with different ways for each epoch. This was introduced in BERT and called static masking. While for dynamic masking, masking patterns are generated every time we feed a sequence to the model. And … as the results presented, we indeed see the increase though being marginal.\nNext Sentence Prediction: NSP loss was questioned by replication experiments. The authors found:\nUsing individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies. Removing the NSP loss matches or slightly improves downstream task performance. Restricting sequences to come from a single document performs slightly better than packing sequences from multiple documents Training with large batches: Training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training.\nText Encoding: Train BERT model using Byte-Pair Encoding. RoBERTa Training settings Three points: Dynamic masking, trained with FULL-SENTENCES dataset without NSP loss, large mini-batches and byte-level BPE. More settings revolves around data used for pretraining and number of passes through the data.\nThe author further conbimed three datasets for training (160GB) and trained the model from 100K to 500K steps.\nEvaluations Models are evaluated on GLUE, SQuAD and RACE.\nGLUE\nThere are 2 types of tasks: single-task and emsembled task in GLUE. RoBERTa was finetuned for single task on each training dataset based on pretrained model. And for ensembled task, RoBERTa did not depend on multi-task finetuning. Instead, for RTE, STS and MRPC, the model was fine-tuned on MNLI single-task model.\nSQuAD\nRoBERTa finetuned only on SQuAD training data without data augmentation like previous works. The single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation.\nRACE\nEach candidate answer was concatenated with the corresponding question and passage. The total length is at most 512 tokens.\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators Before Reading Accepted by ICLR 2020. Authors are from Stanford and Google Brain (Manning!). Citation 4424 up to 01/23/2025.\nMajor improvements on pretraining with MASK (Masked language modeling task, MLM) in BERT. Instead of training with fixed [MASK] token, ELECTRA predicts whether the word is replaced by generator or not, which means all tokens in the input will be considered as prediction objectives.\nMotivation In MLM task, only 15% of the tokens are learnt by the model as the number of masked tokens are limited. In this work, the author proposed a pretraining task replaced token detection, in which the model learns to distinguish real input tokens from generated replacements. The side-product of this setting is that it solves mismatch between training and testing in BERT. (Remember 15%-80%-10%-10%?) Note that the model is seen as a generator that predicts the original identity of corrupted tokens. Moreover, the ELECTRA also features with compute-efficiency and parameter-efficiency in pretraining stage.\nMethod There are two NNs in this work, namely Generator and Discriminator. The generator is in charge of putting mask on input sentence and generating corrupted sentence by replacing masks with other words. The discriminator then tries to distinguish which word in the corrupted sentence is replaced by Generator. In Generator mask words @m_i@, which follows uniform distribution:\n$$ m_i \\sim \\operatorname{unif}\\{1, n\\} \\text{ for } i=1 \\text{ to } k \\quad \\mathbf{x}^{\\text {masked }}=\\operatorname{REPLACE}(\\mathbf{x}, \\mathbf{m},[ \\text{MASK} ]) $$\nwhere @\\text{REPLACE(\\mathbf{x}, \\mathbf{m}, p)}@ means replace masked elements in @\\mathbf{x}@ with p using @\\mathbf{m}@ as mask. @h@ are hidden representations and @e@ are embeddings of generator encoder. Then the masked elements @m_i@ are replaced with new words @\\hat x_i@, which follows the distribution given by softmax normalization:\n$$ \\begin{aligned} \\hat{x}_i \u0026\\sim p_G\\left(x_i \\mid \\mathbf{x}^{\\text { masked }}\\right)\\text{ for } i \\in \\mathbf{m} \\\\ p_G\\left(x_t \\mid \\mathbf{x}\\right)\u0026=\\exp \\left(e\\left(x_t\\right)^T h_G(\\mathbf{x})_t\\right) / \\sum_{x^{\\prime}} \\exp \\left(e\\left(x^{\\prime}\\right)^T h_G(\\mathbf{x})_t\\right) \\\\ \\mathbf{x}^{\\text {corrupt }}\u0026=\\operatorname{REPLACE}(\\mathbf{x}, \\mathbf{m}, \\hat{\\mathbf{x}}) \\end{aligned} $$The corrupted input @\\mathbf{x}^{\\text{corrupt}}@ is the input of Discriminator, which tries to distinguish the word replaced by Generator. The possibility for each word is given by:\n$$ D(\\mathbf{x}^{\\text {corrupt}}, t) = \\text{sigmoid}(w^{T}h_{D}(\\mathbf{x}^{\\text {corrupt }})_t) $$\nThe loss function is the combination of MLM task and discrimination task. Figure 4 illustrates ELECTRA using an example.\nFig 4. An overview of replaced token detection. The generator can be any model that producesan output distribution over tokens, but we usually use a small masked language model that is trainedjointly with the discriminator. Although the models are structured like in a GAN, we train thegenerator with maximum likelihood rather than adversarially due to the difficulty of applying GANsto text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks.\n$$ \\begin{aligned} \u0026\\min _{\\theta_G, \\theta_D} \\sum_{\\mathbf{x} \\in \\mathcal{X}} \\mathcal{L}_{\\mathrm{MLM}}\\left(\\mathbf{x}, \\theta_G\\right)+\\lambda \\mathcal{L}_{\\text {Disc }}\\left(\\mathbf{x}, \\theta_D\\right) \\\\ \u0026 \\mathcal{L}_{\\mathrm{MLM}}\\left(\\mathbf{x}, \\theta_G\\right)=\\mathbb{E}\\left(\\sum_{i \\in \\mathbf{m}}-\\log p_G\\left(x_i \\mid \\mathbf{x}^{\\text {masked }}\\right)\\right) \\\\ \u0026 \\mathcal{L}_{\\mathrm{Disc}}\\left(\\mathbf{x}, \\theta_D\\right)=\\mathbb{E}\\left(\\sum_{t=1}^n-\\mathbb{1}\\left(x_t^{\\mathrm{corrupt}}=x_t\\right) \\log D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)-\\mathbb{1}\\left(x_t^{\\text {corrupt }}f \\neq x_t\\right) \\log \\left(1-D\\left(\\mathbf{x}^{\\text {corrupt }}, t\\right)\\right)\\right) \\end{aligned} $$ Disc loss: cross-entropy loss for discrimination.\nDifference with GAN If the generated token happens to ben correct, the token will be considered “real” instead of “fake”. The generator is trained with maximum likelihood rather than being trained adversarially to fool the\ndiscriminator. (Ad training is challenging because of it is impossible to backpropergate through sampling from generator.) Experiments Datasets are GLUE, SQuAD. EM and F1 scores.\nModel extension: Some techiques used in initialization and training. Weight sharing: share all/parts of parameters between generator and discriminator Smaller generators: large models generates challenging tasks for discriminator, and sometimes being too hard to answer by discriminator. Smaller generator works effectively. (small model here: keep some of params in generator constant without updating in BP) Training algorithms: Two stage procedure: training MLM task for n steps; initialize params in discriminator using params in trained generator. Train the discriminator with generator frozen. The author also discusses about the small and large ELECTRA models using weaker training hyperparameters. Further discussions are about the efficiency of ELECTRA. Thr author designed three variations to test token learning in ELECTRA. Results suggest that a large amount of ELECTRA’s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. (btw proves 10% random replacement in BERT is insufficient to solve the issue).\nReferences [1] Tsang, S. (2022, January 8). Review — ELMO: Deep Contextualized Word Representations. Medium. https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c\n[2] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., \u0026 Zettlemoyer, L. (2018, February 15). Deep contextualized word representations. arXiv.org. https://arxiv.org/abs/1802.05365\n[3] Radford, A. (2018). Improving language understanding by generative pre-training.\n[4] Devlin, J., Chang, M., Lee, K., \u0026 Toutanova, K. (2018, October 11). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv.org. https://arxiv.org/abs/1810.04805\n[5] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., \u0026 Stoyanov, V. (2019, July 26). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv.org. https://arxiv.org/abs/1907.11692\n[6] Clark, K., Luong, M., Le, Q., V., \u0026 Manning, C. D. (2020, March 23). ELECTRA: Pre-training text encoders as discriminators rather than generators. arXiv.org. https:// arxiv.org/abs/2003.10555\n","wordCount":"3226","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-02-13T15:10:32+08:00","dateModified":"2025-02-13T15:10:32+08:00","author":{"@type":"Person","name":"Ryan Ming"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/learning/llm/cos597g-22-encoder-only-models/"},"publisher":{"@type":"Organization","name":"Ryan's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/meta/favicon.ico"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["@","@"]]}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/learning/ title=Learning><span>Learning</span></a></li><li><a href=http://localhost:1313/ideas/ title=Ideas><span>Ideas</span></a></li><li><a href=http://localhost:1313/travel/ title=Travel><span>Travel</span></a></li><li><a href=http://localhost:1313/ryanming-portfolio/ title=Academic><span>Academic</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/learning/>Learnings</a></div><h1 class="post-title entry-hint-parent">COS597G 22 Encoder Only Models</h1><div class=post-meta><span title='2025-02-13 15:10:32 +0800 CST'>February 13, 2025</span>&nbsp;·&nbsp;Ryan Ming</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#elmo-deep-contextualized-word-representations>(ELMo) Deep contextualized word representations</a><ul><li><a href=#before-reading>Before Reading</a></li><li><a href=#motivation>Motivation</a></li><li><a href=#elmo-embedidngs-from-lm>ELMo: Embedidngs from LM</a></li><li><a href=#experiments>Experiments</a></li></ul></li><li><a href=#improving-language-understanding-by-generative-pre-training>Improving Language Understanding by Generative Pre-Training</a><ul><li><a href=#before-reading-1>Before reading</a></li><li><a href=#motivation-1>Motivation</a></li><li><a href=#framework>Framework</a></li><li><a href=#experiments-1>Experiments</a></li></ul></li><li><a href=#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><ul><li><a href=#before-reading-2>Before Reading</a></li><li><a href=#motivation-2>Motivation</a></li><li><a href=#bert>BERT</a></li><li><a href=#experiments-2>Experiments</a></li></ul></li><li><a href=#roberta-a-robustly-optimized-bert-pretraining-approach>RoBERTa: A Robustly Optimized BERT Pretraining Approach</a><ul><li><a href=#before-reading-3>Before Reading</a></li><li><a href=#motivation-3>Motivation</a></li><li><a href=#training-settings-of-bert>Training Settings of BERT</a></li><li><a href=#training-analysis>Training Analysis</a></li><li><a href=#roberta-training-settings>RoBERTa Training settings</a></li><li><a href=#evaluations>Evaluations</a></li></ul></li><li><a href=#electra-pre-training-text-encoders-as-discriminators-rather-than-generators>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a><ul><li><a href=#before-reading-4>Before Reading</a></li><li><a href=#motivation-4>Motivation</a></li><li><a href=#method>Method</a></li><li><a href=#experiments-3>Experiments</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p><a href=https://www.cs.princeton.edu/courses/archive/fall22/cos597G/>Homepage</a></p><p>For BERT, RoBERTa and ELECTRA</p><h2 id=elmo-deep-contextualized-word-representations>(ELMo) Deep contextualized word representations<a hidden class=anchor aria-hidden=true href=#elmo-deep-contextualized-word-representations>#</a></h2><h3 id=before-reading>Before Reading<a hidden class=anchor aria-hidden=true href=#before-reading>#</a></h3><p>Authors are from <a href=https://allenai.org/>AI2</a> and <a href=https://www.cs.washington.edu/>UW</a>. Citation 16115 (until 11/25/2024). Paper accepted by NAACL 2018, nominated as Best Paper. Paper introduced a embedding by stacking embeddings from bidirectional LSTMs.</p><h3 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h3><p>ELMo aims to find better embeddings for NLP tasks. Previous methods proposed word vectors, which are encoded in static strategy and failed to deal with words with various meanings in different contexts (Training multiple representation for 1 word partly solved the problem but it is not feasible when it comes to evolving meanings). Improvements are using subword meaning and bidirectional LSTM to encode contexts around the target word. ELMo embeddings are based on biLSTM hidden representations. Previous work also claims that layers from different depth encode meanings of different levels. ELMo takes it into account in hidden representation concatenation.</p><h3 id=elmo-embedidngs-from-lm>ELMo: Embedidngs from LM<a hidden class=anchor aria-hidden=true href=#elmo-embedidngs-from-lm>#</a></h3><p>ELMo is built on biLM respresentations. BiLM gives prediction of token @t_k@ by combining forward and backward LM. Log likelihood of token @t_k@ is given by:</p>$$
\begin{aligned} & \sum_{k=1}^N\left(\log p\left(t_k \mid t_1, \ldots, t_{k-1} ; \Theta_x, \vec{\Theta}_{L S T M}, \Theta_s\right)\right. \\ & \left.\quad+\log p\left(t_k \mid t_{k+1}, \ldots, t_N ; \Theta_x, \overleftarrow{\Theta}_{L S T M}, \Theta_s\right)\right)\end{aligned}
$$<p>where @N@ is the number of tokens, @\Theta@ denotes parameters, with subscript @x@ as token representations and @s@ as softmax layer. Note that parameters of forward and backward LM are separately maintained.</p><p>ELMo is the combination of intermediate respresentation in biLSTMs where representation set with @L@-layer biLM is given by:</p>$$
\begin{aligned} R_k & =\left\{\mathbf{x}_k^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} \mid j=1, \ldots, L\right\} \\ & =\left\{\mathbf{h}_{k, j}^{L M} \mid j=0, \ldots, L\right\}\end{aligned}
$$<p>where @\mathbf{h}_{k, 0}^{L M}@ is the token layer and @\mathbf{h}_{k, j}^{L M} = [\overrightarrow{\mathbf{h}}_{k, j}^{L M}; \overleftarrow{\mathbf{h}}_{k, j}^{L M}]@. Basically, EMLo concatenates hidden representation of forward and backward LSTM model by layer. Architecture shown in Figure 1.</p><figure class=align-center><img loading=lazy src=https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oi2vJesp7L1ElKnQ1nQQUg.png#center alt="Fig. 1 ELMo architecture (illustration form BERT)" width=600><figcaption><p>Fig. 1 ELMo architecture (illustration form BERT)</p></figcaption></figure><p>Collapsed ELMo representations are used in downstream NLP tasks. The author adds scale parameters @\gamma^{\text{task}}@ and softmax-normalized weights @s^{\text{task}}@ for different layers:</p>$$
\mathbf{E L M o}_k^{\text {task }}=E\left(R_k ; \Theta^{\text {task }}\right)=\gamma^{\text {task }} \sum_{j=0}^L s_j^{\text {task }} \mathbf{h}_{k, j}^{L M}.
$$<p>ELMo vector could either be added in inputs for enhanced representation @[x_k;\mathbf{ELMo}_k]@ or be concatenated with output @[h_k;\mathbf{ELMo}_k]@.</p><p>For computational requirements, the author cuts hidden dimensions to half (to 512) and incorporate residual connections from the first to second layer. <code>CNN-BIG-LSTM</code> trained for 10 epochs yield 39.7 on average forward and backward perplexity, with 9.7 increase compared with forward <code>CMM-BIG-LSTM</code></p><h3 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h3><p>Tasks and datasets & mectrics:</p><ul><li>QA: SQuAD, F1</li><li>Textual entailment: SNLI, accuracy</li><li>Semantic role labeling: SRL, F1 (OntoNotes)</li><li>Conference resolution: OntoNotes coreference annotation, avg. F1</li><li>NER: Reuters RCV1 corpus, accuracy</li><li>SST-5: , F1</li></ul><p>Adding ELMo representations yields SOTA results, as illustrated in Figure 2.</p><figure class=align-center><img loading=lazy src=https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQb-FD7F33Z_svGr4jTrfg.png#center alt="Fig. 2 Results by adding ELMo across 6 tasks." width=400><figcaption><p>Fig. 2 Results by adding ELMo across 6 tasks.</p></figcaption></figure><ul><li><p><strong>Where to add ELMo?</strong>: The author add the representation in the lowest layer in this paper yet claims that some tasks may prefer adding representation in the output of the layer.</p></li><li><p><strong>Differences between layers</strong>: for tasks like Word Sense Disambiguation, last layer is better than the first layer probably because of semantic meanings in final layer. However, for tasks like POS Tagging, as structural information is needed, the first layer outperforms the last layer.</p></li><li><p><strong>Efficiency in sampling</strong>: In the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set. Faster convergence by adding &ldquo;offsets&rdquo; to vectors in high dimension space, which helps model be optimized towards optimal points efficiently?</p></li></ul><h2 id=improving-language-understanding-by-generative-pre-training>Improving Language Understanding by Generative Pre-Training<a hidden class=anchor aria-hidden=true href=#improving-language-understanding-by-generative-pre-training>#</a></h2><h3 id=before-reading-1>Before reading<a hidden class=anchor aria-hidden=true href=#before-reading-1>#</a></h3><p>Authors are from OpenAI (Ilya Sutskever!). Cited by 11755 (30/11/2024).</p><blockquote><p>How time flies &mldr; 6 years passed and OpenAI has grown into a renowned tech company with $3.4 billion annual revenue. GPT chat bot is well known by people around the globe and everyone can enjoy part of the bonus that AI continues to bring to our society. But Ilya and many other founders left OpenAI with a growing concern about LLM safety; AGI is coming yet it seems like an illusion given the poor performance of current LLM bots. Well, just embrace the changes and move forward and grow up with AI.</p></blockquote><p>The paper introduced a semi-supervised approach by combining pre-training and fine-tuning. Authors also introduced a task-specific input adaption startegy for fine-tuning.</p><h3 id=motivation-1>Motivation<a hidden class=anchor aria-hidden=true href=#motivation-1>#</a></h3><p>Training on labeled data has received successful results on NLP tasks, while using unlabeled data is challenging. The optimization objective is unclear and there is no consensus on effective way of transfer learning with learnt representations.</p><h3 id=framework>Framework<a hidden class=anchor aria-hidden=true href=#framework>#</a></h3><p>There are two stages of training procedure: unsupervised pretraining and supervised fine-tuning. For fine-tuning, the paper introduces a task-agnostic approach to better adapt learnt representations to spcific tasks.</p><ul><li><strong>Unsupervised pre-training</strong><br>Classic Transformer Decoder next word prediction with multi-head attention, FFN &mldr;</li></ul><p>Next word prediction objective is given by<br></p>$$
L_1(\mathcal{U})=\sum_i \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)
$$$$
\begin{aligned}
h_0 &= UW_e + W_p \\
        h_l &= \text{transformer_block}(h_{l - 1}), \forall \in [1, n] \\
P(u) &= \text{softmax}(h_nW_e^{T})
    \end{aligned}
$$<p><br>where @\mathcal{U = \{u_1, \dots, u_{i - 1}\}}@ are unsupervised tokens, parameters @\Theta@, @W_e@ token embedding matrix, @W_p@ position embedding matrix.</p><ul><li><strong>Supervised fine-tuning</strong><br>We get labeled dataset @\mathcal{C}@ in supervised fine-tuning, in which input tokens @x^{i}, i \in [1, m]@ are labeled with @y@. @y@ prediction is formulated as<br>$$
P\left(y \mid x^1, \ldots, x^m\right)=\operatorname{softmax}\left(h_l^m W_y\right).
$$</li></ul><p>Objective is given by<br></p>$$
L_2(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^1, \ldots, x^m\right).
$$<p>In order to improve generalization and to speed up convergence, the objective is given by<br></p>$$
L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C}).
$$<ul><li><strong>Task-specific input transformations</strong><br>The paper also introduced a task-specific strategy in fine-tuning so as to aviod making extensive changes to the model architecture across tasks. Startegy is illustrated in Figure 3.</li></ul><figure class=align-center><img loading=lazy src=https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ME_kS-46o8zsF2q-Lt1caA.png#center alt="Fig. 3: (left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks. We convert all structured inputs into tokensequences to be processed by our pre-trained model, followed by a linear+softmax layer." width=700><figcaption><p>Fig. 3: (left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks. We convert all structured inputs into tokensequences to be processed by our pre-trained model, followed by a linear+softmax layer.</p></figcaption></figure><p>Sounds like structured prompt input.</p><h3 id=experiments-1>Experiments<a hidden class=anchor aria-hidden=true href=#experiments-1>#</a></h3><ul><li><p><strong>Setups</strong>: For pre-training, the paper use BooksCorpus dataset & 1B Word Benchmark (used by ELMo), because both datasets contains long and contigious contexts. For fine-tuning, parameters are learning rate 6.25e-5, batchsize 32, linear learning rate decay with 0.2% training warm up.</p></li><li><p><strong>Results</strong>: 4 downstream NLP tasks in fine-tuning: Natural Language Inference (recognizing textual entailment), Question answering and commonsense reasoning, Semantic Similarity, Classification. The approach achieved SOTA in 9 out of 12 datasets and works well on both small and large datasets</p></li><li><p><strong>Analysis</strong></p></li></ul><ol><li>Impact of the number of transferred layers on overall performance: all layers are useful and each layer adds approx. 9% of performance increase on datasets RACE and Mutlti NLI.</li><li>Zero-shot performance of pretraining models on NLP tasks: performance steadily increases as over pretraining, which suggests that generative pretraining supports the learning of a wide variety of task relevant functionality.</li><li>Ablation studies:<ol><li>Performance without auxuliary LM objective @L_1(\mathcal{C})@:larger dataset benefit from @L_1(\mathcal{C})@ while smaller dataset not.</li><li>Importance of using Transformer: the author compares Transformer with LSTM using the same framework. Transformer outperforms LLMs on most tasks.</li><li>Importance of pretraining: performance drops when the model is trained directly on tasks without pertraining</li></ol></li></ol><h2 id=bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<a hidden class=anchor aria-hidden=true href=#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding>#</a></h2><h3 id=before-reading-2>Before Reading<a hidden class=anchor aria-hidden=true href=#before-reading-2>#</a></h3><p>Authors are from Google AI Language. Citation 120133 (30/11/2024). Paper accepted by NAACL 2019, awarded with Best Long Paper.</p><p>The paper introduces BERT, a bidirectional pretraining method using Transformer. The representations are learnt from left to right and right to left, which provides better representation.</p><h3 id=motivation-2>Motivation<a hidden class=anchor aria-hidden=true href=#motivation-2>#</a></h3><p>There are two strategies in applying pre-trained language representations: feature-based methods (ELMo) and fine-tuning (OpenAI GPT). However, the current approached limited power of representation bacause of the nature of learning from left to right. BERT uses bidirectional training and applied two novel pretraining objectives, namely MLM and NSP, to get pretraininig representations of both token-level and sentence-level. BERT also reduce the need of task-specific archituctures.</p><h3 id=bert>BERT<a hidden class=anchor aria-hidden=true href=#bert>#</a></h3><ul><li><p><strong>Training:</strong> There are two steps of BERT: pre-training and fine-tuning. During pre-training, BERT utilize two objectives to get pre-training representations. Fine-tuning is firstly initialized with pre-trained parameters and all of the parameters are fine-tuned.</p></li><li><p><strong>Architecture:</strong> BERT is basically a multi-layer bidirectional Transformer encoder, which is different from GPT constrained by left-to-right nature. (In my opinion, bidirectional is mostly illustrated by attention mechanism in encoder)</p></li><li><p><strong>Input/Output Representations:</strong> In BERT, the input sequence might be a single sentence or a pack of two sentences. The first token is always [CLS] and seperation token between two sentences is [SEP]. For two sentences, the author add learned segment embedding @E_A, E_B@ to mark tokens in two sentences @A@ and @B@. The final input is the summation of token, segment embedding and position embedding, as illustrated in Figure 4.</p></li></ul><figure class=align-center><img loading=lazy src=https://upload.wikimedia.org/wikipedia/commons/6/65/BERT_input_embeddings.png#center alt="Fig. 4 BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings." width=700><figcaption><p>Fig. 4 BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.</p></figcaption></figure><p><strong>Pre-training:</strong></p><ol><li>Masked LM (MLM): Because of bi-directional feature of BERT, each word would see itself, therefore we need a new pre-training objective. Inspired by Cloze task, the author decided to randomly mask out 15% tokens in each sequence by replacing it with token [MASK]. However, since token [MASK] will not appear in fine-tuning stage after pre-training, there is a mismatch between pre-training and fine-tuning tokens. The authoer then elaborates on the detailed approach of setting [MASK]: within 15% tokens, 80% tokens are replaced by [MASK], 10% tokens are replaced by a random token and the rest stay unchanged.</li><li>Next Sentence Prediction (NSP): A simple binarized task of deciding whether the sentence @B@ is the next sentence of sentence @A@ in sentence pair @[A, B]@. BERT thus learned sentence-level information.</li></ol><blockquote><p>Pre-training data: document-level literature with long contexts, such as BooksCorpus and English Wikipedia in order ot extract contiguous sequences.</p></blockquote><p><strong>Fine-tuning:</strong> Plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. Sentence @[A, B]@ could be interpreted as different meanings like QA, hypothesis-premise pairs etc..</p><h3 id=experiments-2>Experiments<a hidden class=anchor aria-hidden=true href=#experiments-2>#</a></h3><p>Tested on four different tasks:</p><ol><li>GLUE: last hidden state + classification weights + softmax</li><li>SQuAD v1.1: QA pairs, predict on the answer span index @[i, j]@</li><li>SQuAD v2.0: The answer probably does not exists in contexts. No answer -> span from [CLS] to [CLS]. The rule of SQuAD also applies.</li><li>SWAG: Given a sentence, the task is to choose the most plausible continuation among four choices.</li></ol><ul><li><strong>Ablations</strong></li></ul><ol><li>Effect of Pre-training tasks: experiments on No NSP, LTR(left2right)&amp;No NSP<ol><li>removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1</li><li>The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.</li></ol></li><li>Effect of Model Size:<ol><li>scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.</li><li>We hypothesize that when the model is fine-tuned directly on the downstream tasks and <em>uses only a very small number of randomly initialized additional parameters</em>, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.</li></ol></li><li>Ablation w/o fine-tuning<ol><li>Pre-computed representations lower down the costs</li><li>BERT is effective for both finetuning and feature-based approaches (only pre-training).</li></ol></li></ol><h2 id=roberta-a-robustly-optimized-bert-pretraining-approach>RoBERTa: A Robustly Optimized BERT Pretraining Approach<a hidden class=anchor aria-hidden=true href=#roberta-a-robustly-optimized-bert-pretraining-approach>#</a></h2><h3 id=before-reading-3>Before Reading<a hidden class=anchor aria-hidden=true href=#before-reading-3>#</a></h3><p>17176 citation up to 01/09/2025. It&rsquo;s a follow-up work of BERT, in which the author introduced better settings for BERT model training.</p><h3 id=motivation-3>Motivation<a hidden class=anchor aria-hidden=true href=#motivation-3>#</a></h3><p>The RoBERTa proposed an improved receipe for training BERT models. Main settings are training time, size of batches, elimination of NSP training, longer sequence and dynamic masking patterns. The results showed improved performance on metrics in BERT.</p><h3 id=training-settings-of-bert>Training Settings of BERT<a hidden class=anchor aria-hidden=true href=#training-settings-of-bert>#</a></h3><p>BERT is optimized with Adam with @\beta_1 = 0.9, \beta_2 = 0.999, \epsilon=1e-6@ and @L_2@ weight decay of @0.01@ (warm up 10000 steps to 1e-4 and linearly decayed). Dropout rate 0.1 on all layers. GELU activation. Models trained for 1000000 updates with 256 as batchsize and max-length 512. Models are trained with mixed precision floating point, 8xV100.</p><p>Training data includes BOOKCORPUS, CC-NEWS, OPENWEBTEXT and STORIES.</p><h3 id=training-analysis>Training Analysis<a hidden class=anchor aria-hidden=true href=#training-analysis>#</a></h3><p><strong>Dynamic masking and static masking</strong>: To avoid using the same mask for each epoch, the training data were duplicated 10 times and were masked with different ways for each epoch. This was introduced in BERT and called static masking. While for dynamic masking, masking patterns are generated every time we feed a sequence to the model. And &mldr; as the results presented, we indeed see the increase though being marginal.</p><p><strong>Next Sentence Prediction</strong>: NSP loss was questioned by replication experiments. The authors found:</p><ol><li>Using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.</li><li>Removing the NSP loss matches or slightly improves downstream task performance.</li><li>Restricting sequences to come from a single document performs slightly better than packing sequences from multiple documents</li></ol><p><strong>Training with large batches</strong>: Training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training.</p><ul><li><strong>Text Encoding</strong>: Train BERT model using Byte-Pair Encoding.</li></ul><h3 id=roberta-training-settings>RoBERTa Training settings<a hidden class=anchor aria-hidden=true href=#roberta-training-settings>#</a></h3><p>Three points: Dynamic masking, trained with FULL-SENTENCES dataset without NSP loss, large mini-batches and byte-level BPE. More settings revolves around data used for pretraining and number of passes through the data.</p><p>The author further conbimed three datasets for training (160GB) and trained the model from 100K to 500K steps.</p><h3 id=evaluations>Evaluations<a hidden class=anchor aria-hidden=true href=#evaluations>#</a></h3><p>Models are evaluated on GLUE, SQuAD and RACE.</p><ul><li><p><strong>GLUE</strong><br>There are 2 types of tasks: single-task and emsembled task in GLUE. RoBERTa was finetuned for single task on each training dataset based on pretrained model. And for ensembled task, RoBERTa did not depend on multi-task finetuning. Instead, for RTE, STS and MRPC, the model was fine-tuned on MNLI single-task model.</p></li><li><p><strong>SQuAD</strong><br>RoBERTa finetuned only on SQuAD training data without data augmentation like previous works. The single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation.</p></li><li><p><strong>RACE</strong><br>Each candidate answer was concatenated with the corresponding question and passage. The total length is at most 512 tokens.</p></li></ul><h2 id=electra-pre-training-text-encoders-as-discriminators-rather-than-generators>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators<a hidden class=anchor aria-hidden=true href=#electra-pre-training-text-encoders-as-discriminators-rather-than-generators>#</a></h2><h3 id=before-reading-4>Before Reading<a hidden class=anchor aria-hidden=true href=#before-reading-4>#</a></h3><p>Accepted by ICLR 2020. Authors are from Stanford and Google Brain (Manning!). Citation 4424 up to 01/23/2025.</p><p>Major improvements on pretraining with MASK (Masked language modeling task, MLM) in BERT. Instead of training with fixed [MASK] token, ELECTRA predicts whether the word is replaced by generator or not, which means all tokens in the input will be considered as prediction objectives.</p><h3 id=motivation-4>Motivation<a hidden class=anchor aria-hidden=true href=#motivation-4>#</a></h3><p>In MLM task, only 15% of the tokens are learnt by the model as the number of masked tokens are limited. In this work, the author proposed a pretraining task <em>replaced token detection</em>, in which the model learns to distinguish real input tokens from generated replacements. The side-product of this setting is that it solves mismatch between training and testing in BERT. (Remember 15%-80%-10%-10%?) Note that the model is seen as a generator that predicts the original identity of corrupted tokens. Moreover, the ELECTRA also features with compute-efficiency and parameter-efficiency in pretraining stage.</p><h3 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h3><p>There are two NNs in this work, namely <em>Generator</em> and <em>Discriminator</em>. The generator is in charge of putting mask on input sentence and generating corrupted sentence by replacing masks with other words. The discriminator then tries to distinguish which word in the corrupted sentence is replaced by Generator. In Generator mask words @m_i@, which follows uniform distribution:<br></p>$$
m_i \sim \operatorname{unif}\{1, n\} \text{ for } i=1 \text{ to } k \quad \mathbf{x}^{\text {masked }}=\operatorname{REPLACE}(\mathbf{x}, \mathbf{m},[ \text{MASK} ])
$$<p><br>where @\text{REPLACE(\mathbf{x}, \mathbf{m}, p)}@ means replace masked elements in @\mathbf{x}@ with p using @\mathbf{m}@ as mask. @h@ are hidden representations and @e@ are embeddings of generator encoder. Then the masked elements @m_i@ are replaced with new words @\hat x_i@, which follows the distribution given by softmax normalization:<br></p>$$
\begin{aligned}
\hat{x}_i &\sim p_G\left(x_i \mid \mathbf{x}^{\text { masked }}\right)\text{ for } i \in \mathbf{m} \\
p_G\left(x_t \mid \mathbf{x}\right)&=\exp \left(e\left(x_t\right)^T h_G(\mathbf{x})_t\right) / \sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^T h_G(\mathbf{x})_t\right) \\
\mathbf{x}^{\text {corrupt }}&=\operatorname{REPLACE}(\mathbf{x}, \mathbf{m}, \hat{\mathbf{x}})
\end{aligned}
$$<p>The corrupted input @\mathbf{x}^{\text{corrupt}}@ is the input of Discriminator, which tries to distinguish the word replaced by Generator. The possibility for each word is given by:</p>$$
D(\mathbf{x}^{\text {corrupt}}, t) = \text{sigmoid}(w^{T}h_{D}(\mathbf{x}^{\text {corrupt }})_t)
$$<p><br>The loss function is the combination of MLM task and discrimination task. Figure 4 illustrates ELECTRA using an example.</p><figure class=align-center><img loading=lazy src=https://ar5iv.labs.arxiv.org/html/2207.08141/assets/f222222.png#center alt="Fig 4. An overview of replaced token detection. The generator can be any model that producesan output distribution over tokens, but we usually use a small masked language model that is trainedjointly with the discriminator. Although the models are structured like in a GAN, we train thegenerator with maximum likelihood rather than adversarially due to the difficulty of applying GANsto text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks." width=500><figcaption><p>Fig 4. An overview of replaced token detection. The generator can be any model that producesan output distribution over tokens, but we usually use a small masked language model that is trainedjointly with the discriminator. Although the models are structured like in a GAN, we train thegenerator with maximum likelihood rather than adversarially due to the difficulty of applying GANsto text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks.</p></figcaption></figure>$$
\begin{aligned}
&\min _{\theta_G, \theta_D} \sum_{\mathbf{x} \in \mathcal{X}} \mathcal{L}_{\mathrm{MLM}}\left(\mathbf{x}, \theta_G\right)+\lambda \mathcal{L}_{\text {Disc }}\left(\mathbf{x}, \theta_D\right) \\
& \mathcal{L}_{\mathrm{MLM}}\left(\mathbf{x}, \theta_G\right)=\mathbb{E}\left(\sum_{i \in \mathbf{m}}-\log p_G\left(x_i \mid \mathbf{x}^{\text {masked }}\right)\right) \\
& \mathcal{L}_{\mathrm{Disc}}\left(\mathbf{x}, \theta_D\right)=\mathbb{E}\left(\sum_{t=1}^n-\mathbb{1}\left(x_t^{\mathrm{corrupt}}=x_t\right) \log D\left(\mathbf{x}^{\text {corrupt }}, t\right)-\mathbb{1}\left(x_t^{\text {corrupt }}f \neq x_t\right) \log \left(1-D\left(\mathbf{x}^{\text {corrupt }}, t\right)\right)\right)
\end{aligned}
$$<blockquote><p>Disc loss: cross-entropy loss for discrimination.</p></blockquote><ul><li><strong>Difference with GAN</strong></li></ul><ol><li>If the generated token happens to ben correct, the token will be considered &ldquo;real&rdquo; instead of &ldquo;fake&rdquo;.</li><li>The generator is trained with maximum likelihood rather than being trained adversarially to fool the<br>discriminator. (Ad training is challenging because of it is impossible to backpropergate through sampling from generator.)</li></ol><h3 id=experiments-3>Experiments<a hidden class=anchor aria-hidden=true href=#experiments-3>#</a></h3><p>Datasets are GLUE, SQuAD. EM and F1 scores.</p><ul><li><strong>Model extension</strong>: Some techiques used in initialization and training.</li></ul><ol><li>Weight sharing: share all/parts of parameters between generator and discriminator</li><li>Smaller generators: large models generates challenging tasks for discriminator, and sometimes being too hard to answer by discriminator. Smaller generator works effectively. (small model here: keep some of params in generator constant without updating in BP)</li><li>Training algorithms: Two stage procedure: training MLM task for n steps; initialize params in discriminator using params in trained generator. Train the discriminator with generator frozen.</li></ol><p>The author also discusses about the small and large ELECTRA models using weaker training hyperparameters. Further discussions are about the efficiency of ELECTRA. Thr author designed three variations to test token learning in ELECTRA. Results suggest that a large amount of ELECTRA’s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. (btw proves 10% random replacement in BERT is insufficient to solve the issue).</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] Tsang, S. (2022, January 8). Review — ELMO: Deep Contextualized Word Representations. Medium. <a href=https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c>https://sh-tsang.medium.com/review-elmo-deep-contextualized-word-representations-8eb1e58cd25c</a><br>[2] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018, February 15). Deep contextualized word representations. arXiv.org. <a href=https://arxiv.org/abs/1802.05365>https://arxiv.org/abs/1802.05365</a><br>[3] Radford, A. (2018). Improving language understanding by generative pre-training.<br>[4] Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2018, October 11). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv.org. <a href=https://arxiv.org/abs/1810.04805>https://arxiv.org/abs/1810.04805</a><br>[5] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019, July 26). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv.org. <a href=https://arxiv.org/abs/1907.11692>https://arxiv.org/abs/1907.11692</a><br>[6] Clark, K., Luong, M., Le, Q., V., & Manning, C. D. (2020, March 23). ELECTRA: Pre-training text encoders as discriminators rather than generators. arXiv.org. https:// arxiv.org/abs/2003.10555</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llm/>LLM</a></li><li><a href=http://localhost:1313/tags/cos597g2022/>COS597G:2022</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Ryan's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><script defer src=https://use.fontawesome.com/releases/v5.15.4/js/all.js integrity=sha384-rOA1PnstxnOBLzCLMcre8ybwbTmemjzdNlILg8O7z1lUkLXozs4DHonlDtnE7fpc crossorigin=anonymous></script><a class=busuanzi-counter><span id=busuanzi_container_value_site_pv><i class="far fa-eye fa-fw"></i>
<span id=busuanzi_value_site_pv></span>
</span>&nbsp;|&nbsp;
<span id=busuanzi_container_value_site_uv><i class="fa fa-user"></i>
<span id=busuanzi_value_site_uv></span></span></a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>